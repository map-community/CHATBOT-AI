import os
import re
import time
import pickle
import logging
from datetime import datetime
from collections import defaultdict
import numpy as np
import pytz
from dotenv import load_dotenv
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
from langchain import hub
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import Runnable, RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableSequence, RunnableMap
from langchain_core.runnables import RunnableLambda
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from pymongo import MongoClient
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Mecab import (logger ì •ì˜ ì´í›„)
try:
    from konlpy.tag import Mecab
    MECAB_AVAILABLE = True
    logger.info("âœ… Mecab ì‚¬ìš© ê°€ëŠ¥ (30-50ë°° ë¹ ë¥¸ í˜•íƒœì†Œ ë¶„ì„)")
except Exception as e:
    logger.warning(f"âš ï¸  Mecabì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    logger.warning("âš ï¸  Mecab ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì •í™•ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    MECAB_AVAILABLE = False
    Mecab = None

# StorageManager import
from modules.storage_manager import get_storage_manager

# Configuration import
from config.settings import MINIMUM_SIMILARITY_SCORE

# StorageManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
storage = get_storage_manager()

# URL ìƒìˆ˜
NOTICE_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
COMPANY_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b"
SEMINAR_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4"
PROFESSOR_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2"

def get_korean_time():
    return datetime.now(pytz.timezone('Asia/Seoul'))

# ë‹¨ì–´ ëª…ì‚¬í™” í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - QueryTransformer ì‚¬ìš©)
def transformed_query(content):
    """
    ì§ˆë¬¸ì„ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        content: ì‚¬ìš©ì ì§ˆë¬¸ (ì›ë¬¸)

    Returns:
        List[str]: ì¶”ì¶œëœ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    return storage.query_transformer.transform(content)
###################################################################################################


# Dense Retrieval (Upstage ì„ë² ë”©) - Lazy initializationìœ¼ë¡œ í•¨ìˆ˜ ë‚´ì—ì„œ ìƒì„±í•˜ë„ë¡ ë³€ê²½
# embeddings ê°ì²´ëŠ” í•„ìš”í•  ë•Œ get_embeddings() í•¨ìˆ˜ë¥¼ í†µí•´ ê°€ì ¸ì˜µë‹ˆë‹¤.
def get_embeddings():
    """Upstage Embeddings ê°ì²´ ë°˜í™˜ (Lazy initialization)"""
    return UpstageEmbeddings(
        api_key=storage.upstage_api_key,
        model="solar-embedding-1-large-query"  # ì§ˆë¬¸ ì„ë² ë”©ìš© ëª¨ë¸
    )
# dense_doc_vectors = np.array(embeddings.embed_documents(texts))  # ë¬¸ì„œ ì„ë² ë”©

def fetch_titles_from_pinecone():
    """
    Pineconeì—ì„œ ì „ì²´ ë°ì´í„°(ì œëª©, í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    - list() ë©”ì„œë“œ(Pagination)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œìˆ˜ ì œí•œ ì—†ì´ ëª¨ë“  IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - fetch() ë©”ì„œë“œ(Batch)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - html_available=trueì¸ ê²½ìš° MongoDBì—ì„œ ì‹¤ì œ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    logger.info("ğŸ”„ Pinecone ì „ì²´ ë°ì´í„° ì¡°íšŒ ì‹œì‘...")

    # ==========================================
    # MongoDB ì—°ê²° (HTML ì¡°íšŒìš©)
    # ==========================================
    mongo_collection = None
    mongo_client = None
    try:
        if storage.mongo_collection is not None:
            # StorageManagerì˜ MongoDB connection ì‚¬ìš©
            mongo_collection = storage.mongo_collection.database["multimodal_cache"]
            logger.info("âœ… MongoDB ì—°ê²° ì„±ê³µ (HTML ì¡°íšŒìš©)")
    except Exception as e:
        logger.warning(f"âš ï¸  MongoDB ì—°ê²° ì‹¤íŒ¨ (HTML ì—†ì´ ì§„í–‰): {e}")

    # ==========================================
    # 1. ì „ì²´ ID ê°€ì ¸ì˜¤ê¸° (ê°œìˆ˜ ì œí•œ ì—†ìŒ!)
    # ==========================================
    all_ids = []
    
    try:
        # namespaceê°€ ìˆë‹¤ë©´ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’ "")
        # list()ëŠ” ì „ì²´ IDë¥¼ í˜ì´ì§€ë„¤ì´ì…˜í•˜ì—¬ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
        for ids in storage.pinecone_index.list(namespace=""): 
            all_ids.extend(ids)
        logger.info(f"ğŸ“Š ì´ {len(all_ids)}ê°œì˜ ë²¡í„° IDë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ ID ë¦¬ìŠ¤íŒ… ì‹¤íŒ¨: {e}")
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ë¬¸ì œì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆë‚´
        logger.error("ğŸ‘‰ 'requirements.txt'ì˜ pinecone ë²„ì „ì„ í™•ì¸í•˜ê³  ì¬ë¹Œë“œí•˜ì„¸ìš”.")
        return [], [], [], [], [], [], [], [], [], []

    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
    if not all_ids:
        logger.warning("âš ï¸ ì¡°íšŒëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤.")
        return [], [], [], [], [], [], [], [], [], []


    # ==========================================
    # 2. IDë¡œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Batch Fetch)
    # ==========================================
    # ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    titles = []
    texts = []
    urls = []
    dates = []
    htmls = []
    content_types = []
    sources = []
    image_urls = []
    attachment_urls = []
    attachment_types = []

    # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë°°ì¹˜ í¬ê¸°
    batch_size = 1000

    # ë””ë²„ê¹… ì¹´ìš´í„° ì¶”ê°€
    html_available_count = 0
    mongo_found_count = 0
    html_extracted_count = 0

    # 1,000ê°œì”© ëŠì–´ì„œ ìš”ì²­
    for i in range(0, len(all_ids), batch_size):
        logger.info(f"â³ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({i} / {len(all_ids)})")
        
        batch_ids = all_ids[i:i + batch_size]
        
        try:
            # Fetch ìš”ì²­
            fetch_response = storage.pinecone_index.fetch(ids=batch_ids)
            
            # ì‘ë‹µ ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (v3 í˜¸í™˜ì„± í•´ê²°)
            vectors = {}
            if hasattr(fetch_response, 'to_dict'):
                response_dict = fetch_response.to_dict()
                vectors = response_dict.get('vectors', {})
            elif hasattr(fetch_response, 'vectors'):
                vectors = fetch_response.vectors
            else:
                vectors = fetch_response.get('vectors', {})

            if vectors is None:
                vectors = {}
            
            # ê°€ì ¸ì˜¨ ë°ì´í„° íŒŒì‹±
            for vector_id in batch_ids:
                if vector_id in vectors:
                    vector_data = vectors[vector_id]
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    if isinstance(vector_data, dict):
                        metadata = vector_data.get('metadata', {})
                    elif hasattr(vector_data, 'metadata'):
                        metadata = vector_data.metadata
                    else:
                        metadata = {}

                    if metadata is None:
                        metadata = {}
                    
                    # ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„° ì¶”ê°€
                    titles.append(metadata.get("title", ""))
                    texts.append(metadata.get("text", ""))
                    url = metadata.get("url", "")
                    urls.append(url)
                    dates.append(metadata.get("date", ""))

                    # ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„°: html_availableì´ë©´ MongoDBì—ì„œ HTML ì¡°íšŒ
                    html = ""
                    if metadata.get("html_available"):
                        html_available_count += 1
                        if mongo_collection is not None:
                            try:
                                # html_available=trueì¸ chunkëŠ” ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ì—ì„œ ì¶”ì¶œëœ ê²ƒ
                                # MongoDB cacheëŠ” image_url ë˜ëŠ” attachment_urlì„ keyë¡œ ì‚¬ìš©
                                lookup_url = metadata.get("image_url") or metadata.get("attachment_url")

                                if lookup_url:
                                    # ë””ë²„ê¹…: URL ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
                                    if html_available_count <= 3:
                                        logger.info(f"ğŸ” ì¡°íšŒ ì‹œë„ URL: {lookup_url[:80]}...")

                                    cached = mongo_collection.find_one({"url": lookup_url})
                                    if cached:
                                        mongo_found_count += 1
                                        # ë””ë²„ê¹…: ì°¾ì€ ê²½ìš° ë¡œê¹…
                                        if mongo_found_count <= 3:
                                            logger.info(f"âœ… MongoDBì—ì„œ ë°œê²¬: {lookup_url[:80]}...")
                                            logger.info(f"   í•„ë“œ: {list(cached.keys())}")

                                        # Markdown ìš°ì„  (Upstage API ì œê³µ, ê³ í’ˆì§ˆ í‘œ êµ¬ì¡°)
                                        # ì´ë¯¸ì§€: ocr_markdown, ë¬¸ì„œ: markdown
                                        markdown_content = cached.get("ocr_markdown") or cached.get("markdown", "")

                                        # Markdownì´ ì—†ìœ¼ë©´ HTML ì‚¬ìš© (fallback)
                                        html_content = markdown_content or cached.get("ocr_html") or cached.get("html", "")

                                        if html_content:
                                            html = html_content
                                            html_extracted_count += 1
                                    else:
                                        # ë””ë²„ê¹…: ëª» ì°¾ì€ ê²½ìš° ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
                                        if html_available_count <= 3:
                                            logger.warning(f"âŒ MongoDBì—ì„œ ëª» ì°¾ìŒ: {lookup_url[:80]}...")
                                else:
                                    # image_urlê³¼ attachment_urlì´ ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°
                                    if html_available_count <= 3:
                                        logger.warning(f"âš ï¸  html_available=trueì¸ë° image_url/attachment_url ì—†ìŒ (board URL: {url[:80]}...)")
                            except Exception as e:
                                logger.warning(f"MongoDB HTML ì¡°íšŒ ì‹¤íŒ¨: {e}")

                    htmls.append(html)
                    content_types.append(metadata.get("content_type", "text"))
                    sources.append(metadata.get("source", "original_post"))
                    image_urls.append(metadata.get("image_url", ""))
                    attachment_urls.append(metadata.get("attachment_url", ""))
                    attachment_types.append(metadata.get("attachment_type", ""))
                    
        except Exception as e:
            logger.error(f"âš ï¸ ë°°ì¹˜ Fetch ì‹¤íŒ¨ ({i}~{i+batch_size}): {e}")
            continue

    logger.info(f"âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(titles)}ê°œ ë¬¸ì„œ")
    logger.info(f"ğŸ“Š HTML ì¡°íšŒ í†µê³„:")
    logger.info(f"   - html_available=true ë¬¸ì„œ: {html_available_count}ê°œ")
    logger.info(f"   - MongoDBì—ì„œ ì°¾ì€ ë¬¸ì„œ: {mongo_found_count}ê°œ")
    logger.info(f"   - ì‹¤ì œ HTML ì¶”ì¶œ ì„±ê³µ: {html_extracted_count}ê°œ")

    return titles, texts, urls, dates, htmls, content_types, sources, image_urls, attachment_urls, attachment_types


# ìºì‹± ë°ì´í„° ì´ˆê¸°í™” í•¨ìˆ˜

def initialize_cache():
    """
    ìºì‹œ ì´ˆê¸°í™” í•¨ìˆ˜ (Redis Fast Track ì ìš©)
    - Redis ìºì‹œê°€ ìˆìœ¼ë©´ 3ì´ˆ ë¡œë”©
    - ì—†ìœ¼ë©´ Pineconeì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ Redisì— ì €ì¥ (20ë¶„ ì†Œìš”, ìµœì´ˆ 1íšŒë§Œ)
    """
    try:
        logger.info("ğŸ”„ ìºì‹œ ì´ˆê¸°í™” ì‹œì‘...")

        # ==========================================
        # 1. Redis ìºì‹œ í™•ì¸ (Fast Track)
        # ==========================================
        if storage.redis_client is not None:
            try:
                logger.info("ğŸ” Redis ìºì‹œ í™•ì¸ ì¤‘...")
                cached_data = storage.redis_client.get('pinecone_metadata')

                if cached_data:
                    logger.info("ğŸš€ Redis ìºì‹œ ë°œê²¬! ë¹ ë¥¸ ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

                    # Pickleë¡œ ì €ì¥ëœ ë°ì´í„° ë³µì›
                    (storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
                     storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
                     storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types) = pickle.loads(cached_data)

                    logger.info(f"âœ… Redis ë¡œë“œ ì™„ë£Œ! ({len(storage.cached_titles)}ê°œ ë¬¸ì„œ, Pinecone ë‹¤ìš´ë¡œë“œ ìƒëµ)")
                    logger.info(f"   - HTML êµ¬ì¡° ìˆëŠ” ë¬¸ì„œ: {sum(1 for html in storage.cached_htmls if html)}ê°œ")
                    logger.info(f"   - ì´ë¯¸ì§€ OCR ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'image')}ê°œ")
                    logger.info(f"   - ì²¨ë¶€íŒŒì¼ ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'attachment')}ê°œ")

                    # Retriever ì´ˆê¸°í™”ë¡œ ì í”„ (Pinecone Fetch ìƒëµ!)
                    _initialize_retrievers()
                    logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(storage.cached_titles)}, texts: {len(storage.cached_texts)})")
                    return
                else:
                    logger.info("â¬‡ï¸  Redisì— ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

            except Exception as e:
                logger.warning(f"âš ï¸  Redis ë¡œë“œ ì‹¤íŒ¨ (Pineconeì—ì„œ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤): {e}")

        # ==========================================
        # 2. Pineconeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Slow Track)
        # ==========================================
        logger.info("â³ Pinecone ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ìµœì´ˆ 1íšŒ, ì•½ 20ë¶„ ì†Œìš”)...")
        (storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
         storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
         storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types) = fetch_titles_from_pinecone()
        logger.info(f"âœ… Pineconeì—ì„œ {len(storage.cached_titles)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        logger.info(f"   - HTML êµ¬ì¡° ìˆëŠ” ë¬¸ì„œ: {sum(1 for html in storage.cached_htmls if html)}ê°œ")
        logger.info(f"   - ì´ë¯¸ì§€ OCR ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'image')}ê°œ")
        logger.info(f"   - ì²¨ë¶€íŒŒì¼ ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'attachment')}ê°œ")

        # ==========================================
        # 3. Redisì— ì €ì¥ (ë‹¤ìŒ ì¬ì‹œì‘ì„ ìœ„í•´)
        # ==========================================
        if storage.redis_client is not None:
            try:
                cache_data = (
                    storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
                    storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
                    storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types
                )
                # 24ì‹œê°„ ìœ íš¨ (86400ì´ˆ)
                storage.redis_client.setex('pinecone_metadata', 86400, pickle.dumps(cache_data))
                logger.info("ğŸ’¾ ë°ì´í„°ë¥¼ Redisì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ë‹¤ìŒ ì¬ì‹œì‘ë¶€í„°ëŠ” 3ì´ˆ ë¡œë”©!)")
            except Exception as e:
                logger.warning(f"âš ï¸  Redis ì €ì¥ ì‹¤íŒ¨ (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©): {e}")
        else:
            logger.warning("âš ï¸  Redis ë¯¸ì‚¬ìš© (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©)")

        # Retriever ì´ˆê¸°í™”
        _initialize_retrievers()
        logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(storage.cached_titles)}, texts: {len(storage.cached_texts)})")

    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì•±ì´ í¬ë˜ì‹œí•˜ì§€ ì•Šë„ë¡ í•¨
        storage.cached_titles = []
        storage.cached_texts = []
        storage.cached_urls = []
        storage.cached_dates = []
        storage.cached_htmls = []
        storage.cached_content_types = []
        storage.cached_sources = []
        storage.cached_image_urls = []
        storage.cached_attachment_urls = []
        storage.cached_attachment_types = []
        logger.warning("âš ï¸  ìºì‹œë¥¼ ë¹ˆ ìƒíƒœë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")


def _initialize_retrievers():
    """Retriever ì´ˆê¸°í™” ë¡œì§ (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë¶„ë¦¬)"""
    logger.info("ğŸ”§ ê²€ìƒ‰ ì—”ì§„(BM25/Dense) êµ¬ì¶• ì¤‘...")

    from modules.retrieval import (
        BM25Retriever,
        DenseRetriever,
        DocumentCombiner,
        DocumentClusterer
    )

    # BM25Retriever ì´ˆê¸°í™” (HTML ë°ì´í„° í¬í•¨, Redis ìºì‹±)
    bm25_retriever = BM25Retriever(
        titles=storage.cached_titles,
        texts=storage.cached_texts,
        urls=storage.cached_urls,
        dates=storage.cached_dates,
        query_transformer=transformed_query,
        similarity_adjuster=adjust_similarity_scores,
        htmls=storage.cached_htmls,  # HTML êµ¬ì¡°í™” ë°ì´í„° ì¶”ê°€
        k1=1.5,
        b=0.75,
        redis_client=storage.redis_client  # Redis ìºì‹± í™œì„±í™”
    )
    storage.set_bm25_retriever(bm25_retriever)

    # DenseRetriever ì´ˆê¸°í™”
    dense_retriever = DenseRetriever(
        embeddings_factory=get_embeddings,
        pinecone_index=storage.pinecone_index,
        date_adjuster=adjust_date_similarity,
        similarity_scale=3.26,
        noun_weight=0.20,
        digit_weight=0.24
    )
    storage.set_dense_retriever(dense_retriever)

    # DocumentCombiner ì´ˆê¸°í™”
    document_combiner = DocumentCombiner(
        keyword_filter=last_filter_keyword,
        date_adjuster=adjust_date_similarity
    )
    storage.set_document_combiner(document_combiner)

    # DocumentClusterer ì´ˆê¸°í™”
    document_clusterer = DocumentClusterer(
        date_parser=parse_date_change_korea_time,
        similarity_threshold=0.89
    )
    storage.set_document_clusterer(document_clusterer)

    logger.info("âœ… ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")

                    #################################   24.11.16ê¸°ì¤€ ì •í™•ë„ ì¸¡ì •ì™„ë£Œ #####################################################
######################################################################################################################

# ë‚ ì§œë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
# ì´ì œëŠ” utils.date_utils.parse_date_change_korea_time ì‚¬ìš© ê¶Œì¥

def parse_date_change_korea_time(date_str):
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
    ISO 8601 í˜•ì‹ê³¼ ë ˆê±°ì‹œ í•œêµ­ì–´ í˜•ì‹ ëª¨ë‘ ì§€ì›

    Args:
        date_str: ISO 8601 í˜•ì‹ ë˜ëŠ” "ì‘ì„±ì¼25-10-17 15:48" í˜•ì‹

    Returns:
        datetime ê°ì²´ (í•œêµ­ ì‹œê°„ëŒ€)
    """
    # ë¹ˆ ë¬¸ìì—´ì´ë©´ None
    if not date_str:
        return None

    try:
        # ë¨¼ì € ISO 8601 í˜•ì‹ ì‹œë„ (ìƒˆ í˜•ì‹)
        dt = datetime.fromisoformat(date_str)
        # ì‹œê°„ëŒ€ê°€ ì—†ìœ¼ë©´ í•œêµ­ ì‹œê°„ëŒ€ ì¶”ê°€
        if dt.tzinfo is None:
            korea_timezone = pytz.timezone('Asia/Seoul')
            return korea_timezone.localize(dt)
        return dt
    except (ValueError, TypeError):
        pass

    try:
        # ë ˆê±°ì‹œ í•œêµ­ì–´ í˜•ì‹ ì‹œë„ (í•˜ìœ„ í˜¸í™˜ì„±)
        clean_date_str = date_str.replace("ì‘ì„±ì¼", "").strip()
        naive_date = datetime.strptime(clean_date_str, "%y-%m-%d %H:%M")
        # í•œêµ­ ì‹œê°„ëŒ€ ì¶”ê°€
        korea_timezone = pytz.timezone('Asia/Seoul')
        return korea_timezone.localize(naive_date)
    except (ValueError, TypeError):
        return None


def calculate_weight_by_days_difference(post_date, current_date, query_nouns):

    # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
    days_diff = (current_date - post_date).days

    # ê¸°ì¤€ ë‚ ì§œ (24-01-01 00:00) ì„¤ì •
    baseline_date_str = "24-01-01 00:00"
    baseline_date = parse_date_change_korea_time(baseline_date_str)
    graduate_weight = 1.0 if any(keyword in query_nouns for keyword in ['ì¡¸ì—…', 'ì¸í„°ë·°']) else 0
    scholar_weight = 1.0 if 'ì¥í•™' in query_nouns else 0
    # ì‘ì„±ì¼ì´ ê¸°ì¤€ ë‚ ì§œ ì´ì „ì´ë©´ ê°€ì¤‘ì¹˜ë¥¼ 1.35ë¡œ ê³ ì •
    if post_date <= baseline_date:
        return 1.35 + graduate_weight / 5

    # 'ìµœê·¼', 'ìµœì‹ ' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°, ìµœê·¼ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€
    add_recent_weight = 1.5 if any(keyword in query_nouns for keyword in ['ìµœê·¼', 'ìµœì‹ ', 'ì§€ê¸ˆ', 'í˜„ì¬']) else 0

    # **10ì¼ ë‹¨ìœ„ êµ¬ë¶„**: ìµœê·¼ ë¬¸ì„œì— ëŒ€í•œ ì„¸ë°€í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    if days_diff <= 6:
        return 1.355 + add_recent_weight + graduate_weight + scholar_weight
    elif days_diff <= 12:
        return 1.330 + add_recent_weight / 3.0 + graduate_weight / 1.2 + scholar_weight / 1.5
    elif days_diff <= 18:
        return 1.321 + add_recent_weight / 5.0 + graduate_weight / 1.3 + scholar_weight / 2.0
    elif days_diff <= 24:
        return 1.310 + add_recent_weight / 7.0 + graduate_weight / 1.4 + scholar_weight / 2.5
    elif days_diff <= 30:
        return 1.290 + add_recent_weight / 9.0 + graduate_weight / 1.5 + scholar_weight / 3.0
    elif days_diff <= 36:
        return 1.270 + graduate_weight / 1.6 + scholar_weight / 3.5
    elif days_diff <= 45:
        return 1.250 +graduate_weight / 1.7 + scholar_weight / 4.0
    elif days_diff <= 60:
        return 1.230 +graduate_weight / 1.8 + scholar_weight / 4.5
    elif days_diff <= 90:
        return 1.210 +graduate_weight / 2.0 + scholar_weight / 5.0

    # **ì›” ë‹¨ìœ„ êµ¬ë¶„**: 2ê°œì›” ì´í›„ëŠ” ì›” ë‹¨ìœ„ë¡œ ë‹¨ìˆœí™”
    month_diff = (days_diff - 90) // 30
    month_weight_map = {
        0: 1.19,
        1: 1.17 - add_recent_weight / 6 - scholar_weight / 10,
        2: 1.15 - add_recent_weight / 5 - scholar_weight / 9,
        3: 1.13 - add_recent_weight / 4 - scholar_weight / 7,
        4: 1.11 - add_recent_weight / 3  - scholar_weight / 5,
    }

    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë°˜í™˜ (6ê°œì›” ì´í›„)
    return month_weight_map.get(month_diff, 0.88 - add_recent_weight /2  - scholar_weight / 5)


# ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜

def adjust_date_similarity(similarity, date_str,query_nouns):
    # í˜„ì¬ í•œêµ­ ì‹œê°„
    current_time = get_korean_time()
    # ì‘ì„±ì¼ íŒŒì‹±
    post_date = parse_date_change_korea_time(date_str)
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    weight = calculate_weight_by_days_difference(post_date, current_time,query_nouns)
    # ì¡°ì •ëœ ìœ ì‚¬ë„ ë°˜í™˜
    return similarity * weight

# ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ ëª…ì‚¬ì™€ ê° ë¬¸ì„œ ì œëª©ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜
# (ì´ì „ ë²„ì „ì€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤ - ìµœì í™”ëœ ë²„ì „ë§Œ ìœ ì§€)

def adjust_similarity_scores(query_noun, title, texts, similarities):
    query_noun_set = set(query_noun)
    title_tokens = [set(titl.split()) for titl in title]

    for idx, titl_tokens in enumerate(title_tokens):
        matching_noun = query_noun_set.intersection(titl_tokens)
        
        if texts[idx] == "No content":
            similarities[idx] *= 1.5
            if "êµ­ê°€ì¥í•™ê¸ˆ" in query_noun_set and "êµ­ê°€ì¥í•™ê¸ˆ" in titl_tokens:
                similarities[idx] *= 5.0
        
        for noun in matching_noun:
            len_adjustment = len(noun) * 0.21
            similarities[idx] += len_adjustment
            if re.search(r'\d', noun):  # ìˆ«ì í¬í•¨ ì—¬ë¶€
                similarities[idx] += len(noun) * (0.22 if noun in titl_tokens else 0.19)

        if query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and titl_tokens.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}):
            similarities[idx] += 2.0
        if not query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and 'ëŒ€í•™ì›' in titl_tokens:
            similarities[idx] -= 2.0

    return similarities


#############################################################################################

# í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - KeywordFilter ì‚¬ìš©)
def last_filter_keyword(DOCS, query_noun, user_question):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§

    Args:
        DOCS: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ [(score, title, date, text, url), ...]
        query_noun: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
        user_question: ì›ë³¸ ì§ˆë¬¸

    Returns:
        List[Tuple]: í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ì¡°ì •ë¨)
    """
    return storage.keyword_filter.filter(DOCS, query_noun, user_question)

#################################################################################################

def find_url(url, title, doc_date, text, doc_url, number):
    return_docs = []
    for i, urls in enumerate(doc_url):
        if urls.startswith(url):  # indexsì™€ ì‹œì‘ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            return_docs.append((title[i], doc_date[i], text[i], doc_url[i]))
    
    # doc_url[i] ìˆœì„œëŒ€ë¡œ ì •ë ¬
    return_docs.sort(key=lambda x: x[3],reverse=True) 

    # ê³ ìœ  ìˆ«ìë¥¼ ì¶”ì í•˜ë©° numberê°œì˜ ë¬¸ì„œ ì„ íƒ
    unique_numbers = set()
    filtered_docs = []

    for doc in return_docs:
        # ìˆ«ìê°€ ì„œë¡œ ë‹¤ë¥¸ numberê°œê°€ ëª¨ì´ë©´ ì¢…ë£Œ
        if len(unique_numbers) >= number:
            break
        url_number = ''.join(filter(str.isdigit, doc[3]))  # URLì—ì„œ ìˆ«ì ì¶”ì¶œ
        unique_numbers.add(url_number)
        filtered_docs.append(doc)


    return filtered_docs


########################################################################################  best_docs ì‹œì‘ ##########################################################################################

def parse_temporal_intent(query, current_date=None):
    """
    ì§ˆë¬¸ì—ì„œ ì‹œê°„ í‘œí˜„ì„ ê°ì§€í•˜ê³  í•„í„° ì¡°ê±´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        current_date: í˜„ì¬ ë‚ ì§œ (ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°)

    Returns:
        dict: {"year": int, "semester": int, "date_from": datetime} ë˜ëŠ” None
    """
    from datetime import datetime

    if current_date is None:
        current_date = datetime.now()

    current_year = current_date.year
    current_month = current_date.month

    # í•œêµ­ í•™ê¸° ê³„ì‚°: 1í•™ê¸°(3-8ì›”), 2í•™ê¸°(9-2ì›”)
    # ë‹¨, 1-2ì›”ì€ ì „ë…„ë„ 2í•™ê¸°ë¡œ ê°„ì£¼
    if 3 <= current_month <= 8:
        current_semester = 1
    else:  # 9-12ì›” ë˜ëŠ” 1-2ì›”
        current_semester = 2
        if current_month <= 2:
            current_year -= 1  # 1-2ì›”ì€ ì „ë…„ë„ 2í•™ê¸°

    # 1ë‹¨ê³„: ê°„ë‹¨í•œ ì‹œê°„ í‘œí˜„ì€ ê·œì¹™ìœ¼ë¡œ ì²˜ë¦¬ (ë¹ ë¥´ê³  ë¹„ìš© 0)
    simple_temporal_keywords = {
        'ì´ë²ˆí•™ê¸°': {'year': current_year, 'semester': current_semester},
        'ì´ë²ˆ í•™ê¸°': {'year': current_year, 'semester': current_semester},
        'ì´ë²ˆí•™ë…„': {'year': current_year, 'semester': current_semester},
        'ì´ë²ˆ í•™ë…„': {'year': current_year, 'semester': current_semester},
        'ì˜¬í•´': {'year': current_year},
        'ê¸ˆë…„': {'year': current_year},
        'ìµœê·¼': {'year_from': current_year - 1},  # ìµœê·¼ 1ë…„
    }

    for keyword, time_filter in simple_temporal_keywords.items():
        if keyword in query:
            logger.info(f"â° ì‹œê°„ í‘œí˜„ ê°ì§€ (ê·œì¹™): '{keyword}' â†’ {time_filter}")
            return time_filter

    # 2ë‹¨ê³„: ë³µì¡í•œ ì‹œê°„ í‘œí˜„ì€ LLMìœ¼ë¡œ í•´ì„ (ìœ ì—°í•˜ê³  ì •í™•)
    # "ì €ë²ˆí•™ê¸°", "ì‘ë…„ 2í•™ê¸°", "ë‹¤ìŒ í•™ê¸°", "ì§€ë‚œë‹¬" ë“±
    complex_temporal_keywords = ['í•™ê¸°', 'í•™ë…„', 'ë…„ë„', 'ì‘ë…„', 'ì˜¬í•´', 'ë‚´ë…„', 'ì§€ë‚œ', 'ë‹¤ìŒ', 'ì „', 'í›„']

    if any(keyword in query for keyword in complex_temporal_keywords):
        logger.info(f"ğŸ¤” ë³µì¡í•œ ì‹œê°„ í‘œí˜„ ê°ì§€ â†’ LLM ë¦¬ë¼ì´íŒ… ì‹œì‘...")
        llm_filter = rewrite_query_with_llm(query, current_date)
        if llm_filter:
            logger.info(f"âœ¨ LLM ë¦¬ë¼ì´íŒ… ê²°ê³¼: {llm_filter}")
            return llm_filter

    return None


def rewrite_query_with_llm(query, current_date):
    """
    LLMì„ ì‚¬ìš©í•´ ë³µì¡í•œ ì‹œê°„ í‘œí˜„ì„ í•´ì„í•˜ê³  í•„í„° ì¡°ê±´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        current_date: í˜„ì¬ ë‚ ì§œ

    Returns:
        dict: {"year": int, "semester": int} ë˜ëŠ” None
    """
    from datetime import datetime
    import json

    current_year = current_date.year
    current_month = current_date.month

    # í˜„ì¬ í•™ê¸° ê³„ì‚°
    if 3 <= current_month <= 8:
        current_semester = 1
    else:
        current_semester = 2
        if current_month <= 2:
            current_year -= 1

    prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ í•™ì‚¬ ì¼ì • ì‹œê°„ í‘œí˜„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í˜„ì¬ ë‚ ì§œ: {current_date.strftime('%Yë…„ %mì›” %dì¼')}
í˜„ì¬ í•™ê¸°: {current_year}í•™ë…„ë„ {current_semester}í•™ê¸°

í•œêµ­ ëŒ€í•™ í•™ê¸° ê¸°ì¤€:
- 1í•™ê¸°: 3ì›”~8ì›”
- 2í•™ê¸°: 9ì›”~2ì›” (ë‹¤ìŒí•´ 2ì›”ê¹Œì§€)
- ì—¬ë¦„í•™ê¸°: 6ì›”~8ì›”
- ê²¨ìš¸í•™ê¸°: 12ì›”~2ì›”

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ìœ„ ì§ˆë¬¸ì—ì„œ ì‹œê°„ í‘œí˜„ì„ ì¶”ì¶œí•˜ê³ , ì •í™•í•œ í•™ë…„ë„ì™€ í•™ê¸°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSONë§Œ):
{{
  "year": 2025,
  "semester": 1,
  "reasoning": "í˜„ì¬ 2025ë…„ 2í•™ê¸°ì´ë¯€ë¡œ, ì €ë²ˆí•™ê¸°ëŠ” 2025ë…„ 1í•™ê¸°ì…ë‹ˆë‹¤"
}}

ì‹œê°„ í‘œí˜„ì´ ì—†ìœ¼ë©´:
{{
  "year": null,
  "semester": null,
  "reasoning": "ì‹œê°„ í‘œí˜„ ì—†ìŒ"
}}

ì˜ˆì‹œ:
- "ì €ë²ˆí•™ê¸°" â†’ {{"year": {current_year if current_semester == 2 else current_year - 1}, "semester": {2 if current_semester == 1 else 1}, "reasoning": "..."}}
- "ì‘ë…„ 2í•™ê¸°" â†’ {{"year": {current_year - 1}, "semester": 2, "reasoning": "..."}}
- "ë‹¤ìŒ í•™ê¸°" â†’ {{"year": {current_year + 1 if current_semester == 2 else current_year}, "semester": {1 if current_semester == 2 else 2}, "reasoning": "..."}}

**ì¤‘ìš”**: JSONë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""

    try:
        llm = ChatUpstage(api_key=storage.upstage_api_key, model="solar-pro")
        response = llm.invoke(prompt)

        # JSON íŒŒì‹±
        result = json.loads(response.content.strip())

        if result.get('year') is None or result.get('semester') is None:
            return None

        logger.info(f"   ğŸ’¬ LLM ì¶”ë¡ : {result.get('reasoning', '')}")

        return {
            'year': result['year'],
            'semester': result['semester']
        }

    except Exception as e:
        logger.warning(f"âš ï¸  LLM ë¦¬ë¼ì´íŒ… ì‹¤íŒ¨ (ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±): {e}")
        return None


def best_docs(user_question):
      # ì‚¬ìš©ì ì§ˆë¬¸
      noun_time=time.time()
      query_noun=transformed_query(user_question)
      query_noun_time=time.time()-noun_time
      print(f"ëª…ì‚¬í™” ë³€í™˜ ì‹œê°„ : {query_noun_time}")
      titles_from_pinecone, texts_from_pinecone, urls_from_pinecone, dates_from_pinecone = storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates
      if not query_noun:
        return None,None
      #######  ìµœê·¼ ê³µì§€ì‚¬í•­, ì±„ìš©, ì„¸ë¯¸ë‚˜, í–‰ì‚¬, íŠ¹ê°•ì˜ ë‹¨ìˆœí•œ ì •ë³´ë¥¼ ìš”êµ¬í•˜ëŠ” ê²½ìš°ë¥¼ í•„í„°ë§ í•˜ê¸° ìœ„í•œ ë§¤ì»¤ë‹ˆì¦˜ ########
      remove_noticement = ['ëª©ë¡','ë¦¬ìŠ¤íŠ¸','ë‚´ìš©','ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì—…ë°ì´íŠ¸',
                           'ì»´í“¨í„°í•™ë¶€','ì»´í•™','ìƒìœ„','ì •ë³´','ê´€ë ¨','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ ','ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬']
      query_nouns = [noun for noun in query_noun if noun not in remove_noticement]
      return_docs=[]
      key=None
      numbers=5 ## ê¸°ë³¸ìœ¼ë¡œ 5ê°œ ë¬¸ì„œ ë°˜í™˜í•  ê²ƒ.
      check_num=0
      recent_time=time.time()
      for noun in query_nouns:
        if 'ê°œ' in noun:
            # ìˆ«ì ì¶”ì¶œ
            num = re.findall(r'\d+', noun)
            if num:
                numbers=int(num[0])
                check_num=1
      if (any(keyword in query_noun for keyword in ['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ '])and any(keyword in query_noun for keyword in ['ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬'])and len(query_nouns)<1 or check_num==1):    
        if numbers ==0:
          #### 0ê°œì˜ keywordì— ëŒ€í•´ì„œ ì§ˆë¬¸í•œë‹¤ë©´? ex) ê°€ì¥ ìµœê·¼ ê³µì§€ì‚¬í•­ 0ê°œ ì•Œë ¤ì¤˜######
          keys=['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©']
          return None,[keyword for keyword in keys if keyword in user_question]
        if 'ê³µì§€ì‚¬í•­' in query_noun:
          key=['ê³µì§€ì‚¬í•­']
          notice_url = NOTICE_BASE_URL + "&wr_id="
          return_docs=find_url(notice_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        if 'ì±„ìš©' in query_noun:
          key=['ì±„ìš©']
          company_url = COMPANY_BASE_URL + "&wr_id="
          return_docs=find_url(company_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        other_key = ['ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°']
        if any(keyword in query_noun for keyword in other_key):
          seminar_url = SEMINAR_BASE_URL + "&wr_id="
          key = [keyword for keyword in other_key if keyword in user_question]
          return_docs=find_url(seminar_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        recent_finish_time=time.time()-recent_time
        print(f"ìµœê·¼ ê³µì§€ì‚¬í•­ ë¬¸ì„œ ë½‘ëŠ” ì‹œê°„ {recent_finish_time}")
        if (len(return_docs)>0):
          return return_docs,key


      remove_noticement = ['ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì»´í•™','ìƒìœ„','ê´€ë ¨']

      # âœ… ì‹œê°„ í‘œí˜„ ê°ì§€ ë° í•„í„° ìƒì„±
      temporal_filter = parse_temporal_intent(user_question)

      # BM25 ê²€ìƒ‰ (ë¦¬íŒ©í† ë§ë¨ - BM25Retriever ì‚¬ìš©)
      bm_title_time = time.time()
      Bm25_best_docs, adjusted_similarities = storage.bm25_retriever.search(
          query_nouns=query_noun,
          top_k=50,  # âœ¨ 25â†’50 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ìœ„í•œ í›„ë³´êµ° í™•ëŒ€
          normalize_factor=24.0
      )
      bm_title_f_time = time.time() - bm_title_time
      print(f"bm25 ë¬¸ì„œ ë½‘ëŠ”ì‹œê°„: {bm_title_f_time}")
      ####################################################################################################
      # Dense Retrieval (ë¦¬íŒ©í† ë§ë¨ - DenseRetriever ì‚¬ìš©)
      dense_time = time.time()
      combine_dense_docs = storage.dense_retriever.search(
          user_question=user_question,
          query_nouns=query_noun,
          top_k=50  # âœ¨ 30â†’50 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ìœ„í•œ í›„ë³´êµ° í™•ëŒ€
      )
      pinecone_time = time.time() - dense_time
      print(f"íŒŒì¸ì½˜ì—ì„œ top k ë½‘ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ {pinecone_time}")

      # âœ… ì‹œê°„ í•„í„° ì ìš© (ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§)
      if temporal_filter:
          from datetime import datetime

          def matches_temporal_filter(doc_date_str, time_filter):
              """ë‚ ì§œ ë¬¸ìì—´ì´ ì‹œê°„ í•„í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸"""
              try:
                  # ISO í¬ë§· íŒŒì‹±: "2024-09-19T10:57:00+09:00"
                  doc_date = datetime.fromisoformat(doc_date_str.replace('+09:00', ''))
                  doc_year = doc_date.year
                  doc_month = doc_date.month

                  # í•™ê¸° ê³„ì‚°
                  if 3 <= doc_month <= 8:
                      doc_semester = 1
                  else:
                      doc_semester = 2
                      if doc_month <= 2:
                          doc_year -= 1

                  # í•„í„° ì¡°ê±´ ì²´í¬
                  if 'year' in time_filter and doc_year != time_filter['year']:
                      return False
                  if 'semester' in time_filter and doc_semester != time_filter['semester']:
                      return False
                  if 'year_from' in time_filter and doc_year < time_filter['year_from']:
                      return False

                  return True
              except Exception as e:
                  logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {doc_date_str} - {e}")
                  return True  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨ (ì•ˆì „ì¥ì¹˜)

          # BM25 ê²°ê³¼ í•„í„°ë§
          original_bm25_count = len(Bm25_best_docs)
          Bm25_best_docs = [
              (title, date, text, url)
              for title, date, text, url in Bm25_best_docs
              if matches_temporal_filter(date, temporal_filter)
          ]

          # Dense ê²°ê³¼ í•„í„°ë§
          original_dense_count = len(combine_dense_docs)
          combine_dense_docs = [
              (score, doc)
              for score, doc in combine_dense_docs
              if matches_temporal_filter(doc[1], temporal_filter)  # doc[1] = date
          ]

          logger.info(f"ğŸ“… ë‚ ì§œ í•„í„°ë§ ì™„ë£Œ: BM25 {original_bm25_count}â†’{len(Bm25_best_docs)}ê°œ, Dense {original_dense_count}â†’{len(combine_dense_docs)}ê°œ")

      # ## ê²°ê³¼ ì¶œë ¥
      # print("\ní†µí•©ëœ íŒŒì¸ì½˜ë¬¸ì„œ ìœ ì‚¬ë„:")
      # for score, doc in combine_dense_docs:
      #     title, date, text, url = doc
      #     print(f"ì œëª©: {title}\nìœ ì‚¬ë„: {score} {url}")
      #     print('---------------------------------')


      #################################################3#################################################3
      #####################################################################################################3

      # BM25ì™€ Dense Retrieval ê²°ê³¼ ê²°í•© (ë¦¬íŒ©í† ë§ë¨ - DocumentCombiner ì‚¬ìš©)
      combine_time = time.time()
      final_best_docs = storage.document_combiner.combine(
          dense_results=combine_dense_docs,
          bm25_results=Bm25_best_docs,
          bm25_similarities=adjusted_similarities,
          titles_from_pinecone=titles_from_pinecone,
          query_nouns=query_noun,
          user_question=user_question,
          top_k=30  # âœ¨ 20â†’30 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ì „ í›„ë³´êµ° í™•ëŒ€
      )
      combine_f_time = time.time() - combine_time
      print(f"Bm25ë‘ pinecone ê²°í•© ì‹œê°„: {combine_f_time}")

      # âœ… ë‚ ì§œ ë¶€ìŠ¤íŒ… (Recency Boost) - ì‹œê°„ í‘œí˜„ ì—†ì–´ë„ ìµœì‹  ë¬¸ì„œ ìš°ì„ !
      # ì‚¬ìš©ì ì§€ì : "ì‹œê°„ ë§¥ë½ ì—†ìœ¼ë©´ ë‹¹ì—°íˆ ìµœì‹ ìˆœìœ¼ë¡œ"
      from datetime import datetime

      def calculate_recency_boost(doc_date_str):
          """ë¬¸ì„œ ë‚ ì§œì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê³„ì‚° (ìµœì‹  ë¬¸ì„œ ìš°ì„ )"""
          try:
              current_date = datetime.now()
              doc_date = datetime.fromisoformat(doc_date_str.replace('+09:00', ''))

              # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
              days_old = (current_date - doc_date).days

              # ê°€ì¤‘ì¹˜ ê³„ì‚°
              if days_old < 0:  # ë¯¸ë˜ ë‚ ì§œ (ì˜¤ë¥˜)
                  return 1.0
              elif days_old <= 180:  # 6ê°œì›” ì´ë‚´ (ì´ë²ˆí•™ê¸°/ì €ë²ˆí•™ê¸°)
                  return 1.5  # 50% ë¶€ìŠ¤íŒ…
              elif days_old <= 365:  # 1ë…„ ì´ë‚´ (ì‘ë…„)
                  return 1.3  # 30% ë¶€ìŠ¤íŒ…
              elif days_old <= 730:  # 2ë…„ ì´ë‚´
                  return 1.1  # 10% ë¶€ìŠ¤íŒ…
              else:  # 2ë…„ ì´ìƒ
                  return 0.9  # 10% íŒ¨ë„í‹°

          except Exception as e:
              logger.debug(f"ë‚ ì§œ ë¶€ìŠ¤íŒ… ê³„ì‚° ì‹¤íŒ¨: {doc_date_str} - {e}")
              return 1.0  # ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½

      # ê²°í•©ëœ ê²°ê³¼ì— ë‚ ì§œ ë¶€ìŠ¤íŒ… ì ìš©
      boosted_docs = []
      for score, title, date, text, url in final_best_docs:
          boost = calculate_recency_boost(date)
          boosted_score = score * boost
          boosted_docs.append((boosted_score, title, date, text, url))

      # ë¶€ìŠ¤íŒ…ëœ ì ìˆ˜ë¡œ ì¬ì •ë ¬
      boosted_docs.sort(key=lambda x: x[0], reverse=True)
      final_best_docs = boosted_docs

      logger.info(f"ğŸš€ ë‚ ì§œ ë¶€ìŠ¤íŒ… ì™„ë£Œ (ìµœì‹  ë¬¸ì„œ ìš°ì„ : 6ê°œì›” ì´ë‚´ +50%, 1ë…„ ì´ë‚´ +30%)")

      # âœ¨ URL ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê°™ì€ ê²Œì‹œê¸€ì˜ ì„œë¡œ ë‹¤ë¥¸ ì²­í¬ ì œê±°)
      # ëª©ì : ê²€ìƒ‰ ê²°ê³¼ ë‹¤ì–‘ì„± í™•ë³´ (Top Nì´ ëª¨ë‘ ì„œë¡œ ë‹¤ë¥¸ ê²Œì‹œê¸€ì´ ë˜ë„ë¡)
      # ì „ëµ: ê°™ì€ URL(ê²Œì‹œê¸€)ì—ì„œ ìµœê³  ì ìˆ˜ ì²­í¬ë§Œ ì„ íƒ
      # íš¨ê³¼:
      #   - BGE-Reranker íš¨ìœ¨ì„± í–¥ìƒ (ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ì¬ì •ë ¬)
      #   - ë¡œê·¸ ê°€ë…ì„± í–¥ìƒ (ë‹¤ì–‘ì„± ì§€í‘œ ê°œì„ )
      #   - í–¥í›„ í™•ì¥ ëŒ€ë¹„ (ë³µìˆ˜ ë‹µë³€, ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ ë“±)
      dedup_time = time.time()

      seen_urls = {}  # {url: (score, title, date, text, url)}
      deduplicated_docs = []
      duplicate_count = 0
      original_count = len(final_best_docs)

      for score, title, date, text, url in final_best_docs:
          if url in seen_urls:
              # ê°™ì€ URLì´ ì´ë¯¸ ìˆìŒ â†’ ì ìˆ˜ ë¹„êµ
              existing_score = seen_urls[url][0]

              if score > existing_score:
                  # ë” ë†’ì€ ì ìˆ˜ë©´ ê¸°ì¡´ ë¬¸ì„œ ì œê±°í•˜ê³  ìƒˆ ë¬¸ì„œ ì¶”ê°€
                  deduplicated_docs.remove(seen_urls[url])
                  deduplicated_docs.append((score, title, date, text, url))
                  seen_urls[url] = (score, title, date, text, url)
                  logger.debug(f"ğŸ”„ URL ì¤‘ë³µ - ë” ë†’ì€ ì ìˆ˜ë¡œ êµì²´: {title[:30]}... ({existing_score:.2f} â†’ {score:.2f})")
              else:
                  # ë‚®ì€ ì ìˆ˜ë©´ ë¬´ì‹œ
                  duplicate_count += 1
                  logger.debug(f"â­ï¸  URL ì¤‘ë³µ ì œê±°: {title[:30]}... (ì ìˆ˜: {score:.2f} < {existing_score:.2f})")
          else:
              # ìƒˆ URLì´ë©´ ì¶”ê°€
              seen_urls[url] = (score, title, date, text, url)
              deduplicated_docs.append((score, title, date, text, url))

      # ì ìˆ˜ìˆœ ì¬ì •ë ¬ í›„ Top 20
      deduplicated_docs.sort(key=lambda x: x[0], reverse=True)
      final_best_docs = deduplicated_docs[:20]

      dedup_f_time = time.time() - dedup_time
      unique_urls = len(seen_urls)
      print(f"URL ì¤‘ë³µ ì œê±°: {dedup_f_time:.4f}ì´ˆ (ì›ë³¸: {original_count}ê°œ â†’ ì¤‘ë³µ {duplicate_count}ê°œ ì œê±° â†’ ìµœì¢…: {len(final_best_docs)}ê°œ ì„œë¡œ ë‹¤ë¥¸ ê²Œì‹œê¸€, ê³ ìœ  URL {unique_urls}ê°œ)")

      # í´ëŸ¬ìŠ¤í„°ë§ ì œê±°: URL ì¤‘ë³µ ì œê±°ë§Œìœ¼ë¡œ ì¶©ë¶„ (ê° ê²Œì‹œê¸€ë‹¹ ëŒ€í‘œ ì²­í¬ 1ê°œ ì„ íƒ ì™„ë£Œ)
      # get_ai_message()ì—ì„œ ìµœì¢… ì„ íƒëœ ë¬¸ì„œì˜ ì „ì²´ ì²­í¬ë¥¼ ë‹¤ì‹œ ìˆ˜ì§‘í•˜ë¯€ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆí•„ìš”
      return final_best_docs, query_noun

prompt_template = """ë‹¹ì‹ ì€ ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ ê³µì§€ì‚¬í•­ì„ ì „ë‹¬í•˜ëŠ” ì§ì›ì´ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì •í™•í•˜ê²Œ ì „ë‹¬í•´ì•¼ í•  ì˜ë¬´ê°€ ìˆìŠµë‹ˆë‹¤.
í˜„ì¬ í•œêµ­ ì‹œê°„: {current_time}

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:

1. **ì‹œê°„ ë¹„êµ ë° ëª…ì‹œ (ë§¤ìš° ì¤‘ìš”!):**
  - ì§ˆë¬¸ì— ì‹œê°„ í‘œí˜„(ì´ë²ˆí•™ê¸°, ì˜¬í•´ ë“±)ì´ ì—†ë”ë¼ë„, ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚ ì§œì™€ í˜„ì¬ ì‹œê°„ì„ ë¹„êµí•˜ì„¸ìš”.
  - ë¬¸ì„œê°€ ì˜¬í•´ê°€ ì•„ë‹ˆë¼ë©´ **ë°˜ë“œì‹œ ëª…ì‹œ**í•˜ì„¸ìš”. ì˜ˆ: "âš ï¸ ì£¼ì˜: ì´ ì •ë³´ëŠ” 2024ë…„ ìë£Œì…ë‹ˆë‹¤."
  - ì´ë²¤íŠ¸ ê¸°ê°„ ë¹„êµ:
    * "2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì¼ì •ì€ ì–¸ì œì•¼?" (í˜„ì¬ 11ì›”) â†’ "2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ì€ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (8ì›”). ì¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì•˜ìŠµë‹ˆë‹¤: ..."
    * "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì€ ì–¸ì œì•¼?" (í˜„ì¬ 11ì›” 12ì¼, ì‹ ì²­ 11ì›” 13ì¼) â†’ "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ì€ ë‚´ì¼(11ì›” 13ì¼)ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤."
    * "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì€ ì–¸ì œì•¼?" (í˜„ì¬ 11ì›” 13ì¼, ì‹ ì²­ 11ì›” 13ì¼) â†’ "í˜„ì¬ ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì…ë‹ˆë‹¤ (11ì›” 13ì¼ë¶€í„°)."

  - **ê³¼ê±° ë°ì´í„° ì‚¬ìš© ì‹œ ê²½ê³ :**
    * ë¬¸ì„œê°€ ì‘ë…„(2024ë…„) ê²ƒì´ë©´: "âš ï¸ ì£¼ì˜: 2025ë…„ ìë£Œê°€ ì—†ì–´ 2024ë…„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìµœì‹  ì •ë³´ëŠ” ê³µì§€ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”."
    * ë¬¸ì„œê°€ 2ë…„ ì´ìƒ ì˜¤ë˜ëìœ¼ë©´: "âš ï¸ ì£¼ì˜: ì´ ì •ë³´ëŠ” 20XXë…„ ìë£Œë¡œ ì˜¤ë˜ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì‹  ì •ë³´ëŠ” ê³µì§€ì‚¬í•­ì„ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”."
2. ì§ˆë¬¸ì—ì„œ í•µì‹¬ì ì¸ í‚¤ì›Œë“œë“¤ì„ ê³¨ë¼ í‚¤ì›Œë“œë“¤ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„ì„œ í•´ë‹¹ ë¬¸ì„œë¥¼ ì½ê³  ì •í™•í•œ ë‚´ìš©ì„ ë‹µë³€í•´ì£¼ì„¸ìš”.
3. **ë‹µë³€ ì™„ì „ì„± vs ê°„ê²°ì„± (ë§¤ìš° ì¤‘ìš”!):**
   - **âš ï¸ ì™„ì „ì„± ìš”êµ¬ í‚¤ì›Œë“œ**: ì§ˆë¬¸ì— "ì „ë¶€", "ëª¨ë“ ", "ëª¨ë‘", "ë¹ ì§ì—†ì´", "ì „ì²´", "ë‹¤", "ëª…ë‹¨", "ëª©ë¡", "ë¦¬ìŠ¤íŠ¸", "ëˆ„êµ¬" ë“±ì´ í¬í•¨ë˜ë©´, ë¬¸ì„œì˜ **ëª¨ë“  í•­ëª©ì„ ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ê³  ì „ë¶€ ë‚˜ì—´**í•˜ì„¸ìš”.
     * ì˜ˆ: "ë©´ì ‘ ëŒ€ìƒì **ì „ë¶€** ì•Œë ¤ì¤˜" â†’ ë¬¸ì„œì— ìˆëŠ” **ëª¨ë“ ** ëŒ€ìƒìë¥¼ 1ëª…ë„ ë¹ ì§ì—†ì´ ë‚˜ì—´ (ìš”ì•½ ê¸ˆì§€!)
     * ì˜ˆ: "ì¥í•™ê¸ˆ ìˆ˜í˜œì **ëˆ„êµ¬**ë‹ˆ?" â†’ **ëª¨ë“ ** ìˆ˜í˜œì ì´ë¦„ê³¼ í•™ë²ˆì„ ì „ë¶€ ì œê³µ (ì¼ë¶€ë§Œ ì œê³µ ê¸ˆì§€!)
     * ì˜ˆ: "**ëª¨ë“ ** íŠœí„° ì•Œë ¤ì¤˜" â†’ ë¬¸ì„œì˜ **ëª¨ë“ ** íŠœí„°ë¥¼ ë¹ ì§ì—†ì´ ë‚˜ì—´
     * **ì ˆëŒ€ ê·œì¹™**: ì™„ì „ì„± í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ "...ë“±", "ì¼ë¶€", "ì£¼ìš”" ê°™ì€ ìš”ì•½ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€. ë¬¸ì„œì˜ ë§ˆì§€ë§‰ í•­ëª©ê¹Œì§€ ì „ë¶€ ë‚˜ì—´í•  ê²ƒ!
   - **ì¼ë°˜ ì§ˆë¬¸**: ì™„ì „ì„± í‚¤ì›Œë“œê°€ ì—†ëŠ” ì¼ë°˜ ì§ˆë¬¸ì€ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
     * ì˜ˆ: "ìˆ˜ê°•ì‹ ì²­ ë°©ë²•ì€?" â†’ í•µì‹¬ ì ˆì°¨ë§Œ ê°„ê²°í•˜ê²Œ ì„¤ëª…
4. ì—ì´ë¹…ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ì„ì˜ë¡œ íŒë‹¨í•´ì„œ ë„¤ ì•„ë‹ˆì˜¤ í•˜ì§€ ë§ê³  ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.
5. ë‹µë³€ì€ ì¹œì ˆí•˜ê²Œ ì¡´ëŒ“ë§ë¡œ ì œê³µí•˜ì„¸ìš”.
6. ì§ˆë¬¸ì´ ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ë‹¤ê³  íŒë‹¨í•˜ë©´ ì‘ë‹µí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ë©´ "ë„ˆëŠ” ë¬´ì—‡ì„ ì•Œê¹Œ", "ì ì‹¬ë©”ë‰´ ì¶”ì²œ"ê³¼ ê°™ì´ ì¼ë°˜ ìƒì‹ì„ ìš”êµ¬í•˜ëŠ” ì§ˆë¬¸ì€ ê±°ì ˆí•´ì£¼ì„¸ìš”.
7. ì—ì´ë¹… ì¸ì • ê´€ë ¨ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ê³„ì ˆí•™ê¸°ì¸ì§€ ê·¸ëƒ¥ í•™ê¸°ë¥¼ ë¬»ëŠ”ê²ƒì¸ì§€ ì§ˆë¬¸ì„ ì²´í¬í•´ì•¼í•©ë‹ˆë‹¤. ê³„ì ˆí•™ê¸°ê°€ ì•„ë‹Œ ê²½ìš°ì— ì‹¬ì»´,ê¸€ì†,ì¸ì»´ ê°œì„¤ì´ ì•„ë‹ˆë©´ ì—ì´ë¹… ì¸ì •ì´ ì•ˆë©ë‹ˆë‹¤.

**ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ í™œìš© ê°€ì´ë“œ:**
8. ì»¨í…ìŠ¤íŠ¸ì— HTML í‘œ(<table>, <tr>, <td> ë“±) ë˜ëŠ” Markdown í‘œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, í‘œ êµ¬ì¡°ë¥¼ ì •í™•íˆ íŒŒì‹±í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
  - ì˜ˆì‹œ: <tr><td>ì„±ì ìš°ìˆ˜ì¥í•™ê¸ˆ</td><td>300ë§Œì›</td></tr>ëŠ” "ì„±ì ìš°ìˆ˜ì¥í•™ê¸ˆ: 300ë§Œì›"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
  - Markdown í‘œ ì˜ˆì‹œ: "| ê³¼ëª© | íŠœí„° | ê°•ì˜ì‹¤ |\n| ì•Œê³ ë¦¬ì¦˜2 | ìµœê¸°ì˜ | IT5-224 |"ëŠ” "ì•Œê³ ë¦¬ì¦˜2ì˜ íŠœí„°ëŠ” ìµœê¸°ì˜, ê°•ì˜ì‹¤ì€ IT5-224"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
  - í‘œì˜ í–‰(row)ê³¼ ì—´(column) ê´€ê³„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
9. ì»¨í…ìŠ¤íŠ¸ ì¶œì²˜ ë¼ë²¨([ë³¸ë¬¸], [ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸], [ì²¨ë¶€íŒŒì¼: PDF])ì„ ì°¸ê³ í•˜ì—¬ ì •ë³´ì˜ ì‹ ë¢°ë„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.
  - [ë³¸ë¬¸]: ì›ë³¸ ê²Œì‹œê¸€ í…ìŠ¤íŠ¸ (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ)
  - [ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸]: ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ (OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„± ê³ ë ¤)
  - [ì²¨ë¶€íŒŒì¼: PDF/HWP/DOCX]: ì²¨ë¶€íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ë° êµ¬ì¡° (ê³µì‹ ë¬¸ì„œë¡œ ì‹ ë¢°ë„ ë†’ìŒ)
10. HTML ë¦¬ìŠ¤íŠ¸(<ul>, <ol>, <li>)ë‚˜ ì¤‘ì²© êµ¬ì¡°ê°€ ìˆìœ¼ë©´, ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

**ê¹¨ì§„ ë°ì´í„° ì²˜ë¦¬ ê°€ì´ë“œ (ê´€ëŒ€í•˜ê²Œ í•´ì„):**
11. HTML/Markdown í‘œê°€ ì¼ë¶€ ì†ìƒë˜ì—ˆê±°ë‚˜ ë¶ˆì™„ì „í•œ ê²½ìš°:
  - í‘œ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ìœ ì¶”í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
  - ì˜ˆì‹œ: "| ê³¼ëª© | íŠœí„° | ê°•ì˜ì‹¤\nì•Œê³ ë¦¬ì¦˜2 ìµœê¸°ì˜ IT5-224" â†’ êµ¬ë¶„ìê°€ ì¼ë¶€ ëˆ„ë½ë˜ì–´ë„ ë¬¸ë§¥ìƒ "ì•Œê³ ë¦¬ì¦˜2ì˜ íŠœí„°ëŠ” ìµœê¸°ì˜, ê°•ì˜ì‹¤ì€ IT5-224"ë¡œ í•´ì„
12. OCR í…ìŠ¤íŠ¸ì˜ ì˜¤íƒ€ë‚˜ ëˆ„ë½ì´ ìˆì„ ê²½ìš°:
  - ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ìœ ì¶”í•˜ì„¸ìš”.
  - ì˜ˆì‹œ: "ìµœ7ã…£ì˜" â†’ "ìµœê¸°ì˜", "IT5äºŒ224" â†’ "IT5-224", "T0T0R" â†’ "TUTOR"
  - ë¹„ìŠ·í•œ í˜•íƒœì˜ ë¬¸ìê°€ ì˜ëª» ì¸ì‹ëœ ê²½ìš° (ìˆ«ì/í•œê¸€/ì˜ë¬¸ í˜¼ë™) ì˜¬ë°”ë¥´ê²Œ í•´ì„í•˜ì„¸ìš”.
13. í‘œì˜ í—¤ë”ì™€ ë°ì´í„°ê°€ ì„ì—¬ìˆê±°ë‚˜ ì¤„ë°”ê¿ˆì´ ëˆ„ë½ëœ ê²½ìš°:
  - íŒ¨í„´ì„ íŒŒì•…í•˜ì—¬ ì •ë³´ë¥¼ ì¬êµ¬ì„±í•˜ì„¸ìš”.
  - ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ë¬¸ì„œê°€ ì¼ë¶€ ì†ìƒë˜ì–´ ì •í™•í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì°¸ê³  URLì„ í™•ì¸í•˜ì„¸ìš”."ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
ë‹µë³€:"""

# PromptTemplate ê°ì²´ ìƒì„±
PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["current_time", "context", "question"]
)

def format_docs(docs):
    """
    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
    ì¶œì²˜(ì›ë³¸/ì´ë¯¸ì§€OCR/ì²¨ë¶€íŒŒì¼)ë¥¼ ë¼ë²¨ë¡œ í‘œì‹œí•˜ì—¬ ë§¥ë½ ì œê³µ
    ê° ì²­í¬ì— ì œëª© ì •ë³´ë¥¼ ëª…ì‹œí•˜ì—¬ ë¬¸ë§¥ ë‹¨ì ˆ(Context Fragmentation) ë¬¸ì œ í•´ê²°

    Args:
        docs: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸

    Returns:
        str: í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    formatted = []

    for doc in docs:
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œëª© ì¶”ì¶œ
        title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')

        # ì¶œì²˜ì— ë”°ë¼ ë¼ë²¨ ìƒì„±
        source = doc.metadata.get('source', 'original_post')
        content_type = doc.metadata.get('content_type', 'text')

        if source == "image_ocr":
            label = "[ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸]"
        elif source == "document_parse":
            # ì²¨ë¶€íŒŒì¼ íƒ€ì… í‘œì‹œ
            attachment_type = doc.metadata.get('attachment_type', 'document')
            label = f"[ì²¨ë¶€íŒŒì¼: {attachment_type.upper()}]"
        else:
            # ì›ë³¸ ê²Œì‹œê¸€
            label = "[ë³¸ë¬¸]"

        # ì œëª© + ë¼ë²¨ + ë‚´ìš© (ì œëª©ì„ ëª…ì‹œí•˜ì—¬ ì²­í¬ì˜ ë¬¸ë§¥ ì œê³µ)
        formatted.append(f"ë¬¸ì„œ ì œëª©: {title}\n{label}\n{doc.page_content}")

    return "\n\n".join(formatted)


def get_answer_from_chain(best_docs, user_question,query_noun):

    # âœ… best_docsì—ì„œ ë©”íƒ€ë°ì´í„° ì§ì ‘ ì¶”ì¶œ (URLë¡œ ë‹¤ì‹œ ì°¾ì§€ ì•ŠìŒ)
    documents = []
    for doc in best_docs:
        score = doc[0]
        title = doc[1]
        date = doc[2]
        text = doc[3]
        url = doc[4]
        # âœ… ë©”íƒ€ë°ì´í„°ë¥¼ tupleì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´ (ë²„ê·¸ ìˆ˜ì •!)
        html = doc[5] if len(doc) > 5 else ""
        content_type = doc[6] if len(doc) > 6 else "text"
        source = doc[7] if len(doc) > 7 else "original_post"
        attachment_type = doc[8] if len(doc) > 8 else ""

        # HTML/Markdown ìš°ì„  ì‚¬ìš© (í‘œ êµ¬ì¡° ë³´ì¡´), ì—†ìœ¼ë©´ text ì‚¬ìš©
        if html:
            # Markdown í˜•ì‹ ê°ì§€ (Upstage API ì œê³µ, ê³ í’ˆì§ˆ í‘œ êµ¬ì¡°)
            # ì´ë¯¸ Markdownì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í† í° íš¨ìœ¨ì , LLM ìµœì í™”)
            if '|' in html and ('---' in html or '\n' in html):
                # Markdown í‘œ í˜•ì‹
                page_content = html
            else:
                # HTML â†’ Markdown ë³€í™˜ (fallback)
                try:
                    soup = BeautifulSoup(html, 'html.parser')

                    # í…Œì´ë¸”ì´ ìˆìœ¼ë©´ Markdown í‘œë¡œ ë³€í™˜
                    markdown_content = ""
                    for table in soup.find_all('table'):
                        markdown_content += "\n\n**[í‘œ ë°ì´í„°]**\n"
                        rows = table.find_all('tr')
                        for row_idx, row in enumerate(rows):
                            cells = row.find_all(['th', 'td'])
                            row_text = " | ".join([cell.get_text(strip=True) for cell in cells])
                            markdown_content += f"| {row_text} |\n"
                            # í—¤ë” í–‰ ë‹¤ìŒì— êµ¬ë¶„ì„  ì¶”ê°€
                            if row_idx == 0:
                                markdown_content += "| " + " | ".join(["---"] * len(cells)) + " |\n"
                        markdown_content += "\n"

                    # í…Œì´ë¸” ì™¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    for table in soup.find_all('table'):
                        table.decompose()  # í…Œì´ë¸” ì œê±° (ì¤‘ë³µ ë°©ì§€)

                    plain_text_from_html = soup.get_text(separator='\n', strip=True)

                    # ìµœì¢… page_content: Markdown í‘œ + í‰ë¬¸
                    page_content = (markdown_content + "\n" + plain_text_from_html).strip()

                    # ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì›ë³¸ text ì‚¬ìš©
                    if not page_content:
                        page_content = text
                except Exception as e:
                    logger.debug(f"HTML ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
                    page_content = text
        else:
            page_content = text

        # ë‚ ì§œ íŒŒì‹± (ISO 8601ê³¼ ë ˆê±°ì‹œ í˜•ì‹ ëª¨ë‘ ì§€ì›)
        try:
            if date.startswith("ì‘ì„±ì¼"):
                doc_date = datetime.strptime(date, 'ì‘ì„±ì¼%y-%m-%d %H:%M')
            else:
                doc_date = datetime.fromisoformat(date)
        except:
            doc_date = datetime.now()

        # Document ê°ì²´ ìƒì„± (ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° í¬í•¨)
        doc = Document(
            page_content=page_content,  # HTML ìš°ì„ , ì—†ìœ¼ë©´ text
            metadata={
                "title": title,
                "url": url,
                "doc_date": doc_date,
                "content_type": content_type,
                "source": source,
                "attachment_type": attachment_type,
                "plain_text": text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ë³´ê´€
            }
        )
        documents.append(doc)

    # âœ… ê°œì„ ëœ í•„í„°ë§: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ vs í‚¤ì›Œë“œ í•„í„°ë§
    # í•µì‹¬ ê°œì„ : ê°™ì€ ê²Œì‹œê¸€ì—ì„œ ìˆ˜ì§‘ëœ ì²­í¬ë“¤ì€ ì´ë¯¸ BM25 + Dense + Rerankerë¡œ ê²€ì¦ë¨
    # â†’ í‚¤ì›Œë“œ í•„í„°ë§ìœ¼ë¡œ ì¤‘ìš” ì •ë³´(ì´ë¦„, í•™ë²ˆ ë“±)ë¥¼ ë‹´ì€ ì²­í¬ê°€ ì œê±°ë˜ëŠ” ë¬¸ì œ í•´ê²°

    # ëª¨ë“  ë¬¸ì„œê°€ ê°™ì€ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸ (ì œëª© ê¸°ì¤€)
    unique_titles = set(doc.metadata.get('title', '') for doc in documents)

    if len(unique_titles) == 1:
        # âœ… ê°™ì€ ê²Œì‹œê¸€ì˜ ì²­í¬ë“¤ â†’ ëª¨ë‘ í¬í•¨ (í‚¤ì›Œë“œ í•„í„°ë§ ìŠ¤í‚µ)
        # ì´ìœ : ì´ë¯¸ ë©€í‹°ìŠ¤í…Œì´ì§€ ê²€ìƒ‰(BM25 + Dense + Reranker)ìœ¼ë¡œ ìµœì  ê²Œì‹œê¸€ ì„ ì • ì™„ë£Œ
        # í•´ë‹¹ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì •ë³´(ë³¸ë¬¸, ì´ë¯¸ì§€ OCR, ì²¨ë¶€íŒŒì¼)ë¥¼ LLMì— ì „ë‹¬í•´ì•¼ ì™„ì „í•œ ë‹µë³€ ê°€ëŠ¥
        logger.info(f"   âœ… ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ ê°ì§€ â†’ í‚¤ì›Œë“œ í•„í„°ë§ ìŠ¤í‚µ ({len(documents)}ê°œ ëª¨ë‘ í¬í•¨)")
        relevant_docs = documents
    else:
        # âŒ ì—¬ëŸ¬ ê²Œì‹œê¸€ í˜¼ì¬ â†’ í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
        logger.info(f"   ğŸ” ì—¬ëŸ¬ ê²Œì‹œê¸€ í˜¼ì¬ ({len(unique_titles)}ê°œ) â†’ í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©")
        relevant_docs = [
            doc for doc in documents if
            any(keyword in doc.page_content for keyword in query_noun) or  # í‚¤ì›Œë“œ ë§¤ì¹­
            doc.metadata.get('source') in ['image_ocr', 'document_parse']  # ë©€í‹°ëª¨ë‹¬ í•­ìƒ í¬í•¨
        ]

    if not relevant_docs:
      return None, None

    # ğŸ” ë””ë²„ê¹…: ê° ì²­í¬ì˜ ë‚´ìš© ê¸¸ì´ í™•ì¸ (ë°ì´í„° ëˆ„ë½ ê²€ì¦)
    logger.info(f"   ğŸ“‹ LLMì— ì „ë‹¬ë  ì²­í¬ ìƒì„¸:")
    for i, doc in enumerate(relevant_docs):
        source = doc.metadata.get('source', 'unknown')
        content_len = len(doc.page_content)
        logger.info(f"      ì²­í¬{i+1}: [{source}] {content_len}ì")

    # LLM ì´ˆê¸°í™” (ëª…ë‹¨ ì§ˆë¬¸ì„ ìœ„í•œ ì¶©ë¶„í•œ max_tokens ì„¤ì •)
    llm = ChatUpstage(
        api_key=storage.upstage_api_key,
        max_tokens=4096  # ê¸´ ëª…ë‹¨ë„ ì™„ì „íˆ ë‚˜ì—´í•  ìˆ˜ ìˆë„ë¡ ì¶©ë¶„í•œ í† í° í™•ë³´
    )
    relevant_docs_content=format_docs(relevant_docs)

    # ğŸ” ë””ë²„ê¹…: ì „ì²´ context í¬ê¸° ë° ë‚´ìš© í™•ì¸
    logger.info(f"   ğŸ“Š ì „ì²´ Context í¬ê¸°: {len(relevant_docs_content)}ì")
    logger.info(f"   ğŸ“„ ì‹¤ì œ ì „ë‹¬ë˜ëŠ” Context ìš”ì•½ (ê° ì²­í¬ë‹¹ ì• 20ì + ë’¤ 20ì):")
    logger.info(f"{'='*80}")

    # ê° ì²­í¬ë¥¼ "\n\në¬¸ì„œ ì œëª©:"ìœ¼ë¡œ ë¶„ë¦¬
    chunks = relevant_docs_content.split('\n\në¬¸ì„œ ì œëª©:')
    for i, chunk in enumerate(chunks):
        if i > 0:  # ì²« ë²ˆì§¸ëŠ” ë¹ˆ ë¬¸ìì—´ì´ë¯€ë¡œ ìŠ¤í‚µ
            chunk = 'ë¬¸ì„œ ì œëª©:' + chunk  # ë¶„ë¦¬ ì‹œ ì œê±°ëœ ë¶€ë¶„ ë³µì›

        chunk_len = len(chunk)

        if chunk_len <= 40:
            # 40ì ì´í•˜ë©´ ì „ì²´ ì¶œë ¥
            logger.info(chunk)
        else:
            # ì• 20ì + ... + ë’¤ 20ì
            preview = chunk[:20] + f'... ({chunk_len - 40}ì ìƒëµ) ...' + chunk[-20:]
            logger.info(preview)

        if i < len(chunks) - 1:
            logger.info('')  # ì²­í¬ êµ¬ë¶„ìš© ë¹ˆ ì¤„

    logger.info(f"{'='*80}")

    qa_chain = (
        {
            "current_time": lambda _: get_korean_time().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„"),
            "context": RunnableLambda(lambda _: relevant_docs_content),
            "question": RunnablePassthrough()
        }
        | PROMPT
        | llm
        | StrOutputParser()
    )

    return qa_chain, relevant_docs, relevant_docs_content



#######################################################################

##### ìœ ì‚¬ë„ ì œëª© ë‚ ì§œ ë³¸ë¬¸  url image_urlìˆœìœ¼ë¡œ ì €ì¥ë¨

def get_ai_message(question):
    s_time=time.time()
    best_time=time.time()
    top_doc, query_noun = best_docs(question)  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    best_f_time=time.time()-best_time
    print(f"best_docs ë½‘ëŠ” ì‹œê°„:{best_f_time}")

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹…
    logger.info(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {question}")
    logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {query_noun}")

    # query_nounì´ ì—†ê±°ë‚˜ top_docì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not query_noun or not top_doc or len(top_doc) == 0:
        notice_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }
        return not_in_notices_response
    if len(query_noun)==1 and any(keyword in query_noun for keyword in ['ì±„ìš©','ê³µì§€ì‚¬í•­','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','ê°•ì—°','íŠ¹ê°•']):
      seen_urls = set()  # ì´ë¯¸ ë³¸ URLì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©
      response = f"'{query_noun[0]}'ì— ëŒ€í•œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
      show_url=""
      if top_doc !=None:
        for title, date, _, url in top_doc:  # top_docì—ì„œ ì œëª©, ë‚ ì§œ, URL ì¶”ì¶œ
            if url not in seen_urls:
                response += f"ì œëª©: {title}, ë‚ ì§œ: {date} \n----------------------------------------------------\n"
                seen_urls.add(url)  # URL ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
      if 'ì±„ìš©' in query_noun:
        show_url = COMPANY_BASE_URL + "&wr_id="
      elif 'ê³µì§€ì‚¬í•­' in query_noun:
        show_url = NOTICE_BASE_URL + "&wr_id="
      else:
        show_url = SEMINAR_BASE_URL + "&wr_id="

      # ìµœì¢… data êµ¬ì¡° ìƒì„±
      data = {
        "answer": response,
        "references": show_url,  # show_urlì„ ë„˜ê¸°ê¸°
        "disclaimer": "\n\ní•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "images": ["No content"]
      }
      f_time=time.time()-s_time
      print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
      return data
    top_docs = [list(doc) for doc in top_doc]

    # âœ… Reranking ì „ Top 5 ë¡œê¹…
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Reranking ì „ ê²€ìƒ‰ ê²°ê³¼ Top {min(5, len(top_docs))}:")
    for i, doc in enumerate(top_docs[:5]):
        score, title, date, text, url = doc[:5]
        logger.info(f"   {i+1}ìœ„: [{score:.4f}] {title[:50]}... ({date})")
    logger.info("=" * 60)

    # âœ… BGE-Rerankerë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ê´€ë ¨ì„± ê¸°ì¤€)
    if storage.reranker and len(top_docs) > 1:
        logger.info("ğŸ¯ BGE-Reranker í™œì„±í™”!")
        rerank_time = time.time()
        logger.info(f"   ì…ë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ â†’ Reranking ì‹œì‘...")

        # RerankerëŠ” tuple ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ë³€í™˜
        top_docs_tuples = [tuple(doc) for doc in top_docs]

        # Reranking (ì–´ì°¨í”¼ 1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ Top 5ë¡œ ì••ì¶•)
        reranked_docs_tuples = storage.reranker.rerank(
            query=question,
            documents=top_docs_tuples,
            top_k=5  # ìµœëŒ€ 5ê°œë¡œ ì••ì¶• (1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ íš¨ìœ¨í™”)
        )

        # ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        top_docs = [list(doc) for doc in reranked_docs_tuples]

        rerank_f_time = time.time() - rerank_time
        logger.info(f"   ì¶œë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ (ì²˜ë¦¬ ì‹œê°„: {rerank_f_time:.2f}ì´ˆ)")
        print(f"âœ… Reranking ì™„ë£Œ: {rerank_f_time:.2f}ì´ˆ")
    elif not storage.reranker:
        logger.info("â­ï¸  BGE-Reranker ë¹„í™œì„±í™” (ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨)")
        logger.info("   â†’ ì›ë³¸ ê²€ìƒ‰ ìˆœì„œ ìœ ì§€")
    elif len(top_docs) <= 1:
        logger.info("â­ï¸  BGE-Reranker ìŠ¤í‚µ (ë¬¸ì„œ 1ê°œ ì´í•˜)")
        logger.info("   â†’ Reranking ë¶ˆí•„ìš”")

    # âœ… Reranking í›„ Top 5 ë¡œê¹…
    logger.info("=" * 60)
    logger.info(f"ğŸ” Reranking í›„ ìµœì¢… ê²°ê³¼ Top {min(5, len(top_docs))}:")
    seen_urls = set()
    unique_url_count = 0
    for i, doc in enumerate(top_docs[:5]):
        score, title, date, text, url = doc[:5]

        # URL ì¤‘ë³µ ì²´í¬
        if url not in seen_urls:
            seen_urls.add(url)
            unique_url_count += 1
            url_marker = "ğŸ†•"  # ìƒˆë¡œìš´ URL
        else:
            url_marker = "ğŸ”"  # ì¤‘ë³µ URL (ê°™ì€ ë¬¸ì„œì˜ ë‹¤ë¥¸ ì²­í¬)

        logger.info(f"   {i+1}ìœ„: [{score:.4f}] {url_marker} {title[:50]}... ({date})")
        logger.info(f"      URL: {url}")

    logger.info(f"   ğŸ’¡ ë‹¤ì–‘ì„±: Top 5 ì¤‘ {unique_url_count}ê°œ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ")
    logger.info("=" * 60)

    final_score = top_docs[0][0]
    final_title = top_docs[0][1]
    final_date = top_docs[0][2]
    final_text = top_docs[0][3]
    final_url = top_docs[0][4]
    final_image = []

    # ìµœì¢… ì„ íƒëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹…
    logger.info(f"ğŸ“„ ìµœì¢… ì„ íƒ ë¬¸ì„œ:")
    logger.info(f"   ì œëª©: {final_title}")
    logger.info(f"   ë‚ ì§œ: {final_date}")
    logger.info(f"   ìœ ì‚¬ë„: {final_score:.4f}")
    logger.info(f"   URL: {final_url}")
    logger.info(f"   ë³¸ë¬¸ ê¸¸ì´: {len(final_text)}ì")
    if len(final_text) > 0:
        logger.info(f"   ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {final_text[:100]}...")

    # MongoDB ì—°ê²° í™•ì¸ í›„ ì´ë¯¸ì§€ URL ì¡°íšŒ
    if storage.mongo_collection is not None:
        record = storage.mongo_collection.find_one({"title" : final_title})
        if record :
            if(isinstance(record["image_url"], list)):
              final_image.extend(record["image_url"])
            else :
              final_image.append(record["image_url"])
            logger.info(f"   ì´ë¯¸ì§€: {len(final_image)}ê°œ")

            # HTML êµ¬ì¡° ì •ë³´ ë¡œê¹…
            if record.get("html"):
                html_length = len(record["html"])
                logger.info(f"   HTML êµ¬ì¡°: âœ… ìˆìŒ ({html_length}ì)")
            else:
                logger.info(f"   HTML êµ¬ì¡°: âŒ ì—†ìŒ")

            # ì½˜í…ì¸  íƒ€ì… ë¡œê¹…
            content_type = record.get("content_type", "unknown")
            source = record.get("source", "unknown")
            logger.info(f"   ì½˜í…ì¸  íƒ€ì…: {content_type}")
            logger.info(f"   ì†ŒìŠ¤: {source}")
        else :
            print("ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì¡´ì¬ X")
            logger.warning(f"âš ï¸  MongoDBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {final_title}")
            final_score = 0
            final_title = "No content"
            final_date = "No content"
            final_text = "No content"
            final_url = "No URL"
            final_image = ["No content"]
    else:
        logger.warning("âš ï¸  MongoDB ì—°ê²° ì—†ìŒ - ì´ë¯¸ì§€ URL ì¡°íšŒ ë¶ˆê°€")
        final_image = ["No content"]

    # top_docs ì¸ë±ìŠ¤ êµ¬ì„±
    # 0: ìœ ì‚¬ë„, 1: ì œëª©, 2: ë‚ ì§œ, 3: ë³¸ë¬¸ë‚´ìš©, 4: url, 5: ì´ë¯¸ì§€url

    if final_image[0] != "No content" and final_text == "No content" and final_score > MINIMUM_SIMILARITY_SCORE:
        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        only_image_response = {
            "answer": None,
            "references": final_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return only_image_response

    # ì´ë¯¸ì§€ + LLM ë‹µë³€ì´ ìˆëŠ” ê²½ìš°.
    else:
        # âœ… í•µì‹¬ ê°œì„ : ê°™ì€ URLì˜ ëª¨ë“  ì²­í¬(ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€)ë¥¼ LLMì— ì „ë‹¬!
        # ë¬¸ì œ: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ëŠ” ë³¸ë¬¸ ì²­í¬ë§Œ í¬í•¨ (ì²¨ë¶€íŒŒì¼ ëˆ„ë½)
        # í•´ê²°: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê°€ì ¸ì˜´
        enrich_time = time.time()

        # Top ë¬¸ì„œì˜ URL ì¶”ì¶œ (ê²Œì‹œê¸€ URL)
        top_url = top_docs[0][4] if top_docs else None

        if top_url:
            # âœ… ë³€ê²½: URL ê¸°ë°˜ ë§¤ì¹­ ëŒ€ì‹  ì œëª© ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš©!
            # ì´ìœ : ì´ë¯¸ì§€ URL(/data/editor/...)ì€ wr_idë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ
            # í•´ê²°: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ëŠ” ê°™ì€ ì œëª©ì„ ê³µìœ í•˜ë¯€ë¡œ ì œëª©ìœ¼ë¡œ ë§¤ì¹­
            top_title = top_docs[0][1]  # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì œëª©
            wr_id = top_url.split('&wr_id=')[-1] if '&wr_id=' in top_url else top_url.split('wr_id=')[-1] if 'wr_id=' in top_url else None

            logger.info(f"ğŸ” ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ ê²€ìƒ‰: ì œëª©='{top_title}' (wr_id={wr_id})")

            # ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ì°¾ê¸° (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€ OCR)
            enriched_docs = []
            seen_texts = set()  # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°ìš©

            # ë””ë²„ê¹…: ë§¤ì¹­ ìƒí™© ì¶”ì 
            total_checked = 0
            matched_count = 0
            duplicate_count = 0

            for i, url in enumerate(storage.cached_urls):
                # âœ… ê°™ì€ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸ (ì œëª© ê¸°ì¤€ - ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ í¬í•¨!)
                if storage.cached_titles[i] == top_title:
                    total_checked += 1
                    matched_count += 1

                    text = storage.cached_texts[i]
                    content_type = storage.cached_content_types[i] if i < len(storage.cached_content_types) else "unknown"
                    source = storage.cached_sources[i] if i < len(storage.cached_sources) else "unknown"

                    # ë””ë²„ê¹… ë¡œê·¸ (ì²˜ìŒ 5ê°œë§Œ)
                    if matched_count <= 5:
                        logger.info(f"   [{matched_count}] URL: {url[:80]}...")
                        logger.info(f"       íƒ€ì…: {content_type}, ì†ŒìŠ¤: {source}, í…ìŠ¤íŠ¸: {len(text)}ì")

                    # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ì§€ ì•ŠìŒ! (ì¤‘ìš”: "No content"ë„ í¬í•¨)
                    text_key = ''.join(text.split())  # ê³µë°± ì œê±° í›„ ë¹„êµ

                    # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±° (ë¹ˆ ë¬¸ìì—´ì€ ì œì™¸í•˜ì§€ ì•ŠìŒ!)
                    if text_key not in seen_texts:  # âœ… 'text_key and' ì œê±° (ë¹ˆ í…ìŠ¤íŠ¸ë„ í¬í•¨)
                        seen_texts.add(text_key)
                        enriched_docs.append((
                            top_docs[0][0],  # ì ìˆ˜ëŠ” top ë¬¸ì„œì™€ ë™ì¼
                            storage.cached_titles[i],
                            storage.cached_dates[i],
                            text,
                            url,
                            storage.cached_htmls[i] if i < len(storage.cached_htmls) else "",
                            storage.cached_content_types[i] if i < len(storage.cached_content_types) else "unknown",
                            storage.cached_sources[i] if i < len(storage.cached_sources) else "unknown",
                            storage.cached_attachment_types[i] if i < len(storage.cached_attachment_types) else ""
                        ))
                    else:
                        duplicate_count += 1

            logger.info(f"   ğŸ“Š ë§¤ì¹­ í†µê³„: ì „ì²´ {len(storage.cached_urls)}ê°œ ì¤‘ {matched_count}ê°œ ë§¤ì¹­, {duplicate_count}ê°œ ì¤‘ë³µ ì œê±°")

            # ì²­í¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ top_docsë¥¼ êµì²´ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€)
            if enriched_docs:
                logger.info(f"ğŸ”§ ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ìˆ˜ì§‘: {len(top_docs)}ê°œ â†’ {len(enriched_docs)}ê°œ")

                # íƒ€ì…ë³„ ì¹´ìš´íŠ¸ (source ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ì¹´ìš´íŠ¸)
                æœ¬ë¬¸_count = 0
                image_count = 0
                attachment_count = 0

                for i, (score, title, date, text, url, html, content_type, source, attachment_type) in enumerate(enriched_docs):
                    # âœ… sourceë¥¼ tupleì—ì„œ ì§ì ‘ ì‚¬ìš© (URLë¡œ ì°¾ì§€ ì•ŠìŒ)
                    if source == "original_post":
                        æœ¬æ–‡_count += 1
                    elif source == "image_ocr":
                        image_count += 1
                    elif source == "document_parse":
                        attachment_count += 1

                logger.info(f"   ğŸ“¦ ë³¸ë¬¸ ì²­í¬: {æœ¬ë¬¸_count}ê°œ")
                logger.info(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ OCR ì²­í¬: {image_count}ê°œ")
                logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì²­í¬: {attachment_count}ê°œ")
                top_docs = enriched_docs
            else:
                logger.warning(f"âš ï¸  ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤! wr_id={wr_id}")
                logger.warning(f"   Top URL: {top_url}")

        enrich_f_time = time.time() - enrich_time
        print(f"ì²­í¬ ìˆ˜ì§‘ ì‹œê°„: {enrich_f_time}")

        chain_time=time.time()
        qa_chain, relevant_docs, relevant_docs_content = get_answer_from_chain(top_docs, question,query_noun)
        chain_f_time=time.time()-chain_time
        print(f"chain ìƒì„±í•˜ëŠ” ì‹œê°„: {chain_f_time}")
        if final_url == PROFESSOR_BASE_URL + "&lang=kor" and any(keyword in query_noun for keyword in ['ì—°ë½ì²˜', 'ì „í™”', 'ë²ˆí˜¸', 'ì „í™”ë²ˆí˜¸']):
            data = {
                "answer": "í•´ë‹¹ êµìˆ˜ë‹˜ì€ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return data
            
        # prof_title=final_title
        # prof_url=["https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_1"]
        # prof_name=""
        # # ì •ê·œì‹ì„ ì´ìš©í•˜ì—¬ ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ ì¶”ì¶œ
        # if any(final_url.startswith(url) for url in prof_url):
        #     match = re.match(r"^[^\dA-Za-z]+", prof_title)
        #     if match:
        #         prof_name = match.group().strip()  # ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     else:
        #         prof_name = prof_title.strip()  # ìˆ«ìê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     prof_name = re.sub(r"\s+", "", prof_name)
        #     user_question = re.sub(r"\s+", "", question)
        #     if prof_name not in user_question:
        #         refer_url=""
        #         if 'ì§ì›' in query_noun:
        #             refer_url=prof_url[1]
        #         else:
        #             refer_url=prof_url[2]
        #         data = {
        #             "answer": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” êµìˆ˜ë‹˜ ì •ë³´ì…ë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
        #             "references": refer_url,
        #             "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        #             "images": ["No content"]
        #         }
        #         return data

        # ê³µì§€ì‚¬í•­ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        notice_url = NOTICE_BASE_URL
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨
        if not qa_chain or not relevant_docs:
            if final_image[0] != "No content" and final_score > MINIMUM_SIMILARITY_SCORE:
                data = {
                    "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "references": final_url,
                    "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "images": final_image
                }
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return data
            else:
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return not_in_notices_response

        # ìœ ì‚¬ë„ê°€ ë‚®ì€ ê²½ìš°
        if final_score < MINIMUM_SIMILARITY_SCORE:
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return not_in_notices_response

        # LLMì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²½ìš°
        answer_time=time.time()

        # qa_chain.invoke() ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        answer_result = qa_chain.invoke(question)

        answer_f_time=time.time()-answer_time
        print(f"ë‹µë³€ ìƒì„±í•˜ëŠ” ì‹œê°„: {answer_f_time}")

        logger.info(f"ğŸ’¬ LLM ë‹µë³€ ìƒì„± ì™„ë£Œ:")
        logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(answer_result)}ì")
        logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer_result[:150]}...")
        logger.info(f"   ì‚¬ìš©ëœ ì°¸ê³ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")

        # ë‹µë³€ ê²€ì¦ ë° ê²½ê³  ì¶”ê°€ (ë²”ìš©)
        completeness_keywords = ['ì „ë¶€', 'ëª¨ë“ ', 'ëª¨ë‘', 'ë¹ ì§ì—†ì´', 'ì „ì²´', 'ë‹¤', 'ëª…ë‹¨', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ëˆ„êµ¬']
        has_completeness_request = any(keyword in question for keyword in completeness_keywords)

        # ì™„ì „ì„± ìš”êµ¬ + Contextì™€ ë‹µë³€ ì°¨ì´ê°€ í¬ë©´ ê²½ê³ 
        if has_completeness_request:
            # Contextì— ìˆëŠ” ìˆ«ì íŒ¨í„´ (í•™ë²ˆ, ë‚ ì§œ ë“±)
            import re
            context_numbers = len(re.findall(r'\b20\d{6,8}\b', relevant_docs_content))
            answer_numbers = len(re.findall(r'\b20\d{6,8}\b', answer_result))

            logger.info(f"   ğŸ“Š ì™„ì „ì„± ê²€ì¦: Context {context_numbers}ê±´ / ë‹µë³€ {answer_numbers}ê±´")

            # Contextì˜ 50% ë¯¸ë§Œë§Œ ë‹µë³€ì— í¬í•¨ë˜ë©´ ê²½ê³ 
            if context_numbers >= 10 and answer_numbers < context_numbers * 0.5:
                logger.warning(f"   âš ï¸ ì™„ì „ì„± ìš”êµ¬í–ˆìœ¼ë‚˜ ë‹µë³€ ë¶ˆì™„ì „! LLMì´ ì„ì˜ë¡œ ìš”ì•½í•œ ê²ƒìœ¼ë¡œ íŒë‹¨")
                answer_result += f"\n\nâš ï¸ ì¼ë¶€ ë‚´ìš©ì´ ìƒëµë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸ì„œ: ì•½ {context_numbers}ê±´ / ë‹µë³€: {answer_numbers}ê±´). ì „ì²´ ë‚´ìš©ì€ ì°¸ê³  URLì„ í™•ì¸í•˜ì„¸ìš”."

        doc_references = "\n".join([
            f"\nì°¸ê³  ë¬¸ì„œ URL: {doc.metadata['url']}"
            for doc in relevant_docs[:1] if doc.metadata.get('url') != 'No URL'
        ])

        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        data = {
            "answer": answer_result,
            "references": doc_references,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        logger.info(f"âœ… ì´ ì²˜ë¦¬ ì‹œê°„: {f_time:.2f}ì´ˆ")
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return data