import os
import re
import time
import pickle
import logging
from datetime import datetime
from collections import defaultdict
import numpy as np
import pytz
from dotenv import load_dotenv
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
from langchain import hub
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import Runnable, RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableSequence, RunnableMap
from langchain_core.runnables import RunnableLambda
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from pymongo import MongoClient

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Mecab import (logger ì •ì˜ ì´í›„)
try:
    from konlpy.tag import Mecab
    MECAB_AVAILABLE = True
    logger.info("âœ… Mecab ì‚¬ìš© ê°€ëŠ¥ (30-50ë°° ë¹ ë¥¸ í˜•íƒœì†Œ ë¶„ì„)")
except Exception as e:
    logger.warning(f"âš ï¸  Mecabì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    logger.warning("âš ï¸  Mecab ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì •í™•ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    MECAB_AVAILABLE = False
    Mecab = None

# StorageManager import
from modules.storage_manager import get_storage_manager

# Services import
from modules.services.document_service import DocumentService
from modules.services.search_service import SearchService
from modules.services.llm_service import LLMService
from modules.services.scoring_service import ScoringService
from modules.services.response_service import ResponseService

# Configuration import
from config.settings import MINIMUM_SIMILARITY_SCORE
from config.prompts import get_qa_prompt, get_temporal_intent_prompt
from config.ml_settings import get_ml_config

# Constants import
from modules.constants import (
    NOTICE_BASE_URL,
    COMPANY_BASE_URL,
    SEMINAR_BASE_URL,
    PROFESSOR_BASE_URL
)

# Utils import
from modules.utils.date_utils import get_current_kst as get_korean_time, parse_date_change_korea_time
from modules.utils.url_utils import find_url
from modules.utils.formatter import format_temporal_intent, format_docs

# StorageManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
storage = get_storage_manager()

# Service ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
document_service = DocumentService(storage)
search_service = SearchService(storage)
llm_service = LLMService(storage)
scoring_service = ScoringService(
    date_parser_fn=parse_date_change_korea_time,
    current_time_fn=get_korean_time
)
response_service = ResponseService(
    storage_manager=storage,
    search_service=search_service,
    llm_service=llm_service
)

# ML ì„¤ì • ë¡œë“œ
ml_config = get_ml_config()

# ë‹¨ì–´ ëª…ì‚¬í™” í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - QueryTransformer ì‚¬ìš©)
def transformed_query(content):
    """
    ì§ˆë¬¸ì„ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        content: ì‚¬ìš©ì ì§ˆë¬¸ (ì›ë¬¸)

    Returns:
        List[str]: ì¶”ì¶œëœ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    return storage.query_transformer.transform(content)
###################################################################################################


# Dense Retrieval (Upstage ì„ë² ë”©) - Lazy initializationìœ¼ë¡œ í•¨ìˆ˜ ë‚´ì—ì„œ ìƒì„±í•˜ë„ë¡ ë³€ê²½
# embeddings ê°ì²´ëŠ” í•„ìš”í•  ë•Œ get_embeddings() í•¨ìˆ˜ë¥¼ í†µí•´ ê°€ì ¸ì˜µë‹ˆë‹¤.
def get_embeddings():
    """Upstage Embeddings ê°ì²´ ë°˜í™˜ (Lazy initialization)"""
    return UpstageEmbeddings(
        api_key=storage.upstage_api_key,
        model="solar-embedding-1-large-query"  # ì§ˆë¬¸ ì„ë² ë”©ìš© ëª¨ë¸
    )
# dense_doc_vectors = np.array(embeddings.embed_documents(texts))  # ë¬¸ì„œ ì„ë² ë”©

# ==========================================
# Document Service Wrapper Functions
# ==========================================
# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜ë“¤
# ì‹¤ì œ ë¡œì§ì€ DocumentServiceë¡œ ì´ë™ë¨
# ==========================================

def fetch_titles_from_pinecone():
    """
    [DEPRECATED] DocumentService.fetch_all_documents()ë¡œ ì´ë™ë¨
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜
    """
    return document_service.fetch_all_documents()


# ìºì‹± ë°ì´í„° ì´ˆê¸°í™” í•¨ìˆ˜

def initialize_cache():
    """
    [DEPRECATED] DocumentService.initialize_cache()ë¡œ ì´ë™ë¨
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜

    ìºì‹œ ë¡œë“œ í›„ Retriever ì´ˆê¸°í™”ë„ ìˆ˜í–‰
    """
    # 1. ìºì‹œ ë¡œë“œ (DocumentService)
    document_service.initialize_cache()

    # 2. Retriever ì´ˆê¸°í™” (ai_modules ì±…ì„)
    _initialize_retrievers()


def _initialize_retrievers():
    """Retriever ì´ˆê¸°í™” ë¡œì§ (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë¶„ë¦¬)"""
    logger.info("ğŸ”§ ê²€ìƒ‰ ì—”ì§„(BM25/Dense) êµ¬ì¶• ì¤‘...")

    from modules.retrieval import (
        BM25Retriever,
        DenseRetriever,
        DocumentCombiner,
        DocumentClusterer
    )

    # BM25Retriever ì´ˆê¸°í™” (HTML ë°ì´í„° í¬í•¨, Redis ìºì‹±)
    bm25_retriever = BM25Retriever(
        titles=storage.cached_titles,
        texts=storage.cached_texts,
        urls=storage.cached_urls,
        dates=storage.cached_dates,
        query_transformer=transformed_query,
        similarity_adjuster=adjust_similarity_scores,
        htmls=storage.cached_htmls,  # HTML êµ¬ì¡°í™” ë°ì´í„° ì¶”ê°€
        k1=ml_config.bm25.k1,
        b=ml_config.bm25.b,
        redis_client=storage.redis_client  # Redis ìºì‹± í™œì„±í™”
    )
    storage.set_bm25_retriever(bm25_retriever)

    # DenseRetriever ì´ˆê¸°í™”
    dense_retriever = DenseRetriever(
        embeddings_factory=get_embeddings,
        pinecone_index=storage.pinecone_index,
        date_adjuster=adjust_date_similarity,
        similarity_scale=ml_config.dense_retrieval.similarity_scale,
        noun_weight=ml_config.dense_retrieval.noun_weight,
        digit_weight=ml_config.dense_retrieval.digit_weight
    )
    storage.set_dense_retriever(dense_retriever)

    # DocumentCombiner ì´ˆê¸°í™”
    document_combiner = DocumentCombiner(
        keyword_filter=last_filter_keyword,
        date_adjuster=adjust_date_similarity
    )
    storage.set_document_combiner(document_combiner)

    # DocumentClusterer ì´ˆê¸°í™”
    document_clusterer = DocumentClusterer(
        date_parser=parse_date_change_korea_time,
        similarity_threshold=ml_config.clustering.similarity_threshold
    )
    storage.set_document_clusterer(document_clusterer)

    logger.info("âœ… ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")

                    #################################   24.11.16ê¸°ì¤€ ì •í™•ë„ ì¸¡ì •ì™„ë£Œ #####################################################
######################################################################################################################

# ==========================================
# Scoring Service Wrapper Functions
# ==========================================
# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜ë“¤
# ì‹¤ì œ ë¡œì§ì€ ScoringServiceë¡œ ì´ë™ë¨
# ==========================================

def calculate_weight_by_days_difference(post_date, current_date, query_nouns):
    """
    [DEPRECATED] ScoringService.calculate_weight_by_days_difference()ë¡œ ì´ë™ë¨
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜
    """
    return scoring_service.calculate_weight_by_days_difference(post_date, current_date, query_nouns)


def adjust_date_similarity(similarity, date_str, query_nouns):
    """
    [DEPRECATED] ScoringService.adjust_date_similarity()ë¡œ ì´ë™ë¨
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜
    """
    return scoring_service.adjust_date_similarity(similarity, date_str, query_nouns)


def adjust_similarity_scores(query_noun, title, texts, similarities):
    """
    [DEPRECATED] ScoringService.adjust_similarity_scores()ë¡œ ì´ë™ë¨
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜
    """
    return scoring_service.adjust_similarity_scores(query_noun, title, texts, similarities)


#############################################################################################

# í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - KeywordFilter ì‚¬ìš©)
def last_filter_keyword(DOCS, query_noun, user_question):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§

    Args:
        DOCS: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ [(score, title, date, text, url), ...]
        query_noun: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
        user_question: ì›ë³¸ ì§ˆë¬¸

    Returns:
        List[Tuple]: í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ì¡°ì •ë¨)
    """
    return storage.keyword_filter.filter(DOCS, query_noun, user_question)

#################################################################################################

def parse_temporal_intent(query, current_date=None):
      """
      [DEPRECATED] LLMService.parse_temporal_intent()ë¡œ ì´ë™ë¨

      ì§ˆë¬¸ì—ì„œ ì‹œê°„ í‘œí˜„ì„ ê°ì§€í•˜ê³  í•„í„° ì¡°ê±´ì„ ë°˜í™˜

      Args:
          query: ì‚¬ìš©ì ì§ˆë¬¸
          current_date: í˜„ì¬ ë‚ ì§œ (ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°)

      Returns:
          dict: {"year": int, "semester": int} ë˜ëŠ” None
      """
      return llm_service.parse_temporal_intent(query, current_date)


def rewrite_query_with_llm(query, current_date):
      """
      [DEPRECATED] LLMService.rewrite_query_with_llm()ë¡œ ì´ë™ë¨

      LLMì„ ì‚¬ìš©í•´ ë³µì¡í•œ ì‹œê°„ í‘œí˜„ì„ í•´ì„

      Args:
          query: ì‚¬ìš©ì ì§ˆë¬¸
          current_date: í˜„ì¬ ë‚ ì§œ

      Returns:
          dict: {"year": int, "semester": int} ë˜ëŠ” None
      """
      return llm_service.rewrite_query_with_llm(query, current_date)


def best_docs(user_question):
      """
      [DEPRECATED] SearchService.search_documents()ë¡œ ì´ë™ë¨

      ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰

      Args:
          user_question: ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸

      Returns:
          Tuple: (ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ì¿¼ë¦¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸)
      """
      return search_service.search_documents(
          user_question=user_question,
          transformed_query_fn=transformed_query,
          find_url_fn=find_url
      )


# QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ (ì „ì—­ ë³€ìˆ˜)
_qa_prompt_template = None

def get_qa_prompt_template():
    """QA í”„ë¡¬í”„íŠ¸ PromptTemplate ê°ì²´ ë°˜í™˜ (Lazy loading)"""
    global _qa_prompt_template
    if _qa_prompt_template is None:
        prompt_text = get_qa_prompt()
        _qa_prompt_template = PromptTemplate(
            template=prompt_text,
            input_variables=["current_time", "temporal_intent", "context", "question"]
        )
    return _qa_prompt_template

# PromptTemplate ê°ì²´ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
PROMPT = get_qa_prompt_template()
def get_answer_from_chain(best_docs, user_question, query_noun, temporal_filter=None):
      """
      [DEPRECATED] LLMService.get_answer_from_chain()ë¡œ ì´ë™ë¨

      QA Chain ìƒì„± ë° ê´€ë ¨ ë¬¸ì„œ ì²˜ë¦¬

      Args:
          best_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
          user_question: ì‚¬ìš©ì ì§ˆë¬¸
          query_noun: ì¿¼ë¦¬ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
          temporal_filter: ì‹œê°„ í•„í„°

      Returns:
          Tuple: (qa_chain, relevant_docs, relevant_docs_content)
      """
      return llm_service.get_answer_from_chain(
          best_docs=best_docs,
          user_question=user_question,
          query_noun=query_noun,
          temporal_filter=temporal_filter
      )


#######################################################################

##### ìœ ì‚¬ë„ ì œëª© ë‚ ì§œ ë³¸ë¬¸  url image_urlìˆœìœ¼ë¡œ ì €ì¥ë¨

# ==========================================
# Response Service Wrapper Function
# ==========================================
# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜
# ì‹¤ì œ ë¡œì§ì€ ResponseServiceë¡œ ì´ë™ë¨
# ==========================================

def get_ai_message(question):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ AI ì‘ë‹µ ìƒì„±

    [REFACTORED] ResponseService.generate_response()ë¡œ ì´ë™ë¨
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper í•¨ìˆ˜

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸

    Returns:
        Dict: ì‘ë‹µ JSON
            {
                "answer": str,
                "answerable": bool,
                "references": str,
                "disclaimer": str,
                "images": List[str]
            }
    """
    return response_service.generate_response(
        question=question,
        transformed_query_fn=transformed_query,
        find_url_fn=find_url,
        minimum_similarity_score=MINIMUM_SIMILARITY_SCORE
    )


# ==========================================
# Legacy get_ai_message Implementation (ARCHIVED)
# ==========================================
# ì•„ë˜ëŠ” ì´ì „ get_ai_message êµ¬í˜„ì…ë‹ˆë‹¤.
# ResponseServiceë¡œ ì™„ì „íˆ ì´ë™ë˜ì—ˆìœ¼ë¯€ë¡œ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë‚¨ê¹ë‹ˆë‹¤.
# ì‚­ì œ ê°€ëŠ¥í•˜ì§€ë§Œ, ì¼ë‹¨ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ë³´ê´€í•©ë‹ˆë‹¤.
# ==========================================

"""
def get_ai_message_legacy(question):
    s_time=time.time()

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹… (ê°€ì¥ ë¨¼ì €!)
    logger.info(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {question}")

    # âœ… ì‹œê°„ ì˜ë„ íŒŒì‹± (LLM ë‹µë³€ ì‹œ í™œìš©)
    from datetime import datetime
    temporal_filter = parse_temporal_intent(question, datetime.now())

    best_time=time.time()
    top_doc, query_noun = best_docs(question)  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    best_f_time=time.time()-best_time
    print(f"best_docs ë½‘ëŠ” ì‹œê°„:{best_f_time}")
    logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {query_noun}")

    # query_nounì´ ì—†ê±°ë‚˜ top_docì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not query_noun or not top_doc or len(top_doc) == 0:
        notice_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }
        return not_in_notices_response
    if len(query_noun)==1 and any(keyword in query_noun for keyword in ['ì±„ìš©','ê³µì§€ì‚¬í•­','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','ê°•ì—°','íŠ¹ê°•']):
      seen_urls = set()  # ì´ë¯¸ ë³¸ URLì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©
      response = f"'{query_noun[0]}'ì— ëŒ€í•œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
      show_url=""
      if top_doc !=None:
        for title, date, _, url in top_doc:  # top_docì—ì„œ ì œëª©, ë‚ ì§œ, URL ì¶”ì¶œ
            if url not in seen_urls:
                response += f"ì œëª©: {title}, ë‚ ì§œ: {date} \n----------------------------------------------------\n"
                seen_urls.add(url)  # URL ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
      if 'ì±„ìš©' in query_noun:
        show_url = COMPANY_BASE_URL + "&wr_id="
      elif 'ê³µì§€ì‚¬í•­' in query_noun:
        show_url = NOTICE_BASE_URL + "&wr_id="
      else:
        show_url = SEMINAR_BASE_URL + "&wr_id="

      # ìµœì¢… data êµ¬ì¡° ìƒì„±
      data = {
        "answer": response,
        "answerable": True,  # ëª©ë¡ ì œê³µ ì„±ê³µ
        "references": show_url,  # show_urlì„ ë„˜ê¸°ê¸°
        "disclaimer": "\n\ní•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "images": ["No content"]
      }
      f_time=time.time()-s_time
      print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
      return data
    top_docs = [list(doc) for doc in top_doc]

    # âœ… Reranking ì „ Top 5 ë¡œê¹…
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Reranking ì „ ê²€ìƒ‰ ê²°ê³¼ Top {min(5, len(top_docs))}:")
    for i, doc in enumerate(top_docs[:5]):
        score, title, date, text, url = doc[:5]
        logger.info(f"   {i+1}ìœ„: [{score:.4f}] {title[:50]}... ({date})")
    logger.info("=" * 60)

    # âœ… BGE-Rerankerë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ê´€ë ¨ì„± ê¸°ì¤€)
    reranking_used = False  # Reranking ì‚¬ìš© ì—¬ë¶€ ì¶”ì 
    if storage.reranker and len(top_docs) > 1:
        logger.info("ğŸ¯ BGE-Reranker í™œì„±í™”!")
        rerank_time = time.time()
        logger.info(f"   ì…ë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ â†’ Reranking ì‹œì‘...")

        # RerankerëŠ” tuple ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ë³€í™˜
        top_docs_tuples = [tuple(doc) for doc in top_docs]

        # Reranking (ì–´ì°¨í”¼ 1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ Top 5ë¡œ ì••ì¶•)
        reranked_docs_tuples = storage.reranker.rerank(
            query=question,
            documents=top_docs_tuples,
            top_k=5  # ìµœëŒ€ 5ê°œë¡œ ì••ì¶• (1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ íš¨ìœ¨í™”)
        )

        # ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        top_docs = [list(doc) for doc in reranked_docs_tuples]
        reranking_used = True  # Reranking ì‚¬ìš©ë¨

        rerank_f_time = time.time() - rerank_time
        logger.info(f"   ì¶œë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ (ì²˜ë¦¬ ì‹œê°„: {rerank_f_time:.2f}ì´ˆ)")
        print(f"âœ… Reranking ì™„ë£Œ: {rerank_f_time:.2f}ì´ˆ")
    elif not storage.reranker:
        logger.info("â­ï¸  BGE-Reranker ë¹„í™œì„±í™” (ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨)")
        logger.info("   â†’ ì›ë³¸ ê²€ìƒ‰ ìˆœì„œ ìœ ì§€")
    elif len(top_docs) <= 1:
        logger.info("â­ï¸  BGE-Reranker ìŠ¤í‚µ (ë¬¸ì„œ 1ê°œ ì´í•˜)")
        logger.info("   â†’ Reranking ë¶ˆí•„ìš”")

    # âœ… Reranking í›„ Top 5 ë¡œê¹…
    logger.info("=" * 60)
    logger.info(f"ğŸ” Reranking í›„ ìµœì¢… ê²°ê³¼ Top {min(5, len(top_docs))}:")
    seen_urls = set()
    unique_url_count = 0
    for i, doc in enumerate(top_docs[:5]):
        score, title, date, text, url = doc[:5]

        # URL ì¤‘ë³µ ì²´í¬
        if url not in seen_urls:
            seen_urls.add(url)
            unique_url_count += 1
            url_marker = "ğŸ†•"  # ìƒˆë¡œìš´ URL
        else:
            url_marker = "ğŸ”"  # ì¤‘ë³µ URL (ê°™ì€ ë¬¸ì„œì˜ ë‹¤ë¥¸ ì²­í¬)

        logger.info(f"   {i+1}ìœ„: [{score:.4f}] {url_marker} {title[:50]}... ({date})")
        logger.info(f"      URL: {url}")

    logger.info(f"   ğŸ’¡ ë‹¤ì–‘ì„±: Top 5 ì¤‘ {unique_url_count}ê°œ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ")
    logger.info("=" * 60)

    final_score = top_docs[0][0]
    final_title = top_docs[0][1]
    final_date = top_docs[0][2]
    final_text = top_docs[0][3]
    final_url = top_docs[0][4]
    final_image = []

    # ìµœì¢… ì„ íƒëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹…
    logger.info(f"ğŸ“„ ìµœì¢… ì„ íƒ ë¬¸ì„œ:")
    logger.info(f"   ì œëª©: {final_title}")
    logger.info(f"   ë‚ ì§œ: {final_date}")
    logger.info(f"   ìœ ì‚¬ë„: {final_score:.4f}")
    logger.info(f"   URL: {final_url}")
    logger.info(f"   ë³¸ë¬¸ ê¸¸ì´: {len(final_text)}ì")
    if len(final_text) > 0:
        logger.info(f"   ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {final_text[:100]}...")

    # MongoDB ì—°ê²° í™•ì¸ í›„ ì´ë¯¸ì§€ URL ì¡°íšŒ
    if storage.mongo_collection is not None:
        record = storage.mongo_collection.find_one({"title" : final_title})
        if record :
            if(isinstance(record["image_url"], list)):
              final_image.extend(record["image_url"])
            else :
              final_image.append(record["image_url"])
            logger.info(f"   ì´ë¯¸ì§€: {len(final_image)}ê°œ")

            # HTML êµ¬ì¡° ì •ë³´ ë¡œê¹…
            if record.get("html"):
                html_length = len(record["html"])
                logger.info(f"   HTML êµ¬ì¡°: âœ… ìˆìŒ ({html_length}ì)")
            else:
                logger.info(f"   HTML êµ¬ì¡°: âŒ ì—†ìŒ")

            # ì½˜í…ì¸  íƒ€ì… ë¡œê¹…
            content_type = record.get("content_type", "unknown")
            source = record.get("source", "unknown")
            logger.info(f"   ì½˜í…ì¸  íƒ€ì…: {content_type}")
            logger.info(f"   ì†ŒìŠ¤: {source}")
        else :
            print("ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì¡´ì¬ X")
            logger.warning(f"âš ï¸  MongoDBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {final_title}")
            final_score = 0
            final_title = "No content"
            final_date = "No content"
            final_text = "No content"
            final_url = "No URL"
            final_image = ["No content"]
    else:
        logger.warning("âš ï¸  MongoDB ì—°ê²° ì—†ìŒ - ì´ë¯¸ì§€ URL ì¡°íšŒ ë¶ˆê°€")
        final_image = ["No content"]

    # top_docs ì¸ë±ìŠ¤ êµ¬ì„±
    # 0: ìœ ì‚¬ë„, 1: ì œëª©, 2: ë‚ ì§œ, 3: ë³¸ë¬¸ë‚´ìš©, 4: url, 5: ì´ë¯¸ì§€url

    # Reranker ì ìˆ˜ëŠ” ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ final_score < 0ì´ë©´ ìœ ì‚¬ë„ ì²´í¬ ìŠ¤í‚µ
    if final_image[0] != "No content" and final_text == "No content" and (final_score < 0 or final_score > MINIMUM_SIMILARITY_SCORE):
        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        only_image_response = {
            "answer": None,
            "references": final_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return only_image_response

    # ì´ë¯¸ì§€ + LLM ë‹µë³€ì´ ìˆëŠ” ê²½ìš°.
    else:
        # âœ… í•µì‹¬ ê°œì„ : ê°™ì€ URLì˜ ëª¨ë“  ì²­í¬(ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€)ë¥¼ LLMì— ì „ë‹¬!
        # ë¬¸ì œ: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ëŠ” ë³¸ë¬¸ ì²­í¬ë§Œ í¬í•¨ (ì²¨ë¶€íŒŒì¼ ëˆ„ë½)
        # í•´ê²°: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê°€ì ¸ì˜´
        enrich_time = time.time()

        # Top ë¬¸ì„œì˜ URL ì¶”ì¶œ (ê²Œì‹œê¸€ URL)
        top_url = top_docs[0][4] if top_docs else None

        if top_url:
            # âœ… ë³€ê²½: URL ê¸°ë°˜ ë§¤ì¹­ ëŒ€ì‹  ì œëª© ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš©!
            # ì´ìœ : ì´ë¯¸ì§€ URL(/data/editor/...)ì€ wr_idë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ
            # í•´ê²°: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ëŠ” ê°™ì€ ì œëª©ì„ ê³µìœ í•˜ë¯€ë¡œ ì œëª©ìœ¼ë¡œ ë§¤ì¹­
            top_title = top_docs[0][1]  # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì œëª©
            wr_id = top_url.split('&wr_id=')[-1] if '&wr_id=' in top_url else top_url.split('wr_id=')[-1] if 'wr_id=' in top_url else None

            logger.info(f"ğŸ” ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ ê²€ìƒ‰: ì œëª©='{top_title}' (wr_id={wr_id})")

            # ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ì°¾ê¸° (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€ OCR)
            enriched_docs = []
            seen_texts = set()  # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°ìš©

            # ë””ë²„ê¹…: ë§¤ì¹­ ìƒí™© ì¶”ì 
            total_checked = 0
            matched_count = 0
            duplicate_count = 0

            for i, url in enumerate(storage.cached_urls):
                # âœ… ê°™ì€ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸ (ì œëª© ê¸°ì¤€ - ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ í¬í•¨!)
                if storage.cached_titles[i] == top_title:
                    total_checked += 1
                    matched_count += 1

                    text = storage.cached_texts[i]
                    content_type = storage.cached_content_types[i] if i < len(storage.cached_content_types) else "unknown"
                    source = storage.cached_sources[i] if i < len(storage.cached_sources) else "unknown"

                    # ë””ë²„ê¹… ë¡œê·¸ (ì²˜ìŒ 5ê°œë§Œ)
                    if matched_count <= 5:
                        html_data = storage.cached_htmls[i] if i < len(storage.cached_htmls) else ""
                        logger.info(f"   [{matched_count}] URL: {url[:80]}...")
                        logger.info(f"       íƒ€ì…: {content_type}, ì†ŒìŠ¤: {source}")
                        logger.info(f"       í…ìŠ¤íŠ¸: {len(text)}ì, HTML: {len(html_data)}ì")
                        logger.info(f"       ì¸ë±ìŠ¤: {i}")

                    # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ì§€ ì•ŠìŒ! (ì¤‘ìš”: "No content"ë„ í¬í•¨)
                    text_key = ''.join(text.split())  # ê³µë°± ì œê±° í›„ ë¹„êµ

                    # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±° (ë¹ˆ ë¬¸ìì—´ì€ ì œì™¸í•˜ì§€ ì•ŠìŒ!)
                    if text_key not in seen_texts:  # âœ… 'text_key and' ì œê±° (ë¹ˆ í…ìŠ¤íŠ¸ë„ í¬í•¨)
                        seen_texts.add(text_key)
                        enriched_docs.append((
                            top_docs[0][0],  # ì ìˆ˜ëŠ” top ë¬¸ì„œì™€ ë™ì¼
                            storage.cached_titles[i],
                            storage.cached_dates[i],
                            text,
                            url,
                            storage.cached_htmls[i] if i < len(storage.cached_htmls) else "",
                            storage.cached_content_types[i] if i < len(storage.cached_content_types) else "unknown",
                            storage.cached_sources[i] if i < len(storage.cached_sources) else "unknown",
                            storage.cached_attachment_types[i] if i < len(storage.cached_attachment_types) else ""
                        ))
                    else:
                        duplicate_count += 1

            logger.info(f"   ğŸ“Š ë§¤ì¹­ í†µê³„: ì „ì²´ {len(storage.cached_urls)}ê°œ ì¤‘ {matched_count}ê°œ ë§¤ì¹­, {duplicate_count}ê°œ ì¤‘ë³µ ì œê±°")

            # ì²­í¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ top_docsë¥¼ êµì²´ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€)
            if enriched_docs:
                logger.info(f"ğŸ”§ ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ìˆ˜ì§‘: {len(top_docs)}ê°œ â†’ {len(enriched_docs)}ê°œ")

                # íƒ€ì…ë³„ ì¹´ìš´íŠ¸ (source ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ì¹´ìš´íŠ¸)
                original_post_count = 0
                image_count = 0
                attachment_count = 0

                for i, (score, title, date, text, url, html, content_type, source, attachment_type) in enumerate(enriched_docs):
                    # âœ… sourceë¥¼ tupleì—ì„œ ì§ì ‘ ì‚¬ìš© (URLë¡œ ì°¾ì§€ ì•ŠìŒ)
                    if source == "original_post":
                        original_post_count += 1
                    elif source == "image_ocr":
                        image_count += 1
                    elif source == "document_parse":
                        attachment_count += 1

                logger.info(f"   ğŸ“¦ ë³¸ë¬¸ ì²­í¬: {original_post_count}ê°œ")
                logger.info(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ OCR ì²­í¬: {image_count}ê°œ")
                logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì²­í¬: {attachment_count}ê°œ")
                top_docs = enriched_docs
            else:
                logger.warning(f"âš ï¸  ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤! wr_id={wr_id}")
                logger.warning(f"   Top URL: {top_url}")

        enrich_f_time = time.time() - enrich_time
        print(f"ì²­í¬ ìˆ˜ì§‘ ì‹œê°„: {enrich_f_time}")

        chain_time=time.time()
        qa_chain, relevant_docs, relevant_docs_content = get_answer_from_chain(top_docs, question, query_noun, temporal_filter)
        chain_f_time=time.time()-chain_time
        print(f"chain ìƒì„±í•˜ëŠ” ì‹œê°„: {chain_f_time}")

        # ğŸ” ë””ë²„ê¹…: get_answer_from_chain ë°˜í™˜ê°’ í™•ì¸
        logger.info(f"ğŸ” get_answer_from_chain ë°˜í™˜ê°’ í™•ì¸:")
        logger.info(f"   qa_chain: {type(qa_chain)} (None? {qa_chain is None})")
        logger.info(f"   relevant_docs: {type(relevant_docs)} (None? {relevant_docs is None}, ê°œìˆ˜: {len(relevant_docs) if relevant_docs else 0})")
        logger.info(f"   relevant_docs_content: {type(relevant_docs_content)} (None? {relevant_docs_content is None})")
        if final_url == PROFESSOR_BASE_URL + "&lang=kor" and any(keyword in query_noun for keyword in ['ì—°ë½ì²˜', 'ì „í™”', 'ë²ˆí˜¸', 'ì „í™”ë²ˆí˜¸']):
            data = {
                "answer": "í•´ë‹¹ êµìˆ˜ë‹˜ì€ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
                "answerable": False,  # ì—°ë½ì²˜ ì •ë³´ ì—†ìŒ
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return data
            
        # prof_title=final_title
        # prof_url=["https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_1"]
        # prof_name=""
        # # ì •ê·œì‹ì„ ì´ìš©í•˜ì—¬ ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ ì¶”ì¶œ
        # if any(final_url.startswith(url) for url in prof_url):
        #     match = re.match(r"^[^\dA-Za-z]+", prof_title)
        #     if match:
        #         prof_name = match.group().strip()  # ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     else:
        #         prof_name = prof_title.strip()  # ìˆ«ìê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     prof_name = re.sub(r"\s+", "", prof_name)
        #     user_question = re.sub(r"\s+", "", question)
        #     if prof_name not in user_question:
        #         refer_url=""
        #         if 'ì§ì›' in query_noun:
        #             refer_url=prof_url[1]
        #         else:
        #             refer_url=prof_url[2]
        #         data = {
        #             "answer": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” êµìˆ˜ë‹˜ ì •ë³´ì…ë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
        #             "references": refer_url,
        #             "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        #             "images": ["No content"]
        #         }
        #         return data

        # ê³µì§€ì‚¬í•­ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        notice_url = NOTICE_BASE_URL
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨
        if not qa_chain or not relevant_docs:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì¡°ê±´ ì§„ì…!")
            logger.warning(f"   ì¡°ê±´: not qa_chain ({not qa_chain}) or not relevant_docs ({not relevant_docs})")
            logger.warning(f"   â†’ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ ì˜ˆì •")
            # Reranker ì ìˆ˜ëŠ” ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ final_score < 0ì´ë©´ ìœ ì‚¬ë„ ì²´í¬ ìŠ¤í‚µ
            if final_image[0] != "No content" and (final_score < 0 or final_score > MINIMUM_SIMILARITY_SCORE):
                data = {
                    "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "answerable": True,  # ì´ë¯¸ì§€ë¡œ ë‹µë³€ ì œê³µ
                    "references": final_url,
                    "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "images": final_image
                }
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return data
            else:
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return not_in_notices_response

        # ìœ ì‚¬ë„ê°€ ë‚®ì€ ê²½ìš° (ë‹¨, Reranker ì ìˆ˜ëŠ” ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²´í¬ ìŠ¤í‚µ)
        # BGE-Reranker ì ìˆ˜ ë²”ìœ„: ì•½ -10 ~ +10 (ìŒìˆ˜ë„ ì •ìƒ)
        # BM25 ì ìˆ˜ ë²”ìœ„: 0 ~ ë¬´í•œëŒ€ (í•­ìƒ ì–‘ìˆ˜)
        if final_score >= 0 and final_score < MINIMUM_SIMILARITY_SCORE:
            logger.warning(f"âš ï¸ ìœ ì‚¬ë„ ì¡°ê±´ ì§„ì…!")
            logger.warning(f"   final_score ({final_score:.4f}) < MINIMUM_SIMILARITY_SCORE ({MINIMUM_SIMILARITY_SCORE})")
            logger.warning(f"   â†’ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return not_in_notices_response
        elif final_score < 0:
            logger.info(f"âœ… Reranker ì ìˆ˜ ê°ì§€ ({final_score:.4f}) â†’ ìœ ì‚¬ë„ ì²´í¬ ìŠ¤í‚µ")

        # LLMì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²½ìš°
        logger.info(f"âœ… ëª¨ë“  ì¡°ê±´ í†µê³¼! LLM ë‹µë³€ ìƒì„± ì‹œì‘...")
        answer_time=time.time()

        # qa_chain.invoke() ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        answer_result = qa_chain.invoke(question)

        answer_f_time=time.time()-answer_time
        print(f"ë‹µë³€ ìƒì„±í•˜ëŠ” ì‹œê°„: {answer_f_time}")

        # âœ… JSON íŒŒì‹± ì‹œë„ (LLMì´ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ í™•ì¸)
        import json
        import re

        llm_answerable = None  # LLMì´ íŒë‹¨í•œ answerable ê°’
        llm_answer_text = None  # LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸

        try:
            # JSON íŒŒì‹± ì‹œë„
            # LLMì´ ê°€ë” ```json...``` ë¡œ ê°ìŒ€ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
            clean_result = answer_result.strip()
            if clean_result.startswith("```json"):
                clean_result = clean_result[7:]
            if clean_result.startswith("```"):
                clean_result = clean_result[3:]
            if clean_result.endswith("```"):
                clean_result = clean_result[:-3]
            clean_result = clean_result.strip()

            parsed = json.loads(clean_result)

            # JSON íŒŒì‹± ì„±ê³µ
            if "answerable" in parsed and "answer" in parsed:
                llm_answerable = parsed["answerable"]
                llm_answer_text = parsed["answer"]
                logger.info(f"âœ… JSON íŒŒì‹± ì„±ê³µ: answerable={llm_answerable}")
                logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
                logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")
            else:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ â†’ í´ë°± ì‚¬ìš©")

        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (LLMì´ í˜•ì‹ ì•ˆ ì§€í‚´) â†’ í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš©")
            logger.debug(f"   ì—ëŸ¬: {e}")
            logger.debug(f"   ì›ë³¸ ì‘ë‹µ: {answer_result[:200]}...")

        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ: ê¸°ì¡´ answer_result ì‚¬ìš©
        if llm_answer_text is None:
            llm_answer_text = answer_result
            logger.info(f"ğŸ’¬ LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ë¹„-JSON í˜•ì‹):")
            logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
            logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")

        logger.info(f"   ì‚¬ìš©ëœ ì°¸ê³ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")

        # ë‹µë³€ ê²€ì¦ ë° ê²½ê³  ì¶”ê°€ (ë²”ìš©)
        completeness_keywords = ['ì „ë¶€', 'ëª¨ë“ ', 'ëª¨ë‘', 'ë¹ ì§ì—†ì´', 'ì „ì²´', 'ë‹¤', 'ëª…ë‹¨', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ëˆ„êµ¬']
        has_completeness_request = any(keyword in question for keyword in completeness_keywords)

        # ì™„ì „ì„± ìš”êµ¬ + Contextì™€ ë‹µë³€ ì°¨ì´ê°€ í¬ë©´ ê²½ê³ 
        if has_completeness_request:
            # Contextì— ìˆëŠ” ìˆ«ì íŒ¨í„´ (í•™ë²ˆ, ë‚ ì§œ ë“±)
            context_numbers = len(re.findall(r'\b20\d{6,8}\b', relevant_docs_content))
            answer_numbers = len(re.findall(r'\b20\d{6,8}\b', llm_answer_text))

            logger.info(f"   ğŸ“Š ì™„ì „ì„± ê²€ì¦: Context {context_numbers}ê±´ / ë‹µë³€ {answer_numbers}ê±´")

            # Contextì˜ 50% ë¯¸ë§Œë§Œ ë‹µë³€ì— í¬í•¨ë˜ë©´ ê²½ê³ 
            if context_numbers >= 10 and answer_numbers < context_numbers * 0.5:
                logger.warning(f"   âš ï¸ ì™„ì „ì„± ìš”êµ¬í–ˆìœ¼ë‚˜ ë‹µë³€ ë¶ˆì™„ì „! LLMì´ ì„ì˜ë¡œ ìš”ì•½í•œ ê²ƒìœ¼ë¡œ íŒë‹¨")
                llm_answer_text += f"\n\nâš ï¸ ì¼ë¶€ ë‚´ìš©ì´ ìƒëµë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸ì„œ: ì•½ {context_numbers}ê±´ / ë‹µë³€: {answer_numbers}ê±´). ì „ì²´ ë‚´ìš©ì€ ì°¸ê³  URLì„ í™•ì¸í•˜ì„¸ìš”."

        doc_references = "\n".join([
            f"\nì°¸ê³  ë¬¸ì„œ URL: {doc.metadata['url']}"
            for doc in relevant_docs[:1] if doc.metadata.get('url') != 'No URL'
        ])

        # âœ… answerable ìµœì¢… íŒë‹¨
        if llm_answerable is not None:
            # JSON íŒŒì‹± ì„±ê³µ â†’ LLMì´ ì§ì ‘ íŒë‹¨í•œ ê°’ ì‚¬ìš©
            answerable = llm_answerable
            logger.info(f"âœ… answerable íŒë‹¨: JSON íŒŒì‹± ê²°ê³¼ ì‚¬ìš© (LLM ì§ì ‘ íŒë‹¨: {answerable})")
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨ â†’ í´ë°±: íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ íŒë‹¨
            answer_start = llm_answer_text[:150]
            if answer_start.startswith("ì œê³µëœ ë¬¸ì„œì—ëŠ”") and any(phrase in answer_start for phrase in ["ì—†ìŠµë‹ˆë‹¤", "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"]):
                answerable = False
            else:
                answerable = True
            logger.info(f"âš ï¸ answerable íŒë‹¨: í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš© (ê²°ê³¼: {answerable})")

        if answerable:
            logger.info("âœ… LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤")
        else:
            logger.info("âŒ LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ˆë¬¸ ì‘ì„± ìš”ì²­ ì•ˆë‚´ í‘œì‹œ)")

        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        data = {
            "answer": llm_answer_text,  # JSON íŒŒì‹±ëœ ë‹µë³€ ë˜ëŠ” ì›ë³¸ ë‹µë³€
            "answerable": answerable,  # ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€
            "references": doc_references,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        logger.info(f"âœ… ì´ ì²˜ë¦¬ ì‹œê°„: {f_time:.2f}ì´ˆ")
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return data
"""