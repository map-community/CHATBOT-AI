import os
import re
import time
import pickle
import logging
from datetime import datetime
from collections import defaultdict
import numpy as np
import pytz
from dotenv import load_dotenv
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
from langchain import hub
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import Runnable, RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableSequence, RunnableMap
from langchain_core.runnables import RunnableLambda
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from pymongo import MongoClient

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Mecab import (logger ì •ì˜ ì´í›„)
try:
    from konlpy.tag import Mecab
    MECAB_AVAILABLE = True
    logger.info("âœ… Mecab ì‚¬ìš© ê°€ëŠ¥ (30-50ë°° ë¹ ë¥¸ í˜•íƒœì†Œ ë¶„ì„)")
except Exception as e:
    logger.warning(f"âš ï¸  Mecabì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    logger.warning("âš ï¸  Mecab ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì •í™•ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    MECAB_AVAILABLE = False
    Mecab = None

# StorageManager import
from modules.storage_manager import get_storage_manager

# Configuration import
from config.settings import MINIMUM_SIMILARITY_SCORE
from config.prompts import get_qa_prompt, get_temporal_intent_prompt
from config.ml_settings import get_ml_config

# Utils import
from modules.utils.date_utils import get_current_kst as get_korean_time, parse_date_change_korea_time
from modules.utils.url_utils import find_url
from modules.utils.formatter import format_temporal_intent, format_docs

# StorageManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
storage = get_storage_manager()

# ML ì„¤ì • ë¡œë“œ
ml_config = get_ml_config()

# URL ìƒìˆ˜
NOTICE_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
COMPANY_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b"
SEMINAR_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4"
PROFESSOR_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2"

# ë‹¨ì–´ ëª…ì‚¬í™” í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - QueryTransformer ì‚¬ìš©)
def transformed_query(content):
    """
    ì§ˆë¬¸ì„ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        content: ì‚¬ìš©ì ì§ˆë¬¸ (ì›ë¬¸)

    Returns:
        List[str]: ì¶”ì¶œëœ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    return storage.query_transformer.transform(content)
###################################################################################################


# Dense Retrieval (Upstage ì„ë² ë”©) - Lazy initializationìœ¼ë¡œ í•¨ìˆ˜ ë‚´ì—ì„œ ìƒì„±í•˜ë„ë¡ ë³€ê²½
# embeddings ê°ì²´ëŠ” í•„ìš”í•  ë•Œ get_embeddings() í•¨ìˆ˜ë¥¼ í†µí•´ ê°€ì ¸ì˜µë‹ˆë‹¤.
def get_embeddings():
    """Upstage Embeddings ê°ì²´ ë°˜í™˜ (Lazy initialization)"""
    return UpstageEmbeddings(
        api_key=storage.upstage_api_key,
        model="solar-embedding-1-large-query"  # ì§ˆë¬¸ ì„ë² ë”©ìš© ëª¨ë¸
    )
# dense_doc_vectors = np.array(embeddings.embed_documents(texts))  # ë¬¸ì„œ ì„ë² ë”©

def fetch_titles_from_pinecone():
    """
    Pineconeì—ì„œ ì „ì²´ ë°ì´í„°(ì œëª©, í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    - list() ë©”ì„œë“œ(Pagination)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œìˆ˜ ì œí•œ ì—†ì´ ëª¨ë“  IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - fetch() ë©”ì„œë“œ(Batch)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - html_available=trueì¸ ê²½ìš° MongoDBì—ì„œ ì‹¤ì œ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    logger.info("ğŸ”„ Pinecone ì „ì²´ ë°ì´í„° ì¡°íšŒ ì‹œì‘...")

    # ==========================================
    # MongoDB ì—°ê²° (HTML ì¡°íšŒìš©)
    # ==========================================
    mongo_collection = None
    mongo_client = None
    try:
        if storage.mongo_collection is not None:
            # StorageManagerì˜ MongoDB connection ì‚¬ìš©
            mongo_collection = storage.mongo_collection.database["multimodal_cache"]
            logger.info("âœ… MongoDB ì—°ê²° ì„±ê³µ (HTML ì¡°íšŒìš©)")
    except Exception as e:
        logger.warning(f"âš ï¸  MongoDB ì—°ê²° ì‹¤íŒ¨ (HTML ì—†ì´ ì§„í–‰): {e}")

    # ==========================================
    # 1. ì „ì²´ ID ê°€ì ¸ì˜¤ê¸° (ê°œìˆ˜ ì œí•œ ì—†ìŒ!)
    # ==========================================
    all_ids = []
    
    try:
        # namespaceê°€ ìˆë‹¤ë©´ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’ "")
        # list()ëŠ” ì „ì²´ IDë¥¼ í˜ì´ì§€ë„¤ì´ì…˜í•˜ì—¬ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
        for ids in storage.pinecone_index.list(namespace=""): 
            all_ids.extend(ids)
        logger.info(f"ğŸ“Š ì´ {len(all_ids)}ê°œì˜ ë²¡í„° IDë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ ID ë¦¬ìŠ¤íŒ… ì‹¤íŒ¨: {e}")
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ë¬¸ì œì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆë‚´
        logger.error("ğŸ‘‰ 'requirements.txt'ì˜ pinecone ë²„ì „ì„ í™•ì¸í•˜ê³  ì¬ë¹Œë“œí•˜ì„¸ìš”.")
        return [], [], [], [], [], [], [], [], [], []

    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
    if not all_ids:
        logger.warning("âš ï¸ ì¡°íšŒëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤.")
        return [], [], [], [], [], [], [], [], [], []


    # ==========================================
    # 2. IDë¡œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Batch Fetch)
    # ==========================================
    # ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    titles = []
    texts = []
    urls = []
    dates = []
    htmls = []
    content_types = []
    sources = []
    image_urls = []
    attachment_urls = []
    attachment_types = []

    # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë°°ì¹˜ í¬ê¸°
    batch_size = 1000

    # ë””ë²„ê¹… ì¹´ìš´í„° ì¶”ê°€
    html_available_count = 0
    mongo_found_count = 0
    html_extracted_count = 0

    # 1,000ê°œì”© ëŠì–´ì„œ ìš”ì²­
    for i in range(0, len(all_ids), batch_size):
        logger.info(f"â³ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({i} / {len(all_ids)})")
        
        batch_ids = all_ids[i:i + batch_size]
        
        try:
            # Fetch ìš”ì²­
            fetch_response = storage.pinecone_index.fetch(ids=batch_ids)
            
            # ì‘ë‹µ ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (v3 í˜¸í™˜ì„± í•´ê²°)
            vectors = {}
            if hasattr(fetch_response, 'to_dict'):
                response_dict = fetch_response.to_dict()
                vectors = response_dict.get('vectors', {})
            elif hasattr(fetch_response, 'vectors'):
                vectors = fetch_response.vectors
            else:
                vectors = fetch_response.get('vectors', {})

            if vectors is None:
                vectors = {}
            
            # ê°€ì ¸ì˜¨ ë°ì´í„° íŒŒì‹±
            for vector_id in batch_ids:
                if vector_id in vectors:
                    vector_data = vectors[vector_id]
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    if isinstance(vector_data, dict):
                        metadata = vector_data.get('metadata', {})
                    elif hasattr(vector_data, 'metadata'):
                        metadata = vector_data.metadata
                    else:
                        metadata = {}

                    if metadata is None:
                        metadata = {}
                    
                    # ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„° ì¶”ê°€
                    titles.append(metadata.get("title", ""))
                    texts.append(metadata.get("text", ""))
                    url = metadata.get("url", "")
                    urls.append(url)
                    dates.append(metadata.get("date", ""))

                    # ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„°: html_availableì´ë©´ MongoDBì—ì„œ HTML ì¡°íšŒ
                    html = ""
                    if metadata.get("html_available"):
                        html_available_count += 1
                        if mongo_collection is not None:
                            try:
                                # html_available=trueì¸ chunkëŠ” ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ì—ì„œ ì¶”ì¶œëœ ê²ƒ
                                # MongoDB cacheëŠ” image_url ë˜ëŠ” attachment_urlì„ keyë¡œ ì‚¬ìš©
                                lookup_url = metadata.get("image_url") or metadata.get("attachment_url")

                                if lookup_url:
                                    # ë””ë²„ê¹…: URL ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
                                    if html_available_count <= 3:
                                        logger.info(f"ğŸ” ì¡°íšŒ ì‹œë„ URL: {lookup_url[:80]}...")

                                    cached = mongo_collection.find_one({"url": lookup_url})
                                    if cached:
                                        mongo_found_count += 1
                                        # ë””ë²„ê¹…: ì°¾ì€ ê²½ìš° ë¡œê¹…
                                        if mongo_found_count <= 3:
                                            logger.info(f"âœ… MongoDBì—ì„œ ë°œê²¬: {lookup_url[:80]}...")
                                            logger.info(f"   í•„ë“œ: {list(cached.keys())}")

                                        # Markdown ìš°ì„  (Upstage API ì œê³µ, ê³ í’ˆì§ˆ í‘œ êµ¬ì¡°)
                                        # ì´ë¯¸ì§€: ocr_markdown, ë¬¸ì„œ: markdown
                                        markdown_content = cached.get("ocr_markdown") or cached.get("markdown", "")

                                        # Markdownì´ ì—†ìœ¼ë©´ HTML ì‚¬ìš© (fallback)
                                        html_content = markdown_content or cached.get("ocr_html") or cached.get("html", "")

                                        if html_content:
                                            html = html_content
                                            html_extracted_count += 1
                                    else:
                                        # ë””ë²„ê¹…: ëª» ì°¾ì€ ê²½ìš° ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
                                        if html_available_count <= 3:
                                            logger.warning(f"âŒ MongoDBì—ì„œ ëª» ì°¾ìŒ: {lookup_url[:80]}...")
                                else:
                                    # image_urlê³¼ attachment_urlì´ ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°
                                    if html_available_count <= 3:
                                        logger.warning(f"âš ï¸  html_available=trueì¸ë° image_url/attachment_url ì—†ìŒ (board URL: {url[:80]}...)")
                            except Exception as e:
                                logger.warning(f"MongoDB HTML ì¡°íšŒ ì‹¤íŒ¨: {e}")

                    htmls.append(html)
                    content_types.append(metadata.get("content_type", "text"))
                    sources.append(metadata.get("source", "original_post"))
                    image_urls.append(metadata.get("image_url", ""))
                    attachment_urls.append(metadata.get("attachment_url", ""))
                    attachment_types.append(metadata.get("attachment_type", ""))
                    
        except Exception as e:
            logger.error(f"âš ï¸ ë°°ì¹˜ Fetch ì‹¤íŒ¨ ({i}~{i+batch_size}): {e}")
            continue

    logger.info(f"âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(titles)}ê°œ ë¬¸ì„œ")
    logger.info(f"ğŸ“Š HTML ì¡°íšŒ í†µê³„:")
    logger.info(f"   - html_available=true ë¬¸ì„œ: {html_available_count}ê°œ")
    logger.info(f"   - MongoDBì—ì„œ ì°¾ì€ ë¬¸ì„œ: {mongo_found_count}ê°œ")
    logger.info(f"   - ì‹¤ì œ HTML ì¶”ì¶œ ì„±ê³µ: {html_extracted_count}ê°œ")

    return titles, texts, urls, dates, htmls, content_types, sources, image_urls, attachment_urls, attachment_types


# ìºì‹± ë°ì´í„° ì´ˆê¸°í™” í•¨ìˆ˜

def initialize_cache():
    """
    ìºì‹œ ì´ˆê¸°í™” í•¨ìˆ˜ (Redis Fast Track ì ìš©)
    - Redis ìºì‹œê°€ ìˆìœ¼ë©´ 3ì´ˆ ë¡œë”©
    - ì—†ìœ¼ë©´ Pineconeì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ Redisì— ì €ì¥ (20ë¶„ ì†Œìš”, ìµœì´ˆ 1íšŒë§Œ)
    """
    try:
        logger.info("ğŸ”„ ìºì‹œ ì´ˆê¸°í™” ì‹œì‘...")

        # ==========================================
        # 1. Redis ìºì‹œ í™•ì¸ (Fast Track)
        # ==========================================
        if storage.redis_client is not None:
            try:
                logger.info("ğŸ” Redis ìºì‹œ í™•ì¸ ì¤‘...")
                cached_data = storage.redis_client.get('pinecone_metadata')

                if cached_data:
                    logger.info("ğŸš€ Redis ìºì‹œ ë°œê²¬! ë¹ ë¥¸ ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

                    # Pickleë¡œ ì €ì¥ëœ ë°ì´í„° ë³µì›
                    (storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
                     storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
                     storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types) = pickle.loads(cached_data)

                    logger.info(f"âœ… Redis ë¡œë“œ ì™„ë£Œ! ({len(storage.cached_titles)}ê°œ ë¬¸ì„œ, Pinecone ë‹¤ìš´ë¡œë“œ ìƒëµ)")
                    logger.info(f"   - HTML êµ¬ì¡° ìˆëŠ” ë¬¸ì„œ: {sum(1 for html in storage.cached_htmls if html)}ê°œ")
                    logger.info(f"   - ì´ë¯¸ì§€ OCR ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'image')}ê°œ")
                    logger.info(f"   - ì²¨ë¶€íŒŒì¼ ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'attachment')}ê°œ")

                    # Retriever ì´ˆê¸°í™”ë¡œ ì í”„ (Pinecone Fetch ìƒëµ!)
                    _initialize_retrievers()
                    logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(storage.cached_titles)}, texts: {len(storage.cached_texts)})")
                    return
                else:
                    logger.info("â¬‡ï¸  Redisì— ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

            except Exception as e:
                logger.warning(f"âš ï¸  Redis ë¡œë“œ ì‹¤íŒ¨ (Pineconeì—ì„œ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤): {e}")

        # ==========================================
        # 2. Pineconeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Slow Track)
        # ==========================================
        logger.info("â³ Pinecone ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ìµœì´ˆ 1íšŒ, ì•½ 20ë¶„ ì†Œìš”)...")
        (storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
         storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
         storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types) = fetch_titles_from_pinecone()
        logger.info(f"âœ… Pineconeì—ì„œ {len(storage.cached_titles)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        logger.info(f"   - HTML êµ¬ì¡° ìˆëŠ” ë¬¸ì„œ: {sum(1 for html in storage.cached_htmls if html)}ê°œ")
        logger.info(f"   - ì´ë¯¸ì§€ OCR ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'image')}ê°œ")
        logger.info(f"   - ì²¨ë¶€íŒŒì¼ ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'attachment')}ê°œ")

        # ==========================================
        # 3. Redisì— ì €ì¥ (ë‹¤ìŒ ì¬ì‹œì‘ì„ ìœ„í•´)
        # ==========================================
        if storage.redis_client is not None:
            try:
                cache_data = (
                    storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
                    storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
                    storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types
                )
                # 24ì‹œê°„ ìœ íš¨ (86400ì´ˆ)
                storage.redis_client.setex('pinecone_metadata', 86400, pickle.dumps(cache_data))
                logger.info("ğŸ’¾ ë°ì´í„°ë¥¼ Redisì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ë‹¤ìŒ ì¬ì‹œì‘ë¶€í„°ëŠ” 3ì´ˆ ë¡œë”©!)")
            except Exception as e:
                logger.warning(f"âš ï¸  Redis ì €ì¥ ì‹¤íŒ¨ (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©): {e}")
        else:
            logger.warning("âš ï¸  Redis ë¯¸ì‚¬ìš© (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©)")

        # Retriever ì´ˆê¸°í™”
        _initialize_retrievers()
        logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(storage.cached_titles)}, texts: {len(storage.cached_texts)})")

    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì•±ì´ í¬ë˜ì‹œí•˜ì§€ ì•Šë„ë¡ í•¨
        storage.cached_titles = []
        storage.cached_texts = []
        storage.cached_urls = []
        storage.cached_dates = []
        storage.cached_htmls = []
        storage.cached_content_types = []
        storage.cached_sources = []
        storage.cached_image_urls = []
        storage.cached_attachment_urls = []
        storage.cached_attachment_types = []
        logger.warning("âš ï¸  ìºì‹œë¥¼ ë¹ˆ ìƒíƒœë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")


def _initialize_retrievers():
    """Retriever ì´ˆê¸°í™” ë¡œì§ (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë¶„ë¦¬)"""
    logger.info("ğŸ”§ ê²€ìƒ‰ ì—”ì§„(BM25/Dense) êµ¬ì¶• ì¤‘...")

    from modules.retrieval import (
        BM25Retriever,
        DenseRetriever,
        DocumentCombiner,
        DocumentClusterer
    )

    # BM25Retriever ì´ˆê¸°í™” (HTML ë°ì´í„° í¬í•¨, Redis ìºì‹±)
    bm25_retriever = BM25Retriever(
        titles=storage.cached_titles,
        texts=storage.cached_texts,
        urls=storage.cached_urls,
        dates=storage.cached_dates,
        query_transformer=transformed_query,
        similarity_adjuster=adjust_similarity_scores,
        htmls=storage.cached_htmls,  # HTML êµ¬ì¡°í™” ë°ì´í„° ì¶”ê°€
        k1=ml_config.bm25.k1,
        b=ml_config.bm25.b,
        redis_client=storage.redis_client  # Redis ìºì‹± í™œì„±í™”
    )
    storage.set_bm25_retriever(bm25_retriever)

    # DenseRetriever ì´ˆê¸°í™”
    dense_retriever = DenseRetriever(
        embeddings_factory=get_embeddings,
        pinecone_index=storage.pinecone_index,
        date_adjuster=adjust_date_similarity,
        similarity_scale=ml_config.dense_retrieval.similarity_scale,
        noun_weight=ml_config.dense_retrieval.noun_weight,
        digit_weight=ml_config.dense_retrieval.digit_weight
    )
    storage.set_dense_retriever(dense_retriever)

    # DocumentCombiner ì´ˆê¸°í™”
    document_combiner = DocumentCombiner(
        keyword_filter=last_filter_keyword,
        date_adjuster=adjust_date_similarity
    )
    storage.set_document_combiner(document_combiner)

    # DocumentClusterer ì´ˆê¸°í™”
    document_clusterer = DocumentClusterer(
        date_parser=parse_date_change_korea_time,
        similarity_threshold=ml_config.clustering.similarity_threshold
    )
    storage.set_document_clusterer(document_clusterer)

    logger.info("âœ… ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")

                    #################################   24.11.16ê¸°ì¤€ ì •í™•ë„ ì¸¡ì •ì™„ë£Œ #####################################################
######################################################################################################################

# ë‚ ì§œë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
# ì´ì œëŠ” utils.date_utils.parse_date_change_korea_time ì‚¬ìš© ê¶Œì¥

def calculate_weight_by_days_difference(post_date, current_date, query_nouns):

    # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
    days_diff = (current_date - post_date).days

    # ê¸°ì¤€ ë‚ ì§œ (24-01-01 00:00) ì„¤ì •
    baseline_date_str = "24-01-01 00:00"
    baseline_date = parse_date_change_korea_time(baseline_date_str)
    graduate_weight = 1.0 if any(keyword in query_nouns for keyword in ['ì¡¸ì—…', 'ì¸í„°ë·°']) else 0
    scholar_weight = 1.0 if 'ì¥í•™' in query_nouns else 0
    # ì‘ì„±ì¼ì´ ê¸°ì¤€ ë‚ ì§œ ì´ì „ì´ë©´ ê°€ì¤‘ì¹˜ë¥¼ 1.35ë¡œ ê³ ì •
    if post_date <= baseline_date:
        return 1.35 + graduate_weight / 5

    # 'ìµœê·¼', 'ìµœì‹ ' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°, ìµœê·¼ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€
    add_recent_weight = 1.5 if any(keyword in query_nouns for keyword in ['ìµœê·¼', 'ìµœì‹ ', 'ì§€ê¸ˆ', 'í˜„ì¬']) else 0

    # **10ì¼ ë‹¨ìœ„ êµ¬ë¶„**: ìµœê·¼ ë¬¸ì„œì— ëŒ€í•œ ì„¸ë°€í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    if days_diff <= 6:
        return 1.355 + add_recent_weight + graduate_weight + scholar_weight
    elif days_diff <= 12:
        return 1.330 + add_recent_weight / 3.0 + graduate_weight / 1.2 + scholar_weight / 1.5
    elif days_diff <= 18:
        return 1.321 + add_recent_weight / 5.0 + graduate_weight / 1.3 + scholar_weight / 2.0
    elif days_diff <= 24:
        return 1.310 + add_recent_weight / 7.0 + graduate_weight / 1.4 + scholar_weight / 2.5
    elif days_diff <= 30:
        return 1.290 + add_recent_weight / 9.0 + graduate_weight / 1.5 + scholar_weight / 3.0
    elif days_diff <= 36:
        return 1.270 + graduate_weight / 1.6 + scholar_weight / 3.5
    elif days_diff <= 45:
        return 1.250 +graduate_weight / 1.7 + scholar_weight / 4.0
    elif days_diff <= 60:
        return 1.230 +graduate_weight / 1.8 + scholar_weight / 4.5
    elif days_diff <= 90:
        return 1.210 +graduate_weight / 2.0 + scholar_weight / 5.0

    # **ì›” ë‹¨ìœ„ êµ¬ë¶„**: 2ê°œì›” ì´í›„ëŠ” ì›” ë‹¨ìœ„ë¡œ ë‹¨ìˆœí™”
    month_diff = (days_diff - 90) // 30
    month_weight_map = {
        0: 1.19,
        1: 1.17 - add_recent_weight / 6 - scholar_weight / 10,
        2: 1.15 - add_recent_weight / 5 - scholar_weight / 9,
        3: 1.13 - add_recent_weight / 4 - scholar_weight / 7,
        4: 1.11 - add_recent_weight / 3  - scholar_weight / 5,
    }

    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë°˜í™˜ (6ê°œì›” ì´í›„)
    return month_weight_map.get(month_diff, 0.88 - add_recent_weight /2  - scholar_weight / 5)


# ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜

def adjust_date_similarity(similarity, date_str,query_nouns):
    # í˜„ì¬ í•œêµ­ ì‹œê°„
    current_time = get_korean_time()
    # ì‘ì„±ì¼ íŒŒì‹±
    post_date = parse_date_change_korea_time(date_str)
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    weight = calculate_weight_by_days_difference(post_date, current_time,query_nouns)
    # ì¡°ì •ëœ ìœ ì‚¬ë„ ë°˜í™˜
    return similarity * weight

# ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ ëª…ì‚¬ì™€ ê° ë¬¸ì„œ ì œëª©ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜
# (ì´ì „ ë²„ì „ì€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤ - ìµœì í™”ëœ ë²„ì „ë§Œ ìœ ì§€)

def adjust_similarity_scores(query_noun, title, texts, similarities):
    query_noun_set = set(query_noun)
    title_tokens = [set(titl.split()) for titl in title]

    for idx, titl_tokens in enumerate(title_tokens):
        matching_noun = query_noun_set.intersection(titl_tokens)
        
        if texts[idx] == "No content":
            similarities[idx] *= 1.5
            if "êµ­ê°€ì¥í•™ê¸ˆ" in query_noun_set and "êµ­ê°€ì¥í•™ê¸ˆ" in titl_tokens:
                similarities[idx] *= 5.0
        
        for noun in matching_noun:
            len_adjustment = len(noun) * 0.21
            similarities[idx] += len_adjustment
            if re.search(r'\d', noun):  # ìˆ«ì í¬í•¨ ì—¬ë¶€
                similarities[idx] += len(noun) * (0.22 if noun in titl_tokens else 0.19)

        if query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and titl_tokens.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}):
            similarities[idx] += 2.0
        if not query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and 'ëŒ€í•™ì›' in titl_tokens:
            similarities[idx] -= 2.0

    return similarities


#############################################################################################

# í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - KeywordFilter ì‚¬ìš©)
def last_filter_keyword(DOCS, query_noun, user_question):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§

    Args:
        DOCS: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ [(score, title, date, text, url), ...]
        query_noun: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
        user_question: ì›ë³¸ ì§ˆë¬¸

    Returns:
        List[Tuple]: í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ì¡°ì •ë¨)
    """
    return storage.keyword_filter.filter(DOCS, query_noun, user_question)

#################################################################################################

########################################################################################  best_docs ì‹œì‘ ##########################################################################################

def parse_temporal_intent(query, current_date=None):
    """
    ì§ˆë¬¸ì—ì„œ ì‹œê°„ í‘œí˜„ì„ ê°ì§€í•˜ê³  í•„í„° ì¡°ê±´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        current_date: í˜„ì¬ ë‚ ì§œ (ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°)

    Returns:
        dict: {"year": int, "semester": int, "date_from": datetime} ë˜ëŠ” None
    """
    from datetime import datetime

    if current_date is None:
        current_date = datetime.now()

    current_year = current_date.year
    current_month = current_date.month

    # í•œêµ­ í•™ê¸° ê³„ì‚°: 1í•™ê¸°(3-8ì›”), 2í•™ê¸°(9-2ì›”)
    # ë‹¨, 1-2ì›”ì€ ì „ë…„ë„ 2í•™ê¸°ë¡œ ê°„ì£¼
    if 3 <= current_month <= 8:
        current_semester = 1
    else:  # 9-12ì›” ë˜ëŠ” 1-2ì›”
        current_semester = 2
        if current_month <= 2:
            current_year -= 1  # 1-2ì›”ì€ ì „ë…„ë„ 2í•™ê¸°

    # 1ë‹¨ê³„: ê°„ë‹¨í•œ ì‹œê°„ í‘œí˜„ì€ ê·œì¹™ìœ¼ë¡œ ì²˜ë¦¬ (ë¹ ë¥´ê³  ë¹„ìš© 0)
    simple_temporal_keywords = {
        'ì´ë²ˆí•™ê¸°': {'year': current_year, 'semester': current_semester},
        'ì´ë²ˆ í•™ê¸°': {'year': current_year, 'semester': current_semester},
        'ì´ë²ˆí•™ë…„': {'year': current_year, 'semester': current_semester},
        'ì´ë²ˆ í•™ë…„': {'year': current_year, 'semester': current_semester},
        'ì˜¬í•´': {'year': current_year},
        'ê¸ˆë…„': {'year': current_year},
        'ìµœê·¼': {'year_from': current_year - 1},  # ìµœê·¼ 1ë…„
    }

    for keyword, time_filter in simple_temporal_keywords.items():
        if keyword in query:
            logger.info(f"â° ì‹œê°„ í‘œí˜„ ê°ì§€ (ê·œì¹™): '{keyword}' â†’ {time_filter}")
            return time_filter

    # 2ë‹¨ê³„: ëª¨ë“  ì§ˆë¬¸ì„ LLMìœ¼ë¡œ ë¶„ì„ (ì‹œê°„ ì˜ë„ íŒŒì•…)
    # í‚¤ì›Œë“œ ì²´í¬ ì œê±° â†’ ëª¨ë“  ì§ˆë¬¸ì—ì„œ ì‹œê°„ ì˜ë„ ê°ì§€
    # ì˜ˆ: "ì¸í„´ì‹­ ìˆì–´?" â†’ ì•”ë¬µì ìœ¼ë¡œ í˜„ì¬ ì§„í–‰ì¤‘ì¸ ê²ƒì„ ë¬»ëŠ” ê²ƒ
    logger.info(f"ğŸ¤” LLMìœ¼ë¡œ ì‹œê°„ ì˜ë„ ë¶„ì„ ì¤‘...")
    llm_filter = rewrite_query_with_llm(query, current_date)
    if llm_filter:
        logger.info(f"âœ¨ LLM ë¶„ì„ ê²°ê³¼: {llm_filter}")
        return llm_filter

    return None


def rewrite_query_with_llm(query, current_date):
    """
    LLMì„ ì‚¬ìš©í•´ ë³µì¡í•œ ì‹œê°„ í‘œí˜„ì„ í•´ì„í•˜ê³  í•„í„° ì¡°ê±´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        current_date: í˜„ì¬ ë‚ ì§œ

    Returns:
        dict: {"year": int, "semester": int} ë˜ëŠ” None
    """
    from datetime import datetime
    import json

    current_year = current_date.year
    current_month = current_date.month

    # í˜„ì¬ í•™ê¸° ê³„ì‚°
    if 3 <= current_month <= 8:
        current_semester = 1
    else:
        current_semester = 2
        if current_month <= 2:
            current_year -= 1

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
    prompt_template = get_temporal_intent_prompt()

    # ë™ì  ê°’ ê³„ì‚°
    prev_year = current_year if current_semester == 2 else current_year - 1
    prev_semester = 2 if current_semester == 1 else 1
    last_year = current_year - 1

    # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
    prompt = prompt_template.format(
        current_date=current_date.strftime('%Yë…„ %mì›” %dì¼'),
        current_semester=f"{current_year}í•™ë…„ë„ {current_semester}í•™ê¸°",
        query=query,
        prev_year=prev_year,
        prev_semester=prev_semester,
        last_year=last_year
    )

    try:
        llm = ChatUpstage(api_key=storage.upstage_api_key, model="solar-mini")
        response = llm.invoke(prompt)

        # JSON íŒŒì‹±
        result = json.loads(response.content.strip())

        # ë¡œê·¸: LLM ì‘ë‹µ JSON ì „ì²´
        logger.info(f"   ğŸ“‹ LLM ì‘ë‹µ JSON: {json.dumps(result, ensure_ascii=False)}")

        # ë¡œê·¸: LLM ì¶”ë¡  ê³¼ì •
        logger.info(f"   ğŸ’¬ LLM ì‹œê°„ ë¶„ì„: {result.get('reasoning', '')}")

        # âœ… ìƒˆë¡œìš´ í•„ë“œ ì¶”ì¶œ
        is_ongoing = result.get('is_ongoing', False)
        is_policy = result.get('is_policy', False)
        year = result.get('year')
        semester = result.get('semester')

        # í•„í„° ì¡°ê±´ ìƒì„±
        if is_ongoing:
            # "ì§„í–‰ì¤‘" ì˜ë„ ê°ì§€
            logger.info(f"   ğŸ¯ 'ì§„í–‰ì¤‘' ì˜ë„ ê°ì§€ë¨ (is_ongoing=true)")
            return {
                'type': 'ongoing',
                'is_ongoing': True,
                'is_policy': is_policy
            }

        elif year is not None and semester is not None:
            # í•™ê¸° í•„í„° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            logger.info(f"   ğŸ“… í•™ê¸° í•„í„°: {year}í•™ë…„ë„ {semester}í•™ê¸°")
            return {
                'year': year,
                'semester': semester,
                'is_ongoing': False,
                'is_policy': is_policy
            }

        elif is_policy:
            # ì •ì±… ì§ˆë¬¸ (ì‹œê°„ ë¬´ê´€)
            logger.info(f"   ğŸ“œ ì •ì±… ì§ˆë¬¸ ê°ì§€ (ì‹œê°„ í•„í„° ë¹„í™œì„±í™”)")
            return {
                'type': 'policy',
                'is_policy': True,
                'is_ongoing': False
            }

        else:
            # ì‹œê°„ í‘œí˜„ ì—†ìŒ
            logger.debug(f"   â„¹ï¸  ì‹œê°„ í‘œí˜„ ì—†ìŒ")
            return None

    except Exception as e:
        logger.warning(f"âš ï¸  LLM ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ (ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±): {e}")
        return None


def best_docs(user_question):
      # ì‚¬ìš©ì ì§ˆë¬¸
      noun_time=time.time()
      query_noun=transformed_query(user_question)
      query_noun_time=time.time()-noun_time
      print(f"ëª…ì‚¬í™” ë³€í™˜ ì‹œê°„ : {query_noun_time}")
      titles_from_pinecone, texts_from_pinecone, urls_from_pinecone, dates_from_pinecone = storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates
      if not query_noun:
        return None,None
      #######  ìµœê·¼ ê³µì§€ì‚¬í•­, ì±„ìš©, ì„¸ë¯¸ë‚˜, í–‰ì‚¬, íŠ¹ê°•ì˜ ë‹¨ìˆœí•œ ì •ë³´ë¥¼ ìš”êµ¬í•˜ëŠ” ê²½ìš°ë¥¼ í•„í„°ë§ í•˜ê¸° ìœ„í•œ ë§¤ì»¤ë‹ˆì¦˜ ########
      remove_noticement = ['ëª©ë¡','ë¦¬ìŠ¤íŠ¸','ë‚´ìš©','ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì—…ë°ì´íŠ¸',
                           'ì»´í“¨í„°í•™ë¶€','ì»´í•™','ìƒìœ„','ì •ë³´','ê´€ë ¨','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ ','ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬']
      query_nouns = [noun for noun in query_noun if noun not in remove_noticement]
      return_docs=[]
      key=None
      numbers=5 ## ê¸°ë³¸ìœ¼ë¡œ 5ê°œ ë¬¸ì„œ ë°˜í™˜í•  ê²ƒ.
      check_num=0
      recent_time=time.time()
      for noun in query_nouns:
        if 'ê°œ' in noun:
            # ìˆ«ì ì¶”ì¶œ
            num = re.findall(r'\d+', noun)
            if num:
                numbers=int(num[0])
                check_num=1
      if (any(keyword in query_noun for keyword in ['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ '])and any(keyword in query_noun for keyword in ['ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬'])and len(query_nouns)<1 or check_num==1):    
        if numbers ==0:
          #### 0ê°œì˜ keywordì— ëŒ€í•´ì„œ ì§ˆë¬¸í•œë‹¤ë©´? ex) ê°€ì¥ ìµœê·¼ ê³µì§€ì‚¬í•­ 0ê°œ ì•Œë ¤ì¤˜######
          keys=['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©']
          return None,[keyword for keyword in keys if keyword in user_question]
        if 'ê³µì§€ì‚¬í•­' in query_noun:
          key=['ê³µì§€ì‚¬í•­']
          notice_url = NOTICE_BASE_URL + "&wr_id="
          return_docs=find_url(notice_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        if 'ì±„ìš©' in query_noun:
          key=['ì±„ìš©']
          company_url = COMPANY_BASE_URL + "&wr_id="
          return_docs=find_url(company_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        other_key = ['ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°']
        if any(keyword in query_noun for keyword in other_key):
          seminar_url = SEMINAR_BASE_URL + "&wr_id="
          key = [keyword for keyword in other_key if keyword in user_question]
          return_docs=find_url(seminar_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        recent_finish_time=time.time()-recent_time
        print(f"ìµœê·¼ ê³µì§€ì‚¬í•­ ë¬¸ì„œ ë½‘ëŠ” ì‹œê°„ {recent_finish_time}")
        if (len(return_docs)>0):
          return return_docs,key


      remove_noticement = ['ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì»´í•™','ìƒìœ„','ê´€ë ¨']

      # BM25 ê²€ìƒ‰ (ë¦¬íŒ©í† ë§ë¨ - BM25Retriever ì‚¬ìš©)
      bm_title_time = time.time()
      Bm25_best_docs, adjusted_similarities = storage.bm25_retriever.search(
          query_nouns=query_noun,
          top_k=50,  # âœ¨ 25â†’50 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ìœ„í•œ í›„ë³´êµ° í™•ëŒ€
          normalize_factor=24.0
      )
      bm_title_f_time = time.time() - bm_title_time
      print(f"bm25 ë¬¸ì„œ ë½‘ëŠ”ì‹œê°„: {bm_title_f_time}")
      ####################################################################################################
      # Dense Retrieval (ë¦¬íŒ©í† ë§ë¨ - DenseRetriever ì‚¬ìš©)
      dense_time = time.time()
      combine_dense_docs = storage.dense_retriever.search(
          user_question=user_question,
          query_nouns=query_noun,
          top_k=50  # âœ¨ 30â†’50 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ìœ„í•œ í›„ë³´êµ° í™•ëŒ€
      )
      pinecone_time = time.time() - dense_time
      print(f"íŒŒì¸ì½˜ì—ì„œ top k ë½‘ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ {pinecone_time}")

      # ## ê²°ê³¼ ì¶œë ¥
      # print("\ní†µí•©ëœ íŒŒì¸ì½˜ë¬¸ì„œ ìœ ì‚¬ë„:")
      # for score, doc in combine_dense_docs:
      #     title, date, text, url = doc
      #     print(f"ì œëª©: {title}\nìœ ì‚¬ë„: {score} {url}")
      #     print('---------------------------------')


      #################################################3#################################################3
      #####################################################################################################3

      # BM25ì™€ Dense Retrieval ê²°ê³¼ ê²°í•© (ë¦¬íŒ©í† ë§ë¨ - DocumentCombiner ì‚¬ìš©)
      combine_time = time.time()
      final_best_docs = storage.document_combiner.combine(
          dense_results=combine_dense_docs,
          bm25_results=Bm25_best_docs,
          bm25_similarities=adjusted_similarities,
          titles_from_pinecone=titles_from_pinecone,
          query_nouns=query_noun,
          user_question=user_question,
          top_k=30  # âœ¨ 20â†’30 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ì „ í›„ë³´êµ° í™•ëŒ€
      )
      combine_f_time = time.time() - combine_time
      print(f"Bm25ë‘ pinecone ê²°í•© ì‹œê°„: {combine_f_time}")

      # âœ… ë‚ ì§œ ë¶€ìŠ¤íŒ… (Recency Boost) - ì‹œê°„ í‘œí˜„ ì—†ì–´ë„ ìµœì‹  ë¬¸ì„œ ìš°ì„ !
      # ì‚¬ìš©ì ì§€ì : "ì‹œê°„ ë§¥ë½ ì—†ìœ¼ë©´ ë‹¹ì—°íˆ ìµœì‹ ìˆœìœ¼ë¡œ"
      from datetime import datetime

      def calculate_recency_boost(doc_date_str):
          """ë¬¸ì„œ ë‚ ì§œì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê³„ì‚° (ìµœì‹  ë¬¸ì„œ ìš°ì„ )"""
          try:
              current_date = datetime.now()
              doc_date = datetime.fromisoformat(doc_date_str.replace('+09:00', ''))

              # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
              days_old = (current_date - doc_date).days

              # ê°€ì¤‘ì¹˜ ê³„ì‚°
              if days_old < 0:  # ë¯¸ë˜ ë‚ ì§œ (ì˜¤ë¥˜)
                  return 1.0
              elif days_old <= 180:  # 6ê°œì›” ì´ë‚´ (ì´ë²ˆí•™ê¸°/ì €ë²ˆí•™ê¸°)
                  return 1.5  # 50% ë¶€ìŠ¤íŒ…
              elif days_old <= 365:  # 1ë…„ ì´ë‚´ (ì‘ë…„)
                  return 1.3  # 30% ë¶€ìŠ¤íŒ…
              elif days_old <= 730:  # 2ë…„ ì´ë‚´
                  return 1.1  # 10% ë¶€ìŠ¤íŒ…
              else:  # 2ë…„ ì´ìƒ
                  return 0.9  # 10% íŒ¨ë„í‹°

          except Exception as e:
              logger.debug(f"ë‚ ì§œ ë¶€ìŠ¤íŒ… ê³„ì‚° ì‹¤íŒ¨: {doc_date_str} - {e}")
              return 1.0  # ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½

      # ê²°í•©ëœ ê²°ê³¼ì— ë‚ ì§œ ë¶€ìŠ¤íŒ… ì ìš©
      boosted_docs = []
      for score, title, date, text, url in final_best_docs:
          boost = calculate_recency_boost(date)
          boosted_score = score * boost
          boosted_docs.append((boosted_score, title, date, text, url))

      # ë¶€ìŠ¤íŒ…ëœ ì ìˆ˜ë¡œ ì¬ì •ë ¬
      boosted_docs.sort(key=lambda x: x[0], reverse=True)
      final_best_docs = boosted_docs

      logger.info(f"ğŸš€ ë‚ ì§œ ë¶€ìŠ¤íŒ… ì™„ë£Œ (ìµœì‹  ë¬¸ì„œ ìš°ì„ : 6ê°œì›” ì´ë‚´ +50%, 1ë…„ ì´ë‚´ +30%)")

      # âœ¨ URL ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê°™ì€ ê²Œì‹œê¸€ì˜ ì„œë¡œ ë‹¤ë¥¸ ì²­í¬ ì œê±°)
      # ëª©ì : ê²€ìƒ‰ ê²°ê³¼ ë‹¤ì–‘ì„± í™•ë³´ (Top Nì´ ëª¨ë‘ ì„œë¡œ ë‹¤ë¥¸ ê²Œì‹œê¸€ì´ ë˜ë„ë¡)
      # ì „ëµ: ê°™ì€ URL(ê²Œì‹œê¸€)ì—ì„œ ìµœê³  ì ìˆ˜ ì²­í¬ë§Œ ì„ íƒ
      # íš¨ê³¼:
      #   - BGE-Reranker íš¨ìœ¨ì„± í–¥ìƒ (ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ì¬ì •ë ¬)
      #   - ë¡œê·¸ ê°€ë…ì„± í–¥ìƒ (ë‹¤ì–‘ì„± ì§€í‘œ ê°œì„ )
      #   - í–¥í›„ í™•ì¥ ëŒ€ë¹„ (ë³µìˆ˜ ë‹µë³€, ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ ë“±)
      dedup_time = time.time()

      seen_urls = {}  # {url: (score, title, date, text, url)}
      deduplicated_docs = []
      duplicate_count = 0
      original_count = len(final_best_docs)

      for score, title, date, text, url in final_best_docs:
          if url in seen_urls:
              # ê°™ì€ URLì´ ì´ë¯¸ ìˆìŒ â†’ ì ìˆ˜ ë¹„êµ
              existing_score = seen_urls[url][0]

              if score > existing_score:
                  # ë” ë†’ì€ ì ìˆ˜ë©´ ê¸°ì¡´ ë¬¸ì„œ ì œê±°í•˜ê³  ìƒˆ ë¬¸ì„œ ì¶”ê°€
                  deduplicated_docs.remove(seen_urls[url])
                  deduplicated_docs.append((score, title, date, text, url))
                  seen_urls[url] = (score, title, date, text, url)
                  logger.debug(f"ğŸ”„ URL ì¤‘ë³µ - ë” ë†’ì€ ì ìˆ˜ë¡œ êµì²´: {title[:30]}... ({existing_score:.2f} â†’ {score:.2f})")
              else:
                  # ë‚®ì€ ì ìˆ˜ë©´ ë¬´ì‹œ
                  duplicate_count += 1
                  logger.debug(f"â­ï¸  URL ì¤‘ë³µ ì œê±°: {title[:30]}... (ì ìˆ˜: {score:.2f} < {existing_score:.2f})")
          else:
              # ìƒˆ URLì´ë©´ ì¶”ê°€
              seen_urls[url] = (score, title, date, text, url)
              deduplicated_docs.append((score, title, date, text, url))

      # ì ìˆ˜ìˆœ ì¬ì •ë ¬ í›„ Top 20
      deduplicated_docs.sort(key=lambda x: x[0], reverse=True)
      final_best_docs = deduplicated_docs[:20]

      dedup_f_time = time.time() - dedup_time
      unique_urls = len(seen_urls)
      print(f"URL ì¤‘ë³µ ì œê±°: {dedup_f_time:.4f}ì´ˆ (ì›ë³¸: {original_count}ê°œ â†’ ì¤‘ë³µ {duplicate_count}ê°œ ì œê±° â†’ ìµœì¢…: {len(final_best_docs)}ê°œ ì„œë¡œ ë‹¤ë¥¸ ê²Œì‹œê¸€, ê³ ìœ  URL {unique_urls}ê°œ)")

      # í´ëŸ¬ìŠ¤í„°ë§ ì œê±°: URL ì¤‘ë³µ ì œê±°ë§Œìœ¼ë¡œ ì¶©ë¶„ (ê° ê²Œì‹œê¸€ë‹¹ ëŒ€í‘œ ì²­í¬ 1ê°œ ì„ íƒ ì™„ë£Œ)
      # get_ai_message()ì—ì„œ ìµœì¢… ì„ íƒëœ ë¬¸ì„œì˜ ì „ì²´ ì²­í¬ë¥¼ ë‹¤ì‹œ ìˆ˜ì§‘í•˜ë¯€ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆí•„ìš”
      return final_best_docs, query_noun

# QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ (ì „ì—­ ë³€ìˆ˜)
_qa_prompt_template = None

def get_qa_prompt_template():
    """QA í”„ë¡¬í”„íŠ¸ PromptTemplate ê°ì²´ ë°˜í™˜ (Lazy loading)"""
    global _qa_prompt_template
    if _qa_prompt_template is None:
        prompt_text = get_qa_prompt()
        _qa_prompt_template = PromptTemplate(
            template=prompt_text,
            input_variables=["current_time", "temporal_intent", "context", "question"]
        )
    return _qa_prompt_template

# PromptTemplate ê°ì²´ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
PROMPT = get_qa_prompt_template()


def get_answer_from_chain(best_docs, user_question, query_noun, temporal_filter=None):

    # âœ… HTML(Markdown) ì¤‘ë³µ ì œê±° - ë¹„ì‹¼ Upstage API ê²°ê³¼ ìµœëŒ€ í™œìš©!
    # ê°™ì€ ì´ë¯¸ì§€ì˜ ì—¬ëŸ¬ ì²­í¬ê°€ ëª¨ë‘ ê°™ì€ Markdownì„ ê°€ì§€ë¯€ë¡œ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©
    seen_htmls = set()
    deduplicated_docs = []
    duplicate_html_count = 0

    # ë””ë²„ê¹…: ì¤‘ë³µ ì œê±° ì „ ë¬¸ì„œ ëª©ë¡
    logger.info(f"   ğŸ“¦ ì¤‘ë³µ ì œê±° ì „: {len(best_docs)}ê°œ ì²­í¬")
    for i, doc in enumerate(best_docs[:10]):  # ì²˜ìŒ 10ê°œë§Œ
        source = doc[7] if len(doc) > 7 else "unknown"
        html_len = len(doc[5]) if len(doc) > 5 and doc[5] else 0
        text_len = len(doc[3])
        logger.info(f"      [{i+1}] {source}: text={text_len}ì, html={html_len}ì")

    for doc in best_docs:
        html = doc[5] if len(doc) > 5 else ""

        # HTMLì´ ìˆê³  ì´ë¯¸ ë³¸ ì  ìˆìœ¼ë©´ ìŠ¤í‚µ (ì¤‘ë³µ Markdown ì œê±°)
        if html and html in seen_htmls:
            duplicate_html_count += 1
            continue

        # ìƒˆë¡œìš´ HTMLì´ê±°ë‚˜ HTMLì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if html:
            seen_htmls.add(html)
        deduplicated_docs.append(doc)

    logger.info(f"   ğŸ”„ ì¤‘ë³µ ì œê±° í›„: {len(deduplicated_docs)}ê°œ ì²­í¬ ({duplicate_html_count}ê°œ Markdown ì¤‘ë³µ ì œê±°)")
    if duplicate_html_count > 0:
        logger.info(f"      ğŸ’¡ ê³ ìœ  Markdown: {len(seen_htmls)}ê°œ (Upstage API ê²°ê³¼ íš¨ìœ¨ì  í™œìš©)")

    # âœ… best_docsì—ì„œ ë©”íƒ€ë°ì´í„° ì§ì ‘ ì¶”ì¶œ (URLë¡œ ë‹¤ì‹œ ì°¾ì§€ ì•ŠìŒ)
    documents = []
    markdown_used = 0
    html_converted = 0
    text_fallback = 0

    for doc in deduplicated_docs:
        score = doc[0]
        title = doc[1]
        date = doc[2]
        text = doc[3]
        url = doc[4]
        # âœ… ë©”íƒ€ë°ì´í„°ë¥¼ tupleì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´ (ë²„ê·¸ ìˆ˜ì •!)
        html = doc[5] if len(doc) > 5 else ""
        content_type = doc[6] if len(doc) > 6 else "text"
        source = doc[7] if len(doc) > 7 else "original_post"
        attachment_type = doc[8] if len(doc) > 8 else ""

        # HTML/Markdown ìš°ì„  ì‚¬ìš© (í‘œ êµ¬ì¡° ë³´ì¡´), ì—†ìœ¼ë©´ text ì‚¬ìš©
        if html:
            from utils.html_parser import is_markdown, html_to_markdown_with_text

            # Markdown í˜•ì‹ ê°ì§€ (Upstage API ì œê³µ, ê³ í’ˆì§ˆ í‘œ êµ¬ì¡°)
            # ì´ë¯¸ Markdownì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í† í° íš¨ìœ¨ì , LLM ìµœì í™”)
            if is_markdown(html):
                # â‘  Markdown í‘œ í˜•ì‹ (Upstage API ê²°ê³¼)
                page_content = html
                markdown_used += 1
            else:
                # â‘¡ HTML â†’ Markdown ë³€í™˜ (fallback)
                page_content = html_to_markdown_with_text(html)

                # ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì›ë³¸ text ì‚¬ìš©
                if not page_content:
                    page_content = text
                    text_fallback += 1
                else:
                    html_converted += 1
        else:
            # â‘¢ html ì—†ìŒ â†’ text ì‚¬ìš©
            page_content = text
            text_fallback += 1

        # ë‚ ì§œ íŒŒì‹± (ISO 8601ê³¼ ë ˆê±°ì‹œ í˜•ì‹ ëª¨ë‘ ì§€ì›)
        try:
            if date.startswith("ì‘ì„±ì¼"):
                doc_date = datetime.strptime(date, 'ì‘ì„±ì¼%y-%m-%d %H:%M')
            else:
                doc_date = datetime.fromisoformat(date)
        except:
            doc_date = datetime.now()

        # Document ê°ì²´ ìƒì„± (ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° í¬í•¨)
        doc = Document(
            page_content=page_content,  # HTML ìš°ì„ , ì—†ìœ¼ë©´ text
            metadata={
                "title": title,
                "url": url,
                "doc_date": doc_date,
                "content_type": content_type,
                "source": source,
                "attachment_type": attachment_type,
                "plain_text": text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ë³´ê´€
            }
        )
        documents.append(doc)

    # í´ë°± í†µê³„ ë¡œê·¸
    logger.info(f"   ğŸ“Š ì½˜í…ì¸  ì†ŒìŠ¤ í†µê³„:")
    logger.info(f"      â‘  Markdown (Upstage API): {markdown_used}ê°œ")
    logger.info(f"      â‘¡ HTML â†’ Markdown ë³€í™˜: {html_converted}ê°œ")
    logger.info(f"      â‘¢ Text í´ë°±: {text_fallback}ê°œ")
    logger.info(f"      ì´ {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±")

    # âœ… ê°œì„ ëœ í•„í„°ë§: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ vs í‚¤ì›Œë“œ í•„í„°ë§
    # í•µì‹¬ ê°œì„ : ê°™ì€ ê²Œì‹œê¸€ì—ì„œ ìˆ˜ì§‘ëœ ì²­í¬ë“¤ì€ ì´ë¯¸ BM25 + Dense + Rerankerë¡œ ê²€ì¦ë¨
    # â†’ í‚¤ì›Œë“œ í•„í„°ë§ìœ¼ë¡œ ì¤‘ìš” ì •ë³´(ì´ë¦„, í•™ë²ˆ ë“±)ë¥¼ ë‹´ì€ ì²­í¬ê°€ ì œê±°ë˜ëŠ” ë¬¸ì œ í•´ê²°

    # ëª¨ë“  ë¬¸ì„œê°€ ê°™ì€ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸ (ì œëª© ê¸°ì¤€)
    unique_titles = set(doc.metadata.get('title', '') for doc in documents)

    if len(unique_titles) == 1:
        # âœ… ê°™ì€ ê²Œì‹œê¸€ì˜ ì²­í¬ë“¤ â†’ ëª¨ë‘ í¬í•¨ (í‚¤ì›Œë“œ í•„í„°ë§ ìŠ¤í‚µ)
        # ì´ìœ : ì´ë¯¸ ë©€í‹°ìŠ¤í…Œì´ì§€ ê²€ìƒ‰(BM25 + Dense + Reranker)ìœ¼ë¡œ ìµœì  ê²Œì‹œê¸€ ì„ ì • ì™„ë£Œ
        # í•´ë‹¹ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì •ë³´(ë³¸ë¬¸, ì´ë¯¸ì§€ OCR, ì²¨ë¶€íŒŒì¼)ë¥¼ LLMì— ì „ë‹¬í•´ì•¼ ì™„ì „í•œ ë‹µë³€ ê°€ëŠ¥
        logger.info(f"   âœ… ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ ê°ì§€ â†’ í‚¤ì›Œë“œ í•„í„°ë§ ìŠ¤í‚µ ({len(documents)}ê°œ ëª¨ë‘ í¬í•¨)")
        relevant_docs = documents
    else:
        # âŒ ì—¬ëŸ¬ ê²Œì‹œê¸€ í˜¼ì¬ â†’ í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
        logger.info(f"   ğŸ” ì—¬ëŸ¬ ê²Œì‹œê¸€ í˜¼ì¬ ({len(unique_titles)}ê°œ) â†’ í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©")
        relevant_docs = [
            doc for doc in documents if
            any(keyword in doc.page_content for keyword in query_noun) or  # í‚¤ì›Œë“œ ë§¤ì¹­
            doc.metadata.get('source') in ['image_ocr', 'document_parse']  # ë©€í‹°ëª¨ë‹¬ í•­ìƒ í¬í•¨
        ]

    if not relevant_docs:
      return None, None, None

    # ğŸ” ë””ë²„ê¹…: ê° ì²­í¬ì˜ ë‚´ìš© ê¸¸ì´ í™•ì¸ (ë°ì´í„° ëˆ„ë½ ê²€ì¦)
    logger.info(f"   ğŸ“‹ LLMì— ì „ë‹¬ë  ì²­í¬ ìƒì„¸:")
    for i, doc in enumerate(relevant_docs):
        source = doc.metadata.get('source', 'unknown')
        content_len = len(doc.page_content)
        logger.info(f"      ì²­í¬{i+1}: [{source}] {content_len}ì")

    # LLM ì´ˆê¸°í™” (ëª…ë‹¨ ì§ˆë¬¸ì„ ìœ„í•œ ì¶©ë¶„í•œ max_tokens ì„¤ì •)
    llm = ChatUpstage(
        api_key=storage.upstage_api_key,
        max_tokens=4096  # ê¸´ ëª…ë‹¨ë„ ì™„ì „íˆ ë‚˜ì—´í•  ìˆ˜ ìˆë„ë¡ ì¶©ë¶„í•œ í† í° í™•ë³´
    )
    relevant_docs_content=format_docs(relevant_docs)

    # ğŸ” ë””ë²„ê¹…: ì „ì²´ context í¬ê¸° ë° ë‚´ìš© í™•ì¸
    logger.info(f"   ğŸ“Š ì „ì²´ Context í¬ê¸°: {len(relevant_docs_content)}ì")
    logger.info(f"   ğŸ“„ ì‹¤ì œ ì „ë‹¬ë˜ëŠ” Context ìš”ì•½ (ê° ì²­í¬ë‹¹ ì• 100ì + ë’¤ 100ì):")
    logger.info(f"{'='*80}")

    # ê° ì²­í¬ë¥¼ "\n\në¬¸ì„œ ì œëª©:"ìœ¼ë¡œ ë¶„ë¦¬
    chunks = relevant_docs_content.split('\n\në¬¸ì„œ ì œëª©:')
    for i, chunk in enumerate(chunks):
        if i > 0:  # ì²« ë²ˆì§¸ëŠ” ë¹ˆ ë¬¸ìì—´ì´ë¯€ë¡œ ìŠ¤í‚µ
            chunk = 'ë¬¸ì„œ ì œëª©:' + chunk  # ë¶„ë¦¬ ì‹œ ì œê±°ëœ ë¶€ë¶„ ë³µì›

        chunk_len = len(chunk)

        if chunk_len <= 200:
            # 200ì ì´í•˜ë©´ ì „ì²´ ì¶œë ¥
            logger.info(chunk)
        else:
            # ì• 100ì + ... + ë’¤ 100ì
            preview = chunk[:100] + f'... ({chunk_len - 200}ì ìƒëµ) ...' + chunk[-100:]
            logger.info(preview)

        if i < len(chunks) - 1:
            logger.info('')  # ì²­í¬ êµ¬ë¶„ìš© ë¹ˆ ì¤„

    logger.info(f"{'='*80}")

    qa_chain = (
        {
            "current_time": lambda _: get_korean_time().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„"),
            "temporal_intent": lambda _: format_temporal_intent(temporal_filter),
            "context": RunnableLambda(lambda _: relevant_docs_content),
            "question": RunnablePassthrough()
        }
        | PROMPT
        | llm
        | StrOutputParser()
    )

    return qa_chain, relevant_docs, relevant_docs_content



#######################################################################

##### ìœ ì‚¬ë„ ì œëª© ë‚ ì§œ ë³¸ë¬¸  url image_urlìˆœìœ¼ë¡œ ì €ì¥ë¨

def get_ai_message(question):
    s_time=time.time()

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹… (ê°€ì¥ ë¨¼ì €!)
    logger.info(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {question}")

    # âœ… ì‹œê°„ ì˜ë„ íŒŒì‹± (LLM ë‹µë³€ ì‹œ í™œìš©)
    from datetime import datetime
    temporal_filter = parse_temporal_intent(question, datetime.now())

    best_time=time.time()
    top_doc, query_noun = best_docs(question)  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    best_f_time=time.time()-best_time
    print(f"best_docs ë½‘ëŠ” ì‹œê°„:{best_f_time}")
    logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {query_noun}")

    # query_nounì´ ì—†ê±°ë‚˜ top_docì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not query_noun or not top_doc or len(top_doc) == 0:
        notice_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }
        return not_in_notices_response
    if len(query_noun)==1 and any(keyword in query_noun for keyword in ['ì±„ìš©','ê³µì§€ì‚¬í•­','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','ê°•ì—°','íŠ¹ê°•']):
      seen_urls = set()  # ì´ë¯¸ ë³¸ URLì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©
      response = f"'{query_noun[0]}'ì— ëŒ€í•œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
      show_url=""
      if top_doc !=None:
        for title, date, _, url in top_doc:  # top_docì—ì„œ ì œëª©, ë‚ ì§œ, URL ì¶”ì¶œ
            if url not in seen_urls:
                response += f"ì œëª©: {title}, ë‚ ì§œ: {date} \n----------------------------------------------------\n"
                seen_urls.add(url)  # URL ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
      if 'ì±„ìš©' in query_noun:
        show_url = COMPANY_BASE_URL + "&wr_id="
      elif 'ê³µì§€ì‚¬í•­' in query_noun:
        show_url = NOTICE_BASE_URL + "&wr_id="
      else:
        show_url = SEMINAR_BASE_URL + "&wr_id="

      # ìµœì¢… data êµ¬ì¡° ìƒì„±
      data = {
        "answer": response,
        "answerable": True,  # ëª©ë¡ ì œê³µ ì„±ê³µ
        "references": show_url,  # show_urlì„ ë„˜ê¸°ê¸°
        "disclaimer": "\n\ní•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "images": ["No content"]
      }
      f_time=time.time()-s_time
      print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
      return data
    top_docs = [list(doc) for doc in top_doc]

    # âœ… Reranking ì „ Top 5 ë¡œê¹…
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Reranking ì „ ê²€ìƒ‰ ê²°ê³¼ Top {min(5, len(top_docs))}:")
    for i, doc in enumerate(top_docs[:5]):
        score, title, date, text, url = doc[:5]
        logger.info(f"   {i+1}ìœ„: [{score:.4f}] {title[:50]}... ({date})")
    logger.info("=" * 60)

    # âœ… BGE-Rerankerë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ê´€ë ¨ì„± ê¸°ì¤€)
    reranking_used = False  # Reranking ì‚¬ìš© ì—¬ë¶€ ì¶”ì 
    if storage.reranker and len(top_docs) > 1:
        logger.info("ğŸ¯ BGE-Reranker í™œì„±í™”!")
        rerank_time = time.time()
        logger.info(f"   ì…ë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ â†’ Reranking ì‹œì‘...")

        # RerankerëŠ” tuple ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ë³€í™˜
        top_docs_tuples = [tuple(doc) for doc in top_docs]

        # Reranking (ì–´ì°¨í”¼ 1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ Top 5ë¡œ ì••ì¶•)
        reranked_docs_tuples = storage.reranker.rerank(
            query=question,
            documents=top_docs_tuples,
            top_k=5  # ìµœëŒ€ 5ê°œë¡œ ì••ì¶• (1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ íš¨ìœ¨í™”)
        )

        # ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        top_docs = [list(doc) for doc in reranked_docs_tuples]
        reranking_used = True  # Reranking ì‚¬ìš©ë¨

        rerank_f_time = time.time() - rerank_time
        logger.info(f"   ì¶œë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ (ì²˜ë¦¬ ì‹œê°„: {rerank_f_time:.2f}ì´ˆ)")
        print(f"âœ… Reranking ì™„ë£Œ: {rerank_f_time:.2f}ì´ˆ")
    elif not storage.reranker:
        logger.info("â­ï¸  BGE-Reranker ë¹„í™œì„±í™” (ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨)")
        logger.info("   â†’ ì›ë³¸ ê²€ìƒ‰ ìˆœì„œ ìœ ì§€")
    elif len(top_docs) <= 1:
        logger.info("â­ï¸  BGE-Reranker ìŠ¤í‚µ (ë¬¸ì„œ 1ê°œ ì´í•˜)")
        logger.info("   â†’ Reranking ë¶ˆí•„ìš”")

    # âœ… Reranking í›„ Top 5 ë¡œê¹…
    logger.info("=" * 60)
    logger.info(f"ğŸ” Reranking í›„ ìµœì¢… ê²°ê³¼ Top {min(5, len(top_docs))}:")
    seen_urls = set()
    unique_url_count = 0
    for i, doc in enumerate(top_docs[:5]):
        score, title, date, text, url = doc[:5]

        # URL ì¤‘ë³µ ì²´í¬
        if url not in seen_urls:
            seen_urls.add(url)
            unique_url_count += 1
            url_marker = "ğŸ†•"  # ìƒˆë¡œìš´ URL
        else:
            url_marker = "ğŸ”"  # ì¤‘ë³µ URL (ê°™ì€ ë¬¸ì„œì˜ ë‹¤ë¥¸ ì²­í¬)

        logger.info(f"   {i+1}ìœ„: [{score:.4f}] {url_marker} {title[:50]}... ({date})")
        logger.info(f"      URL: {url}")

    logger.info(f"   ğŸ’¡ ë‹¤ì–‘ì„±: Top 5 ì¤‘ {unique_url_count}ê°œ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ")
    logger.info("=" * 60)

    final_score = top_docs[0][0]
    final_title = top_docs[0][1]
    final_date = top_docs[0][2]
    final_text = top_docs[0][3]
    final_url = top_docs[0][4]
    final_image = []

    # ìµœì¢… ì„ íƒëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹…
    logger.info(f"ğŸ“„ ìµœì¢… ì„ íƒ ë¬¸ì„œ:")
    logger.info(f"   ì œëª©: {final_title}")
    logger.info(f"   ë‚ ì§œ: {final_date}")
    logger.info(f"   ìœ ì‚¬ë„: {final_score:.4f}")
    logger.info(f"   URL: {final_url}")
    logger.info(f"   ë³¸ë¬¸ ê¸¸ì´: {len(final_text)}ì")
    if len(final_text) > 0:
        logger.info(f"   ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {final_text[:100]}...")

    # MongoDB ì—°ê²° í™•ì¸ í›„ ì´ë¯¸ì§€ URL ì¡°íšŒ
    if storage.mongo_collection is not None:
        record = storage.mongo_collection.find_one({"title" : final_title})
        if record :
            if(isinstance(record["image_url"], list)):
              final_image.extend(record["image_url"])
            else :
              final_image.append(record["image_url"])
            logger.info(f"   ì´ë¯¸ì§€: {len(final_image)}ê°œ")

            # HTML êµ¬ì¡° ì •ë³´ ë¡œê¹…
            if record.get("html"):
                html_length = len(record["html"])
                logger.info(f"   HTML êµ¬ì¡°: âœ… ìˆìŒ ({html_length}ì)")
            else:
                logger.info(f"   HTML êµ¬ì¡°: âŒ ì—†ìŒ")

            # ì½˜í…ì¸  íƒ€ì… ë¡œê¹…
            content_type = record.get("content_type", "unknown")
            source = record.get("source", "unknown")
            logger.info(f"   ì½˜í…ì¸  íƒ€ì…: {content_type}")
            logger.info(f"   ì†ŒìŠ¤: {source}")
        else :
            print("ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì¡´ì¬ X")
            logger.warning(f"âš ï¸  MongoDBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {final_title}")
            final_score = 0
            final_title = "No content"
            final_date = "No content"
            final_text = "No content"
            final_url = "No URL"
            final_image = ["No content"]
    else:
        logger.warning("âš ï¸  MongoDB ì—°ê²° ì—†ìŒ - ì´ë¯¸ì§€ URL ì¡°íšŒ ë¶ˆê°€")
        final_image = ["No content"]

    # top_docs ì¸ë±ìŠ¤ êµ¬ì„±
    # 0: ìœ ì‚¬ë„, 1: ì œëª©, 2: ë‚ ì§œ, 3: ë³¸ë¬¸ë‚´ìš©, 4: url, 5: ì´ë¯¸ì§€url

    # Reranker ì ìˆ˜ëŠ” ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ final_score < 0ì´ë©´ ìœ ì‚¬ë„ ì²´í¬ ìŠ¤í‚µ
    if final_image[0] != "No content" and final_text == "No content" and (final_score < 0 or final_score > MINIMUM_SIMILARITY_SCORE):
        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        only_image_response = {
            "answer": None,
            "references": final_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return only_image_response

    # ì´ë¯¸ì§€ + LLM ë‹µë³€ì´ ìˆëŠ” ê²½ìš°.
    else:
        # âœ… í•µì‹¬ ê°œì„ : ê°™ì€ URLì˜ ëª¨ë“  ì²­í¬(ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€)ë¥¼ LLMì— ì „ë‹¬!
        # ë¬¸ì œ: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ëŠ” ë³¸ë¬¸ ì²­í¬ë§Œ í¬í•¨ (ì²¨ë¶€íŒŒì¼ ëˆ„ë½)
        # í•´ê²°: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê°€ì ¸ì˜´
        enrich_time = time.time()

        # Top ë¬¸ì„œì˜ URL ì¶”ì¶œ (ê²Œì‹œê¸€ URL)
        top_url = top_docs[0][4] if top_docs else None

        if top_url:
            # âœ… ë³€ê²½: URL ê¸°ë°˜ ë§¤ì¹­ ëŒ€ì‹  ì œëª© ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš©!
            # ì´ìœ : ì´ë¯¸ì§€ URL(/data/editor/...)ì€ wr_idë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ
            # í•´ê²°: ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ëŠ” ê°™ì€ ì œëª©ì„ ê³µìœ í•˜ë¯€ë¡œ ì œëª©ìœ¼ë¡œ ë§¤ì¹­
            top_title = top_docs[0][1]  # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì œëª©
            wr_id = top_url.split('&wr_id=')[-1] if '&wr_id=' in top_url else top_url.split('wr_id=')[-1] if 'wr_id=' in top_url else None

            logger.info(f"ğŸ” ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ ê²€ìƒ‰: ì œëª©='{top_title}' (wr_id={wr_id})")

            # ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ì°¾ê¸° (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€ OCR)
            enriched_docs = []
            seen_texts = set()  # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°ìš©

            # ë””ë²„ê¹…: ë§¤ì¹­ ìƒí™© ì¶”ì 
            total_checked = 0
            matched_count = 0
            duplicate_count = 0

            for i, url in enumerate(storage.cached_urls):
                # âœ… ê°™ì€ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸ (ì œëª© ê¸°ì¤€ - ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ í¬í•¨!)
                if storage.cached_titles[i] == top_title:
                    total_checked += 1
                    matched_count += 1

                    text = storage.cached_texts[i]
                    content_type = storage.cached_content_types[i] if i < len(storage.cached_content_types) else "unknown"
                    source = storage.cached_sources[i] if i < len(storage.cached_sources) else "unknown"

                    # ë””ë²„ê¹… ë¡œê·¸ (ì²˜ìŒ 5ê°œë§Œ)
                    if matched_count <= 5:
                        html_data = storage.cached_htmls[i] if i < len(storage.cached_htmls) else ""
                        logger.info(f"   [{matched_count}] URL: {url[:80]}...")
                        logger.info(f"       íƒ€ì…: {content_type}, ì†ŒìŠ¤: {source}")
                        logger.info(f"       í…ìŠ¤íŠ¸: {len(text)}ì, HTML: {len(html_data)}ì")
                        logger.info(f"       ì¸ë±ìŠ¤: {i}")

                    # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ì§€ ì•ŠìŒ! (ì¤‘ìš”: "No content"ë„ í¬í•¨)
                    text_key = ''.join(text.split())  # ê³µë°± ì œê±° í›„ ë¹„êµ

                    # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±° (ë¹ˆ ë¬¸ìì—´ì€ ì œì™¸í•˜ì§€ ì•ŠìŒ!)
                    if text_key not in seen_texts:  # âœ… 'text_key and' ì œê±° (ë¹ˆ í…ìŠ¤íŠ¸ë„ í¬í•¨)
                        seen_texts.add(text_key)
                        enriched_docs.append((
                            top_docs[0][0],  # ì ìˆ˜ëŠ” top ë¬¸ì„œì™€ ë™ì¼
                            storage.cached_titles[i],
                            storage.cached_dates[i],
                            text,
                            url,
                            storage.cached_htmls[i] if i < len(storage.cached_htmls) else "",
                            storage.cached_content_types[i] if i < len(storage.cached_content_types) else "unknown",
                            storage.cached_sources[i] if i < len(storage.cached_sources) else "unknown",
                            storage.cached_attachment_types[i] if i < len(storage.cached_attachment_types) else ""
                        ))
                    else:
                        duplicate_count += 1

            logger.info(f"   ğŸ“Š ë§¤ì¹­ í†µê³„: ì „ì²´ {len(storage.cached_urls)}ê°œ ì¤‘ {matched_count}ê°œ ë§¤ì¹­, {duplicate_count}ê°œ ì¤‘ë³µ ì œê±°")

            # ì²­í¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ top_docsë¥¼ êµì²´ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€)
            if enriched_docs:
                logger.info(f"ğŸ”§ ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ìˆ˜ì§‘: {len(top_docs)}ê°œ â†’ {len(enriched_docs)}ê°œ")

                # íƒ€ì…ë³„ ì¹´ìš´íŠ¸ (source ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ì¹´ìš´íŠ¸)
                original_post_count = 0
                image_count = 0
                attachment_count = 0

                for i, (score, title, date, text, url, html, content_type, source, attachment_type) in enumerate(enriched_docs):
                    # âœ… sourceë¥¼ tupleì—ì„œ ì§ì ‘ ì‚¬ìš© (URLë¡œ ì°¾ì§€ ì•ŠìŒ)
                    if source == "original_post":
                        original_post_count += 1
                    elif source == "image_ocr":
                        image_count += 1
                    elif source == "document_parse":
                        attachment_count += 1

                logger.info(f"   ğŸ“¦ ë³¸ë¬¸ ì²­í¬: {original_post_count}ê°œ")
                logger.info(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ OCR ì²­í¬: {image_count}ê°œ")
                logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì²­í¬: {attachment_count}ê°œ")
                top_docs = enriched_docs
            else:
                logger.warning(f"âš ï¸  ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤! wr_id={wr_id}")
                logger.warning(f"   Top URL: {top_url}")

        enrich_f_time = time.time() - enrich_time
        print(f"ì²­í¬ ìˆ˜ì§‘ ì‹œê°„: {enrich_f_time}")

        chain_time=time.time()
        qa_chain, relevant_docs, relevant_docs_content = get_answer_from_chain(top_docs, question, query_noun, temporal_filter)
        chain_f_time=time.time()-chain_time
        print(f"chain ìƒì„±í•˜ëŠ” ì‹œê°„: {chain_f_time}")

        # ğŸ” ë””ë²„ê¹…: get_answer_from_chain ë°˜í™˜ê°’ í™•ì¸
        logger.info(f"ğŸ” get_answer_from_chain ë°˜í™˜ê°’ í™•ì¸:")
        logger.info(f"   qa_chain: {type(qa_chain)} (None? {qa_chain is None})")
        logger.info(f"   relevant_docs: {type(relevant_docs)} (None? {relevant_docs is None}, ê°œìˆ˜: {len(relevant_docs) if relevant_docs else 0})")
        logger.info(f"   relevant_docs_content: {type(relevant_docs_content)} (None? {relevant_docs_content is None})")
        if final_url == PROFESSOR_BASE_URL + "&lang=kor" and any(keyword in query_noun for keyword in ['ì—°ë½ì²˜', 'ì „í™”', 'ë²ˆí˜¸', 'ì „í™”ë²ˆí˜¸']):
            data = {
                "answer": "í•´ë‹¹ êµìˆ˜ë‹˜ì€ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
                "answerable": False,  # ì—°ë½ì²˜ ì •ë³´ ì—†ìŒ
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return data
            
        # prof_title=final_title
        # prof_url=["https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_1"]
        # prof_name=""
        # # ì •ê·œì‹ì„ ì´ìš©í•˜ì—¬ ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ ì¶”ì¶œ
        # if any(final_url.startswith(url) for url in prof_url):
        #     match = re.match(r"^[^\dA-Za-z]+", prof_title)
        #     if match:
        #         prof_name = match.group().strip()  # ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     else:
        #         prof_name = prof_title.strip()  # ìˆ«ìê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     prof_name = re.sub(r"\s+", "", prof_name)
        #     user_question = re.sub(r"\s+", "", question)
        #     if prof_name not in user_question:
        #         refer_url=""
        #         if 'ì§ì›' in query_noun:
        #             refer_url=prof_url[1]
        #         else:
        #             refer_url=prof_url[2]
        #         data = {
        #             "answer": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” êµìˆ˜ë‹˜ ì •ë³´ì…ë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
        #             "references": refer_url,
        #             "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        #             "images": ["No content"]
        #         }
        #         return data

        # ê³µì§€ì‚¬í•­ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        notice_url = NOTICE_BASE_URL
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨
        if not qa_chain or not relevant_docs:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì¡°ê±´ ì§„ì…!")
            logger.warning(f"   ì¡°ê±´: not qa_chain ({not qa_chain}) or not relevant_docs ({not relevant_docs})")
            logger.warning(f"   â†’ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ ì˜ˆì •")
            # Reranker ì ìˆ˜ëŠ” ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ final_score < 0ì´ë©´ ìœ ì‚¬ë„ ì²´í¬ ìŠ¤í‚µ
            if final_image[0] != "No content" and (final_score < 0 or final_score > MINIMUM_SIMILARITY_SCORE):
                data = {
                    "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "answerable": True,  # ì´ë¯¸ì§€ë¡œ ë‹µë³€ ì œê³µ
                    "references": final_url,
                    "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "images": final_image
                }
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return data
            else:
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return not_in_notices_response

        # ìœ ì‚¬ë„ê°€ ë‚®ì€ ê²½ìš° (ë‹¨, Reranker ì ìˆ˜ëŠ” ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²´í¬ ìŠ¤í‚µ)
        # BGE-Reranker ì ìˆ˜ ë²”ìœ„: ì•½ -10 ~ +10 (ìŒìˆ˜ë„ ì •ìƒ)
        # BM25 ì ìˆ˜ ë²”ìœ„: 0 ~ ë¬´í•œëŒ€ (í•­ìƒ ì–‘ìˆ˜)
        if final_score >= 0 and final_score < MINIMUM_SIMILARITY_SCORE:
            logger.warning(f"âš ï¸ ìœ ì‚¬ë„ ì¡°ê±´ ì§„ì…!")
            logger.warning(f"   final_score ({final_score:.4f}) < MINIMUM_SIMILARITY_SCORE ({MINIMUM_SIMILARITY_SCORE})")
            logger.warning(f"   â†’ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return not_in_notices_response
        elif final_score < 0:
            logger.info(f"âœ… Reranker ì ìˆ˜ ê°ì§€ ({final_score:.4f}) â†’ ìœ ì‚¬ë„ ì²´í¬ ìŠ¤í‚µ")

        # LLMì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²½ìš°
        logger.info(f"âœ… ëª¨ë“  ì¡°ê±´ í†µê³¼! LLM ë‹µë³€ ìƒì„± ì‹œì‘...")
        answer_time=time.time()

        # qa_chain.invoke() ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        answer_result = qa_chain.invoke(question)

        answer_f_time=time.time()-answer_time
        print(f"ë‹µë³€ ìƒì„±í•˜ëŠ” ì‹œê°„: {answer_f_time}")

        # âœ… JSON íŒŒì‹± ì‹œë„ (LLMì´ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ í™•ì¸)
        import json
        import re

        llm_answerable = None  # LLMì´ íŒë‹¨í•œ answerable ê°’
        llm_answer_text = None  # LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸

        try:
            # JSON íŒŒì‹± ì‹œë„
            # LLMì´ ê°€ë” ```json...``` ë¡œ ê°ìŒ€ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
            clean_result = answer_result.strip()
            if clean_result.startswith("```json"):
                clean_result = clean_result[7:]
            if clean_result.startswith("```"):
                clean_result = clean_result[3:]
            if clean_result.endswith("```"):
                clean_result = clean_result[:-3]
            clean_result = clean_result.strip()

            parsed = json.loads(clean_result)

            # JSON íŒŒì‹± ì„±ê³µ
            if "answerable" in parsed and "answer" in parsed:
                llm_answerable = parsed["answerable"]
                llm_answer_text = parsed["answer"]
                logger.info(f"âœ… JSON íŒŒì‹± ì„±ê³µ: answerable={llm_answerable}")
                logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
                logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")
            else:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ â†’ í´ë°± ì‚¬ìš©")

        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (LLMì´ í˜•ì‹ ì•ˆ ì§€í‚´) â†’ í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš©")
            logger.debug(f"   ì—ëŸ¬: {e}")
            logger.debug(f"   ì›ë³¸ ì‘ë‹µ: {answer_result[:200]}...")

        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ: ê¸°ì¡´ answer_result ì‚¬ìš©
        if llm_answer_text is None:
            llm_answer_text = answer_result
            logger.info(f"ğŸ’¬ LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ë¹„-JSON í˜•ì‹):")
            logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
            logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")

        logger.info(f"   ì‚¬ìš©ëœ ì°¸ê³ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")

        # ë‹µë³€ ê²€ì¦ ë° ê²½ê³  ì¶”ê°€ (ë²”ìš©)
        completeness_keywords = ['ì „ë¶€', 'ëª¨ë“ ', 'ëª¨ë‘', 'ë¹ ì§ì—†ì´', 'ì „ì²´', 'ë‹¤', 'ëª…ë‹¨', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ëˆ„êµ¬']
        has_completeness_request = any(keyword in question for keyword in completeness_keywords)

        # ì™„ì „ì„± ìš”êµ¬ + Contextì™€ ë‹µë³€ ì°¨ì´ê°€ í¬ë©´ ê²½ê³ 
        if has_completeness_request:
            # Contextì— ìˆëŠ” ìˆ«ì íŒ¨í„´ (í•™ë²ˆ, ë‚ ì§œ ë“±)
            context_numbers = len(re.findall(r'\b20\d{6,8}\b', relevant_docs_content))
            answer_numbers = len(re.findall(r'\b20\d{6,8}\b', llm_answer_text))

            logger.info(f"   ğŸ“Š ì™„ì „ì„± ê²€ì¦: Context {context_numbers}ê±´ / ë‹µë³€ {answer_numbers}ê±´")

            # Contextì˜ 50% ë¯¸ë§Œë§Œ ë‹µë³€ì— í¬í•¨ë˜ë©´ ê²½ê³ 
            if context_numbers >= 10 and answer_numbers < context_numbers * 0.5:
                logger.warning(f"   âš ï¸ ì™„ì „ì„± ìš”êµ¬í–ˆìœ¼ë‚˜ ë‹µë³€ ë¶ˆì™„ì „! LLMì´ ì„ì˜ë¡œ ìš”ì•½í•œ ê²ƒìœ¼ë¡œ íŒë‹¨")
                llm_answer_text += f"\n\nâš ï¸ ì¼ë¶€ ë‚´ìš©ì´ ìƒëµë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸ì„œ: ì•½ {context_numbers}ê±´ / ë‹µë³€: {answer_numbers}ê±´). ì „ì²´ ë‚´ìš©ì€ ì°¸ê³  URLì„ í™•ì¸í•˜ì„¸ìš”."

        doc_references = "\n".join([
            f"\nì°¸ê³  ë¬¸ì„œ URL: {doc.metadata['url']}"
            for doc in relevant_docs[:1] if doc.metadata.get('url') != 'No URL'
        ])

        # âœ… answerable ìµœì¢… íŒë‹¨
        if llm_answerable is not None:
            # JSON íŒŒì‹± ì„±ê³µ â†’ LLMì´ ì§ì ‘ íŒë‹¨í•œ ê°’ ì‚¬ìš©
            answerable = llm_answerable
            logger.info(f"âœ… answerable íŒë‹¨: JSON íŒŒì‹± ê²°ê³¼ ì‚¬ìš© (LLM ì§ì ‘ íŒë‹¨: {answerable})")
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨ â†’ í´ë°±: íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ íŒë‹¨
            answer_start = llm_answer_text[:150]
            if answer_start.startswith("ì œê³µëœ ë¬¸ì„œì—ëŠ”") and any(phrase in answer_start for phrase in ["ì—†ìŠµë‹ˆë‹¤", "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"]):
                answerable = False
            else:
                answerable = True
            logger.info(f"âš ï¸ answerable íŒë‹¨: í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš© (ê²°ê³¼: {answerable})")

        if answerable:
            logger.info("âœ… LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤")
        else:
            logger.info("âŒ LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ˆë¬¸ ì‘ì„± ìš”ì²­ ì•ˆë‚´ í‘œì‹œ)")

        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        data = {
            "answer": llm_answer_text,  # JSON íŒŒì‹±ëœ ë‹µë³€ ë˜ëŠ” ì›ë³¸ ë‹µë³€
            "answerable": answerable,  # ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€
            "references": doc_references,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        logger.info(f"âœ… ì´ ì²˜ë¦¬ ì‹œê°„: {f_time:.2f}ì´ˆ")
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return data