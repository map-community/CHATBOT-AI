import os
import re
import time
import pickle
import logging
from datetime import datetime
from collections import defaultdict
import numpy as np
import pytz
from dotenv import load_dotenv
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
from langchain import hub
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import Runnable, RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableSequence, RunnableMap
from langchain_core.runnables import RunnableLambda
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from pymongo import MongoClient
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Mecab import (logger ì •ì˜ ì´í›„)
try:
    from konlpy.tag import Mecab
    MECAB_AVAILABLE = True
    logger.info("âœ… Mecab ì‚¬ìš© ê°€ëŠ¥ (30-50ë°° ë¹ ë¥¸ í˜•íƒœì†Œ ë¶„ì„)")
except Exception as e:
    logger.warning(f"âš ï¸  Mecabì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    logger.warning("âš ï¸  Mecab ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì •í™•ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    MECAB_AVAILABLE = False
    Mecab = None

# StorageManager import
from modules.storage_manager import get_storage_manager

# StorageManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
storage = get_storage_manager()

# URL ìƒìˆ˜
NOTICE_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
COMPANY_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b"
SEMINAR_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4"
PROFESSOR_BASE_URL = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2"

def get_korean_time():
    return datetime.now(pytz.timezone('Asia/Seoul'))

# ë‹¨ì–´ ëª…ì‚¬í™” í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - QueryTransformer ì‚¬ìš©)
def transformed_query(content):
    """
    ì§ˆë¬¸ì„ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        content: ì‚¬ìš©ì ì§ˆë¬¸ (ì›ë¬¸)

    Returns:
        List[str]: ì¶”ì¶œëœ ëª…ì‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    return storage.query_transformer.transform(content)
###################################################################################################


# Dense Retrieval (Upstage ì„ë² ë”©) - Lazy initializationìœ¼ë¡œ í•¨ìˆ˜ ë‚´ì—ì„œ ìƒì„±í•˜ë„ë¡ ë³€ê²½
# embeddings ê°ì²´ëŠ” í•„ìš”í•  ë•Œ get_embeddings() í•¨ìˆ˜ë¥¼ í†µí•´ ê°€ì ¸ì˜µë‹ˆë‹¤.
def get_embeddings():
    """Upstage Embeddings ê°ì²´ ë°˜í™˜ (Lazy initialization)"""
    return UpstageEmbeddings(
        api_key=storage.upstage_api_key,
        model="solar-embedding-1-large-query"  # ì§ˆë¬¸ ì„ë² ë”©ìš© ëª¨ë¸
    )
# dense_doc_vectors = np.array(embeddings.embed_documents(texts))  # ë¬¸ì„œ ì„ë² ë”©

def fetch_titles_from_pinecone():
    """
    Pineconeì—ì„œ ì „ì²´ ë°ì´í„°(ì œëª©, í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    - list() ë©”ì„œë“œ(Pagination)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œìˆ˜ ì œí•œ ì—†ì´ ëª¨ë“  IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - fetch() ë©”ì„œë“œ(Batch)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - html_available=trueì¸ ê²½ìš° MongoDBì—ì„œ ì‹¤ì œ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    logger.info("ğŸ”„ Pinecone ì „ì²´ ë°ì´í„° ì¡°íšŒ ì‹œì‘...")

    # ==========================================
    # MongoDB ì—°ê²° (HTML ì¡°íšŒìš©)
    # ==========================================
    mongo_collection = None
    mongo_client = None
    try:
        if storage.mongo_collection is not None:
            # StorageManagerì˜ MongoDB connection ì‚¬ìš©
            mongo_collection = storage.mongo_collection.database["multimodal_cache"]
            logger.info("âœ… MongoDB ì—°ê²° ì„±ê³µ (HTML ì¡°íšŒìš©)")
    except Exception as e:
        logger.warning(f"âš ï¸  MongoDB ì—°ê²° ì‹¤íŒ¨ (HTML ì—†ì´ ì§„í–‰): {e}")

    # ==========================================
    # 1. ì „ì²´ ID ê°€ì ¸ì˜¤ê¸° (ê°œìˆ˜ ì œí•œ ì—†ìŒ!)
    # ==========================================
    all_ids = []
    
    try:
        # namespaceê°€ ìˆë‹¤ë©´ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’ "")
        # list()ëŠ” ì „ì²´ IDë¥¼ í˜ì´ì§€ë„¤ì´ì…˜í•˜ì—¬ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
        for ids in storage.pinecone_index.list(namespace=""): 
            all_ids.extend(ids)
        logger.info(f"ğŸ“Š ì´ {len(all_ids)}ê°œì˜ ë²¡í„° IDë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ ID ë¦¬ìŠ¤íŒ… ì‹¤íŒ¨: {e}")
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ë¬¸ì œì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆë‚´
        logger.error("ğŸ‘‰ 'requirements.txt'ì˜ pinecone ë²„ì „ì„ í™•ì¸í•˜ê³  ì¬ë¹Œë“œí•˜ì„¸ìš”.")
        return [], [], [], [], [], [], [], [], [], []

    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
    if not all_ids:
        logger.warning("âš ï¸ ì¡°íšŒëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤.")
        return [], [], [], [], [], [], [], [], [], []


    # ==========================================
    # 2. IDë¡œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Batch Fetch)
    # ==========================================
    # ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    titles = []
    texts = []
    urls = []
    dates = []
    htmls = []
    content_types = []
    sources = []
    image_urls = []
    attachment_urls = []
    attachment_types = []

    # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë°°ì¹˜ í¬ê¸°
    batch_size = 1000

    # ë””ë²„ê¹… ì¹´ìš´í„° ì¶”ê°€
    html_available_count = 0
    mongo_found_count = 0
    html_extracted_count = 0

    # 1,000ê°œì”© ëŠì–´ì„œ ìš”ì²­
    for i in range(0, len(all_ids), batch_size):
        logger.info(f"â³ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({i} / {len(all_ids)})")
        
        batch_ids = all_ids[i:i + batch_size]
        
        try:
            # Fetch ìš”ì²­
            fetch_response = storage.pinecone_index.fetch(ids=batch_ids)
            
            # ì‘ë‹µ ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (v3 í˜¸í™˜ì„± í•´ê²°)
            vectors = {}
            if hasattr(fetch_response, 'to_dict'):
                response_dict = fetch_response.to_dict()
                vectors = response_dict.get('vectors', {})
            elif hasattr(fetch_response, 'vectors'):
                vectors = fetch_response.vectors
            else:
                vectors = fetch_response.get('vectors', {})

            if vectors is None:
                vectors = {}
            
            # ê°€ì ¸ì˜¨ ë°ì´í„° íŒŒì‹±
            for vector_id in batch_ids:
                if vector_id in vectors:
                    vector_data = vectors[vector_id]
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    if isinstance(vector_data, dict):
                        metadata = vector_data.get('metadata', {})
                    elif hasattr(vector_data, 'metadata'):
                        metadata = vector_data.metadata
                    else:
                        metadata = {}

                    if metadata is None:
                        metadata = {}
                    
                    # ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„° ì¶”ê°€
                    titles.append(metadata.get("title", ""))
                    texts.append(metadata.get("text", ""))
                    url = metadata.get("url", "")
                    urls.append(url)
                    dates.append(metadata.get("date", ""))

                    # ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„°: html_availableì´ë©´ MongoDBì—ì„œ HTML ì¡°íšŒ
                    html = ""
                    if metadata.get("html_available"):
                        html_available_count += 1
                        if mongo_collection is not None:
                            try:
                                # html_available=trueì¸ chunkëŠ” ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ì—ì„œ ì¶”ì¶œëœ ê²ƒ
                                # MongoDB cacheëŠ” image_url ë˜ëŠ” attachment_urlì„ keyë¡œ ì‚¬ìš©
                                lookup_url = metadata.get("image_url") or metadata.get("attachment_url")

                                if lookup_url:
                                    # ë””ë²„ê¹…: URL ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
                                    if html_available_count <= 3:
                                        logger.info(f"ğŸ” ì¡°íšŒ ì‹œë„ URL: {lookup_url[:80]}...")

                                    cached = mongo_collection.find_one({"url": lookup_url})
                                    if cached:
                                        mongo_found_count += 1
                                        # ë””ë²„ê¹…: ì°¾ì€ ê²½ìš° ë¡œê¹…
                                        if mongo_found_count <= 3:
                                            logger.info(f"âœ… MongoDBì—ì„œ ë°œê²¬: {lookup_url[:80]}...")
                                            logger.info(f"   í•„ë“œ: {list(cached.keys())}")

                                        # ì´ë¯¸ì§€ OCRì¸ ê²½ìš° ocr_html, ë¬¸ì„œì¸ ê²½ìš° html
                                        html_content = cached.get("ocr_html") or cached.get("html", "")
                                        if html_content:
                                            html = html_content
                                            html_extracted_count += 1
                                    else:
                                        # ë””ë²„ê¹…: ëª» ì°¾ì€ ê²½ìš° ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
                                        if html_available_count <= 3:
                                            logger.warning(f"âŒ MongoDBì—ì„œ ëª» ì°¾ìŒ: {lookup_url[:80]}...")
                                else:
                                    # image_urlê³¼ attachment_urlì´ ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°
                                    if html_available_count <= 3:
                                        logger.warning(f"âš ï¸  html_available=trueì¸ë° image_url/attachment_url ì—†ìŒ (board URL: {url[:80]}...)")
                            except Exception as e:
                                logger.warning(f"MongoDB HTML ì¡°íšŒ ì‹¤íŒ¨: {e}")

                    htmls.append(html)
                    content_types.append(metadata.get("content_type", "text"))
                    sources.append(metadata.get("source", "original_post"))
                    image_urls.append(metadata.get("image_url", ""))
                    attachment_urls.append(metadata.get("attachment_url", ""))
                    attachment_types.append(metadata.get("attachment_type", ""))
                    
        except Exception as e:
            logger.error(f"âš ï¸ ë°°ì¹˜ Fetch ì‹¤íŒ¨ ({i}~{i+batch_size}): {e}")
            continue

    logger.info(f"âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(titles)}ê°œ ë¬¸ì„œ")
    logger.info(f"ğŸ“Š HTML ì¡°íšŒ í†µê³„:")
    logger.info(f"   - html_available=true ë¬¸ì„œ: {html_available_count}ê°œ")
    logger.info(f"   - MongoDBì—ì„œ ì°¾ì€ ë¬¸ì„œ: {mongo_found_count}ê°œ")
    logger.info(f"   - ì‹¤ì œ HTML ì¶”ì¶œ ì„±ê³µ: {html_extracted_count}ê°œ")

    return titles, texts, urls, dates, htmls, content_types, sources, image_urls, attachment_urls, attachment_types


# ìºì‹± ë°ì´í„° ì´ˆê¸°í™” í•¨ìˆ˜

def initialize_cache():
    """
    ìºì‹œ ì´ˆê¸°í™” í•¨ìˆ˜ (Redis Fast Track ì ìš©)
    - Redis ìºì‹œê°€ ìˆìœ¼ë©´ 3ì´ˆ ë¡œë”©
    - ì—†ìœ¼ë©´ Pineconeì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ Redisì— ì €ì¥ (20ë¶„ ì†Œìš”, ìµœì´ˆ 1íšŒë§Œ)
    """
    try:
        logger.info("ğŸ”„ ìºì‹œ ì´ˆê¸°í™” ì‹œì‘...")

        # ==========================================
        # 1. Redis ìºì‹œ í™•ì¸ (Fast Track)
        # ==========================================
        if storage.redis_client is not None:
            try:
                logger.info("ğŸ” Redis ìºì‹œ í™•ì¸ ì¤‘...")
                cached_data = storage.redis_client.get('pinecone_metadata')

                if cached_data:
                    logger.info("ğŸš€ Redis ìºì‹œ ë°œê²¬! ë¹ ë¥¸ ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

                    # Pickleë¡œ ì €ì¥ëœ ë°ì´í„° ë³µì›
                    (storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
                     storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
                     storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types) = pickle.loads(cached_data)

                    logger.info(f"âœ… Redis ë¡œë“œ ì™„ë£Œ! ({len(storage.cached_titles)}ê°œ ë¬¸ì„œ, Pinecone ë‹¤ìš´ë¡œë“œ ìƒëµ)")
                    logger.info(f"   - HTML êµ¬ì¡° ìˆëŠ” ë¬¸ì„œ: {sum(1 for html in storage.cached_htmls if html)}ê°œ")
                    logger.info(f"   - ì´ë¯¸ì§€ OCR ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'image')}ê°œ")
                    logger.info(f"   - ì²¨ë¶€íŒŒì¼ ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'attachment')}ê°œ")

                    # Retriever ì´ˆê¸°í™”ë¡œ ì í”„ (Pinecone Fetch ìƒëµ!)
                    _initialize_retrievers()
                    logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(storage.cached_titles)}, texts: {len(storage.cached_texts)})")
                    return
                else:
                    logger.info("â¬‡ï¸  Redisì— ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

            except Exception as e:
                logger.warning(f"âš ï¸  Redis ë¡œë“œ ì‹¤íŒ¨ (Pineconeì—ì„œ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤): {e}")

        # ==========================================
        # 2. Pineconeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Slow Track)
        # ==========================================
        logger.info("â³ Pinecone ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ìµœì´ˆ 1íšŒ, ì•½ 20ë¶„ ì†Œìš”)...")
        (storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
         storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
         storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types) = fetch_titles_from_pinecone()
        logger.info(f"âœ… Pineconeì—ì„œ {len(storage.cached_titles)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        logger.info(f"   - HTML êµ¬ì¡° ìˆëŠ” ë¬¸ì„œ: {sum(1 for html in storage.cached_htmls if html)}ê°œ")
        logger.info(f"   - ì´ë¯¸ì§€ OCR ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'image')}ê°œ")
        logger.info(f"   - ì²¨ë¶€íŒŒì¼ ë¬¸ì„œ: {sum(1 for ct in storage.cached_content_types if ct == 'attachment')}ê°œ")

        # ==========================================
        # 3. Redisì— ì €ì¥ (ë‹¤ìŒ ì¬ì‹œì‘ì„ ìœ„í•´)
        # ==========================================
        if storage.redis_client is not None:
            try:
                cache_data = (
                    storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates,
                    storage.cached_htmls, storage.cached_content_types, storage.cached_sources,
                    storage.cached_image_urls, storage.cached_attachment_urls, storage.cached_attachment_types
                )
                # 24ì‹œê°„ ìœ íš¨ (86400ì´ˆ)
                storage.redis_client.setex('pinecone_metadata', 86400, pickle.dumps(cache_data))
                logger.info("ğŸ’¾ ë°ì´í„°ë¥¼ Redisì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ë‹¤ìŒ ì¬ì‹œì‘ë¶€í„°ëŠ” 3ì´ˆ ë¡œë”©!)")
            except Exception as e:
                logger.warning(f"âš ï¸  Redis ì €ì¥ ì‹¤íŒ¨ (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©): {e}")
        else:
            logger.warning("âš ï¸  Redis ë¯¸ì‚¬ìš© (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©)")

        # Retriever ì´ˆê¸°í™”
        _initialize_retrievers()
        logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(storage.cached_titles)}, texts: {len(storage.cached_texts)})")

    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì•±ì´ í¬ë˜ì‹œí•˜ì§€ ì•Šë„ë¡ í•¨
        storage.cached_titles = []
        storage.cached_texts = []
        storage.cached_urls = []
        storage.cached_dates = []
        storage.cached_htmls = []
        storage.cached_content_types = []
        storage.cached_sources = []
        storage.cached_image_urls = []
        storage.cached_attachment_urls = []
        storage.cached_attachment_types = []
        logger.warning("âš ï¸  ìºì‹œë¥¼ ë¹ˆ ìƒíƒœë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")


def _initialize_retrievers():
    """Retriever ì´ˆê¸°í™” ë¡œì§ (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë¶„ë¦¬)"""
    logger.info("ğŸ”§ ê²€ìƒ‰ ì—”ì§„(BM25/Dense) êµ¬ì¶• ì¤‘...")

    from modules.retrieval import (
        BM25Retriever,
        DenseRetriever,
        DocumentCombiner,
        DocumentClusterer
    )

    # BM25Retriever ì´ˆê¸°í™” (HTML ë°ì´í„° í¬í•¨)
    bm25_retriever = BM25Retriever(
        titles=storage.cached_titles,
        texts=storage.cached_texts,
        urls=storage.cached_urls,
        dates=storage.cached_dates,
        query_transformer=transformed_query,
        similarity_adjuster=adjust_similarity_scores,
        htmls=storage.cached_htmls,  # HTML êµ¬ì¡°í™” ë°ì´í„° ì¶”ê°€
        k1=1.5,
        b=0.75
    )
    storage.set_bm25_retriever(bm25_retriever)

    # DenseRetriever ì´ˆê¸°í™”
    dense_retriever = DenseRetriever(
        embeddings_factory=get_embeddings,
        pinecone_index=storage.pinecone_index,
        date_adjuster=adjust_date_similarity,
        similarity_scale=3.26,
        noun_weight=0.20,
        digit_weight=0.24
    )
    storage.set_dense_retriever(dense_retriever)

    # DocumentCombiner ì´ˆê¸°í™”
    document_combiner = DocumentCombiner(
        keyword_filter=last_filter_keyword,
        date_adjuster=adjust_date_similarity
    )
    storage.set_document_combiner(document_combiner)

    # DocumentClusterer ì´ˆê¸°í™”
    document_clusterer = DocumentClusterer(
        date_parser=parse_date_change_korea_time,
        similarity_threshold=0.89
    )
    storage.set_document_clusterer(document_clusterer)

    logger.info("âœ… ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")

                    #################################   24.11.16ê¸°ì¤€ ì •í™•ë„ ì¸¡ì •ì™„ë£Œ #####################################################
######################################################################################################################

# ë‚ ì§œë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
# ì´ì œëŠ” utils.date_utils.parse_date_change_korea_time ì‚¬ìš© ê¶Œì¥

def parse_date_change_korea_time(date_str):
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
    ISO 8601 í˜•ì‹ê³¼ ë ˆê±°ì‹œ í•œêµ­ì–´ í˜•ì‹ ëª¨ë‘ ì§€ì›

    Args:
        date_str: ISO 8601 í˜•ì‹ ë˜ëŠ” "ì‘ì„±ì¼25-10-17 15:48" í˜•ì‹

    Returns:
        datetime ê°ì²´ (í•œêµ­ ì‹œê°„ëŒ€)
    """
    # ë¹ˆ ë¬¸ìì—´ì´ë©´ None
    if not date_str:
        return None

    try:
        # ë¨¼ì € ISO 8601 í˜•ì‹ ì‹œë„ (ìƒˆ í˜•ì‹)
        dt = datetime.fromisoformat(date_str)
        # ì‹œê°„ëŒ€ê°€ ì—†ìœ¼ë©´ í•œêµ­ ì‹œê°„ëŒ€ ì¶”ê°€
        if dt.tzinfo is None:
            korea_timezone = pytz.timezone('Asia/Seoul')
            return korea_timezone.localize(dt)
        return dt
    except (ValueError, TypeError):
        pass

    try:
        # ë ˆê±°ì‹œ í•œêµ­ì–´ í˜•ì‹ ì‹œë„ (í•˜ìœ„ í˜¸í™˜ì„±)
        clean_date_str = date_str.replace("ì‘ì„±ì¼", "").strip()
        naive_date = datetime.strptime(clean_date_str, "%y-%m-%d %H:%M")
        # í•œêµ­ ì‹œê°„ëŒ€ ì¶”ê°€
        korea_timezone = pytz.timezone('Asia/Seoul')
        return korea_timezone.localize(naive_date)
    except (ValueError, TypeError):
        return None


def calculate_weight_by_days_difference(post_date, current_date, query_nouns):

    # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
    days_diff = (current_date - post_date).days

    # ê¸°ì¤€ ë‚ ì§œ (24-01-01 00:00) ì„¤ì •
    baseline_date_str = "24-01-01 00:00"
    baseline_date = parse_date_change_korea_time(baseline_date_str)
    graduate_weight = 1.0 if any(keyword in query_nouns for keyword in ['ì¡¸ì—…', 'ì¸í„°ë·°']) else 0
    scholar_weight = 1.0 if 'ì¥í•™' in query_nouns else 0
    # ì‘ì„±ì¼ì´ ê¸°ì¤€ ë‚ ì§œ ì´ì „ì´ë©´ ê°€ì¤‘ì¹˜ë¥¼ 1.35ë¡œ ê³ ì •
    if post_date <= baseline_date:
        return 1.35 + graduate_weight / 5

    # 'ìµœê·¼', 'ìµœì‹ ' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°, ìµœê·¼ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€
    add_recent_weight = 1.5 if any(keyword in query_nouns for keyword in ['ìµœê·¼', 'ìµœì‹ ', 'ì§€ê¸ˆ', 'í˜„ì¬']) else 0

    # **10ì¼ ë‹¨ìœ„ êµ¬ë¶„**: ìµœê·¼ ë¬¸ì„œì— ëŒ€í•œ ì„¸ë°€í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    if days_diff <= 6:
        return 1.355 + add_recent_weight + graduate_weight + scholar_weight
    elif days_diff <= 12:
        return 1.330 + add_recent_weight / 3.0 + graduate_weight / 1.2 + scholar_weight / 1.5
    elif days_diff <= 18:
        return 1.321 + add_recent_weight / 5.0 + graduate_weight / 1.3 + scholar_weight / 2.0
    elif days_diff <= 24:
        return 1.310 + add_recent_weight / 7.0 + graduate_weight / 1.4 + scholar_weight / 2.5
    elif days_diff <= 30:
        return 1.290 + add_recent_weight / 9.0 + graduate_weight / 1.5 + scholar_weight / 3.0
    elif days_diff <= 36:
        return 1.270 + graduate_weight / 1.6 + scholar_weight / 3.5
    elif days_diff <= 45:
        return 1.250 +graduate_weight / 1.7 + scholar_weight / 4.0
    elif days_diff <= 60:
        return 1.230 +graduate_weight / 1.8 + scholar_weight / 4.5
    elif days_diff <= 90:
        return 1.210 +graduate_weight / 2.0 + scholar_weight / 5.0

    # **ì›” ë‹¨ìœ„ êµ¬ë¶„**: 2ê°œì›” ì´í›„ëŠ” ì›” ë‹¨ìœ„ë¡œ ë‹¨ìˆœí™”
    month_diff = (days_diff - 90) // 30
    month_weight_map = {
        0: 1.19,
        1: 1.17 - add_recent_weight / 6 - scholar_weight / 10,
        2: 1.15 - add_recent_weight / 5 - scholar_weight / 9,
        3: 1.13 - add_recent_weight / 4 - scholar_weight / 7,
        4: 1.11 - add_recent_weight / 3  - scholar_weight / 5,
    }

    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë°˜í™˜ (6ê°œì›” ì´í›„)
    return month_weight_map.get(month_diff, 0.88 - add_recent_weight /2  - scholar_weight / 5)


# ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜

def adjust_date_similarity(similarity, date_str,query_nouns):
    # í˜„ì¬ í•œêµ­ ì‹œê°„
    current_time = get_korean_time()
    # ì‘ì„±ì¼ íŒŒì‹±
    post_date = parse_date_change_korea_time(date_str)
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    weight = calculate_weight_by_days_difference(post_date, current_time,query_nouns)
    # ì¡°ì •ëœ ìœ ì‚¬ë„ ë°˜í™˜
    return similarity * weight

# ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ ëª…ì‚¬ì™€ ê° ë¬¸ì„œ ì œëª©ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜
# (ì´ì „ ë²„ì „ì€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤ - ìµœì í™”ëœ ë²„ì „ë§Œ ìœ ì§€)

def adjust_similarity_scores(query_noun, title, texts, similarities):
    query_noun_set = set(query_noun)
    title_tokens = [set(titl.split()) for titl in title]

    for idx, titl_tokens in enumerate(title_tokens):
        matching_noun = query_noun_set.intersection(titl_tokens)
        
        if texts[idx] == "No content":
            similarities[idx] *= 1.5
            if "êµ­ê°€ì¥í•™ê¸ˆ" in query_noun_set and "êµ­ê°€ì¥í•™ê¸ˆ" in titl_tokens:
                similarities[idx] *= 5.0
        
        for noun in matching_noun:
            len_adjustment = len(noun) * 0.21
            similarities[idx] += len_adjustment
            if re.search(r'\d', noun):  # ìˆ«ì í¬í•¨ ì—¬ë¶€
                similarities[idx] += len(noun) * (0.22 if noun in titl_tokens else 0.19)

        if query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and titl_tokens.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}):
            similarities[idx] += 2.0
        if not query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and 'ëŒ€í•™ì›' in titl_tokens:
            similarities[idx] -= 2.0

    return similarities


#############################################################################################

# í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜ (ë¦¬íŒ©í† ë§ë¨ - KeywordFilter ì‚¬ìš©)
def last_filter_keyword(DOCS, query_noun, user_question):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§

    Args:
        DOCS: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ [(score, title, date, text, url), ...]
        query_noun: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
        user_question: ì›ë³¸ ì§ˆë¬¸

    Returns:
        List[Tuple]: í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ì¡°ì •ë¨)
    """
    return storage.keyword_filter.filter(DOCS, query_noun, user_question)

#################################################################################################

def find_url(url, title, doc_date, text, doc_url, number):
    return_docs = []
    for i, urls in enumerate(doc_url):
        if urls.startswith(url):  # indexsì™€ ì‹œì‘ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            return_docs.append((title[i], doc_date[i], text[i], doc_url[i]))
    
    # doc_url[i] ìˆœì„œëŒ€ë¡œ ì •ë ¬
    return_docs.sort(key=lambda x: x[3],reverse=True) 

    # ê³ ìœ  ìˆ«ìë¥¼ ì¶”ì í•˜ë©° numberê°œì˜ ë¬¸ì„œ ì„ íƒ
    unique_numbers = set()
    filtered_docs = []

    for doc in return_docs:
        # ìˆ«ìê°€ ì„œë¡œ ë‹¤ë¥¸ numberê°œê°€ ëª¨ì´ë©´ ì¢…ë£Œ
        if len(unique_numbers) >= number:
            break
        url_number = ''.join(filter(str.isdigit, doc[3]))  # URLì—ì„œ ìˆ«ì ì¶”ì¶œ
        unique_numbers.add(url_number)
        filtered_docs.append(doc)


    return filtered_docs


########################################################################################  best_docs ì‹œì‘ ##########################################################################################


def best_docs(user_question):
      # ì‚¬ìš©ì ì§ˆë¬¸
      noun_time=time.time()
      query_noun=transformed_query(user_question)
      query_noun_time=time.time()-noun_time
      print(f"ëª…ì‚¬í™” ë³€í™˜ ì‹œê°„ : {query_noun_time}")
      titles_from_pinecone, texts_from_pinecone, urls_from_pinecone, dates_from_pinecone = storage.cached_titles, storage.cached_texts, storage.cached_urls, storage.cached_dates
      if not query_noun:
        return None,None
      #######  ìµœê·¼ ê³µì§€ì‚¬í•­, ì±„ìš©, ì„¸ë¯¸ë‚˜, í–‰ì‚¬, íŠ¹ê°•ì˜ ë‹¨ìˆœí•œ ì •ë³´ë¥¼ ìš”êµ¬í•˜ëŠ” ê²½ìš°ë¥¼ í•„í„°ë§ í•˜ê¸° ìœ„í•œ ë§¤ì»¤ë‹ˆì¦˜ ########
      remove_noticement = ['ëª©ë¡','ë¦¬ìŠ¤íŠ¸','ë‚´ìš©','ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì—…ë°ì´íŠ¸',
                           'ì»´í“¨í„°í•™ë¶€','ì»´í•™','ìƒìœ„','ì •ë³´','ê´€ë ¨','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ ','ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬']
      query_nouns = [noun for noun in query_noun if noun not in remove_noticement]
      return_docs=[]
      key=None
      numbers=5 ## ê¸°ë³¸ìœ¼ë¡œ 5ê°œ ë¬¸ì„œ ë°˜í™˜í•  ê²ƒ.
      check_num=0
      recent_time=time.time()
      for noun in query_nouns:
        if 'ê°œ' in noun:
            # ìˆ«ì ì¶”ì¶œ
            num = re.findall(r'\d+', noun)
            if num:
                numbers=int(num[0])
                check_num=1
      if (any(keyword in query_noun for keyword in ['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ '])and any(keyword in query_noun for keyword in ['ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬'])and len(query_nouns)<1 or check_num==1):    
        if numbers ==0:
          #### 0ê°œì˜ keywordì— ëŒ€í•´ì„œ ì§ˆë¬¸í•œë‹¤ë©´? ex) ê°€ì¥ ìµœê·¼ ê³µì§€ì‚¬í•­ 0ê°œ ì•Œë ¤ì¤˜######
          keys=['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©']
          return None,[keyword for keyword in keys if keyword in user_question]
        if 'ê³µì§€ì‚¬í•­' in query_noun:
          key=['ê³µì§€ì‚¬í•­']
          notice_url = NOTICE_BASE_URL + "&wr_id="
          return_docs=find_url(notice_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        if 'ì±„ìš©' in query_noun:
          key=['ì±„ìš©']
          company_url = COMPANY_BASE_URL + "&wr_id="
          return_docs=find_url(company_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        other_key = ['ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°']
        if any(keyword in query_noun for keyword in other_key):
          seminar_url = SEMINAR_BASE_URL + "&wr_id="
          key = [keyword for keyword in other_key if keyword in user_question]
          return_docs=find_url(seminar_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        recent_finish_time=time.time()-recent_time
        print(f"ìµœê·¼ ê³µì§€ì‚¬í•­ ë¬¸ì„œ ë½‘ëŠ” ì‹œê°„ {recent_finish_time}")
        if (len(return_docs)>0):
          return return_docs,key


      remove_noticement = ['ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì»´í•™','ìƒìœ„','ê´€ë ¨']

      # BM25 ê²€ìƒ‰ (ë¦¬íŒ©í† ë§ë¨ - BM25Retriever ì‚¬ìš©)
      bm_title_time = time.time()
      Bm25_best_docs, adjusted_similarities = storage.bm25_retriever.search(
          query_nouns=query_noun,
          top_k=25,
          normalize_factor=24.0
      )
      bm_title_f_time = time.time() - bm_title_time
      print(f"bm25 ë¬¸ì„œ ë½‘ëŠ”ì‹œê°„: {bm_title_f_time}")
      ####################################################################################################
      # Dense Retrieval (ë¦¬íŒ©í† ë§ë¨ - DenseRetriever ì‚¬ìš©)
      dense_time = time.time()
      combine_dense_docs = storage.dense_retriever.search(
          user_question=user_question,
          query_nouns=query_noun,
          top_k=30
      )
      pinecone_time = time.time() - dense_time
      print(f"íŒŒì¸ì½˜ì—ì„œ top k ë½‘ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ {pinecone_time}")

      # ## ê²°ê³¼ ì¶œë ¥
      # print("\ní†µí•©ëœ íŒŒì¸ì½˜ë¬¸ì„œ ìœ ì‚¬ë„:")
      # for score, doc in combine_dense_docs:
      #     title, date, text, url = doc
      #     print(f"ì œëª©: {title}\nìœ ì‚¬ë„: {score} {url}")
      #     print('---------------------------------')


      #################################################3#################################################3
      #####################################################################################################3

      # BM25ì™€ Dense Retrieval ê²°ê³¼ ê²°í•© (ë¦¬íŒ©í† ë§ë¨ - DocumentCombiner ì‚¬ìš©)
      combine_time = time.time()
      final_best_docs = storage.document_combiner.combine(
          dense_results=combine_dense_docs,
          bm25_results=Bm25_best_docs,
          bm25_similarities=adjusted_similarities,
          titles_from_pinecone=titles_from_pinecone,
          query_nouns=query_noun,
          user_question=user_question,
          top_k=20
      )
      combine_f_time = time.time() - combine_time
      print(f"Bm25ë‘ pinecone ê²°í•© ì‹œê°„: {combine_f_time}")
      # ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° ìµœì  í´ëŸ¬ìŠ¤í„° ì„ íƒ (ë¦¬íŒ©í† ë§ë¨ - DocumentClusterer ì‚¬ìš©)
      cluster_time = time.time()
      final_cluster, count = storage.document_clusterer.cluster_and_select(
          documents=final_best_docs,
          query_nouns=query_noun,
          all_titles=titles_from_pinecone,
          all_dates=dates_from_pinecone,
          all_texts=texts_from_pinecone,
          all_urls=urls_from_pinecone
      )
      cluster_f_time = time.time() - cluster_time
      print(f"clusterë¡œ ë¬¸ì„œ ì¶”ì¶œí•˜ëŠ” ì‹œê°„:{cluster_f_time}")

      return final_cluster, query_noun

prompt_template = """ë‹¹ì‹ ì€ ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ ê³µì§€ì‚¬í•­ì„ ì „ë‹¬í•˜ëŠ” ì§ì›ì´ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì •í™•í•˜ê²Œ ì „ë‹¬í•´ì•¼ í•  ì˜ë¬´ê°€ ìˆìŠµë‹ˆë‹¤.
í˜„ì¬ í•œêµ­ ì‹œê°„: {current_time}

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:

1. ì§ˆë¬¸ì˜ ë‚´ìš©ì´ ì´ë²¤íŠ¸ì˜ ê¸°ê°„ì— ëŒ€í•œ ê²ƒì¼ ê²½ìš°, ë¬¸ì„œì— ì£¼ì–´ì§„ ê¸°í•œê³¼ í˜„ì¬ í•œêµ­ ì‹œê°„ì„ ë¹„êµí•˜ì—¬ í•´ë‹¹ ì´ë²¤íŠ¸ê°€ ì˜ˆì •ëœ ê²ƒì¸ì§€, ì§„í–‰ ì¤‘ì¸ì§€, ë˜ëŠ” ì´ë¯¸ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
  ì˜ˆë¥¼ ë“¤ì–´, "2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì¼ì •ì€ ì–¸ì œì•¼?"ë¼ëŠ” ì§ˆë¬¸ì„ ë°›ì•˜ì„ ê²½ìš°, í˜„ì¬ ì‹œê°„ì€ 11ì›”ì´ë¼ê³  ê°€ì •í•˜ë©´ ìˆ˜ê°•ì‹ ì²­ì€ ê¸°ê°„ì€ 8ì›”ì´ì—ˆìœ¼ë¯€ë¡œ ì´ë¯¸ ì¢…ë£Œëœ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤.
  ë”°ë¼ì„œ, "2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ì€ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ê³ , 2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
  ë˜ ë‹¤ë¥¸ ì˜ˆì‹œë¡œ í˜„ì¬ ì‹œê°„ì´ 11ì›” 12ì¼ì´ë¼ê³  ê°€ì •í•˜ì˜€ì„ ë•Œ, "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì€ ì–¸ì œì•¼?"ë¼ëŠ” ì§ˆë¬¸ì„ ë°›ì•˜ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì´ 11ì›” 13ì¼ì´ë¼ë©´ ì•„ì§ ì‹œì‘ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤.
  ë”°ë¼ì„œ, "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ì€ ì•„ì§ ì‹œì‘ ì „ì…ë‹ˆë‹¤."ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ ì¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
  ë˜ ë‹¤ë¥¸ ì˜ˆì‹œë¡œ í˜„ì¬ ì‹œê°„ì´ 11ì›” 13ì¼ì´ë¼ê³  ê°€ì •í•˜ì˜€ì„ ë•Œ, "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì€ ì–¸ì œì•¼?"ë¼ëŠ” ì§ˆë¬¸ì„ ë°›ì•˜ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì´ 11ì›” 13ì¼ì´ë¼ë©´ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤.
  ë”°ë¼ì„œ, "í˜„ì¬ ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì…ë‹ˆë‹¤."ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ ì¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
2. ì§ˆë¬¸ì—ì„œ í•µì‹¬ì ì¸ í‚¤ì›Œë“œë“¤ì„ ê³¨ë¼ í‚¤ì›Œë“œë“¤ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„ì„œ í•´ë‹¹ ë¬¸ì„œë¥¼ ì½ê³  ì •í™•í•œ ë‚´ìš©ì„ ë‹µë³€í•´ì£¼ì„¸ìš”.
3. ë¬¸ì„œì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ê¸¸ê²Œ ì „ë‹¬í•˜ê¸°ë³´ë‹¤ëŠ” ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ëŠ” ë‚´ìš©ì— í•´ë‹¹í•˜ëŠ” ë‹µë³€ë§Œì„ ì œê³µí•¨ìœ¼ë¡œì¨ ìµœëŒ€í•œ ë‹µë³€ì„ ê°„ê²°í•˜ê³  ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
4. ì—ì´ë¹…ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ì„ì˜ë¡œ íŒë‹¨í•´ì„œ ë„¤ ì•„ë‹ˆì˜¤ í•˜ì§€ ë§ê³  ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.
5. ë‹µë³€ì€ ì¹œì ˆí•˜ê²Œ ì¡´ëŒ“ë§ë¡œ ì œê³µí•˜ì„¸ìš”.
6. ì§ˆë¬¸ì´ ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ë‹¤ê³  íŒë‹¨í•˜ë©´ ì‘ë‹µí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ë©´ "ë„ˆëŠ” ë¬´ì—‡ì„ ì•Œê¹Œ", "ì ì‹¬ë©”ë‰´ ì¶”ì²œ"ê³¼ ê°™ì´ ì¼ë°˜ ìƒì‹ì„ ìš”êµ¬í•˜ëŠ” ì§ˆë¬¸ì€ ê±°ì ˆí•´ì£¼ì„¸ìš”.
7. ì—ì´ë¹… ì¸ì • ê´€ë ¨ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ê³„ì ˆí•™ê¸°ì¸ì§€ ê·¸ëƒ¥ í•™ê¸°ë¥¼ ë¬»ëŠ”ê²ƒì¸ì§€ ì§ˆë¬¸ì„ ì²´í¬í•´ì•¼í•©ë‹ˆë‹¤. ê³„ì ˆí•™ê¸°ê°€ ì•„ë‹Œ ê²½ìš°ì— ì‹¬ì»´,ê¸€ì†,ì¸ì»´ ê°œì„¤ì´ ì•„ë‹ˆë©´ ì—ì´ë¹… ì¸ì •ì´ ì•ˆë©ë‹ˆë‹¤.

**ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ í™œìš© ê°€ì´ë“œ:**
8. ì»¨í…ìŠ¤íŠ¸ì— HTML í‘œ(<table>, <tr>, <td> ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, í‘œ êµ¬ì¡°ë¥¼ ì •í™•íˆ íŒŒì‹±í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
  - ì˜ˆì‹œ: <tr><td>ì„±ì ìš°ìˆ˜ì¥í•™ê¸ˆ</td><td>300ë§Œì›</td></tr>ëŠ” "ì„±ì ìš°ìˆ˜ì¥í•™ê¸ˆ: 300ë§Œì›"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
  - í‘œì˜ í–‰(row)ê³¼ ì—´(column) ê´€ê³„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
9. ì»¨í…ìŠ¤íŠ¸ ì¶œì²˜ ë¼ë²¨([ë³¸ë¬¸], [ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸], [ì²¨ë¶€íŒŒì¼: PDF])ì„ ì°¸ê³ í•˜ì—¬ ì •ë³´ì˜ ì‹ ë¢°ë„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.
  - [ë³¸ë¬¸]: ì›ë³¸ ê²Œì‹œê¸€ í…ìŠ¤íŠ¸ (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ)
  - [ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸]: ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ (OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„± ê³ ë ¤)
  - [ì²¨ë¶€íŒŒì¼: PDF/HWP/DOCX]: ì²¨ë¶€íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ë° êµ¬ì¡° (ê³µì‹ ë¬¸ì„œë¡œ ì‹ ë¢°ë„ ë†’ìŒ)
10. HTML ë¦¬ìŠ¤íŠ¸(<ul>, <ol>, <li>)ë‚˜ ì¤‘ì²© êµ¬ì¡°ê°€ ìˆìœ¼ë©´, ê³„ì¸µ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€:"""

# PromptTemplate ê°ì²´ ìƒì„±
PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["current_time", "context", "question"]
)

def format_docs(docs):
    """
    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
    ì¶œì²˜(ì›ë³¸/ì´ë¯¸ì§€OCR/ì²¨ë¶€íŒŒì¼)ë¥¼ ë¼ë²¨ë¡œ í‘œì‹œí•˜ì—¬ ë§¥ë½ ì œê³µ
    ê° ì²­í¬ì— ì œëª© ì •ë³´ë¥¼ ëª…ì‹œí•˜ì—¬ ë¬¸ë§¥ ë‹¨ì ˆ(Context Fragmentation) ë¬¸ì œ í•´ê²°

    Args:
        docs: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸

    Returns:
        str: í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    formatted = []

    for doc in docs:
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œëª© ì¶”ì¶œ
        title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')

        # ì¶œì²˜ì— ë”°ë¼ ë¼ë²¨ ìƒì„±
        source = doc.metadata.get('source', 'original_post')
        content_type = doc.metadata.get('content_type', 'text')

        if source == "image_ocr":
            label = "[ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸]"
        elif source == "document_parse":
            # ì²¨ë¶€íŒŒì¼ íƒ€ì… í‘œì‹œ
            attachment_type = doc.metadata.get('attachment_type', 'document')
            label = f"[ì²¨ë¶€íŒŒì¼: {attachment_type.upper()}]"
        else:
            # ì›ë³¸ ê²Œì‹œê¸€
            label = "[ë³¸ë¬¸]"

        # ì œëª© + ë¼ë²¨ + ë‚´ìš© (ì œëª©ì„ ëª…ì‹œí•˜ì—¬ ì²­í¬ì˜ ë¬¸ë§¥ ì œê³µ)
        formatted.append(f"ë¬¸ì„œ ì œëª©: {title}\n{label}\n{doc.page_content}")

    return "\n\n".join(formatted)


def get_answer_from_chain(best_docs, user_question,query_noun):

    documents = []
    doc_titles = []
    doc_dates = []
    doc_texts = []
    doc_urls = []
    for doc in best_docs:
        tit = doc[1]
        date = doc[2]
        text = doc[3]
        url = doc[4]
        # score,tit, date, text, url,im_url = doc
        doc_titles.append(tit)  # ì œëª©
        doc_dates.append(date)    # ë‚ ì§œ
        doc_texts.append(text)    # ë³¸ë¬¸
        doc_urls.append(url)     # URL

    # ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ Document ê°ì²´ ìƒì„±
    documents = []
    for title, text, url, date in zip(doc_titles, doc_texts, doc_urls, doc_dates):
        # URLë¡œ ìºì‹œëœ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° ì°¾ê¸°
        try:
            idx = storage.cached_urls.index(url)
            html = storage.cached_htmls[idx] if idx < len(storage.cached_htmls) else ""
            content_type = storage.cached_content_types[idx] if idx < len(storage.cached_content_types) else "text"
            source = storage.cached_sources[idx] if idx < len(storage.cached_sources) else "original_post"
            attachment_type = storage.cached_attachment_types[idx] if idx < len(storage.cached_attachment_types) else ""
        except (ValueError, IndexError):
            # URLì„ ì°¾ì§€ ëª»í•˜ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            html = ""
            content_type = "text"
            source = "original_post"
            attachment_type = ""

        # HTMLì´ ìˆìœ¼ë©´ Markdownìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©, ì—†ìœ¼ë©´ textë¥¼ ì‚¬ìš©
        if html:
            # HTMLì„ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í‘œ êµ¬ì¡° ë³´ì¡´)
            try:
                soup = BeautifulSoup(html, 'html.parser')

                # í…Œì´ë¸”ì´ ìˆìœ¼ë©´ Markdown í‘œë¡œ ë³€í™˜
                markdown_content = ""
                for table in soup.find_all('table'):
                    markdown_content += "\n\n**[í‘œ ë°ì´í„°]**\n"
                    rows = table.find_all('tr')
                    for row_idx, row in enumerate(rows):
                        cells = row.find_all(['th', 'td'])
                        row_text = " | ".join([cell.get_text(strip=True) for cell in cells])
                        markdown_content += f"| {row_text} |\n"
                        # í—¤ë” í–‰ ë‹¤ìŒì— êµ¬ë¶„ì„  ì¶”ê°€
                        if row_idx == 0:
                            markdown_content += "| " + " | ".join(["---"] * len(cells)) + " |\n"
                    markdown_content += "\n"

                # í…Œì´ë¸” ì™¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for table in soup.find_all('table'):
                    table.decompose()  # í…Œì´ë¸” ì œê±° (ì¤‘ë³µ ë°©ì§€)

                plain_text_from_html = soup.get_text(separator='\n', strip=True)

                # ìµœì¢… page_content: Markdown í‘œ + í‰ë¬¸
                page_content = (markdown_content + "\n" + plain_text_from_html).strip()

                # ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì›ë³¸ text ì‚¬ìš©
                if not page_content:
                    page_content = text
            except Exception as e:
                logger.debug(f"HTML ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
                page_content = text
        else:
            page_content = text

        # ë‚ ì§œ íŒŒì‹± (ISO 8601ê³¼ ë ˆê±°ì‹œ í˜•ì‹ ëª¨ë‘ ì§€ì›)
        try:
            if date.startswith("ì‘ì„±ì¼"):
                doc_date = datetime.strptime(date, 'ì‘ì„±ì¼%y-%m-%d %H:%M')
            else:
                doc_date = datetime.fromisoformat(date)
        except:
            doc_date = datetime.now()

        # Document ê°ì²´ ìƒì„± (ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° í¬í•¨)
        doc = Document(
            page_content=page_content,  # HTML ìš°ì„ , ì—†ìœ¼ë©´ text
            metadata={
                "title": title,
                "url": url,
                "doc_date": doc_date,
                "content_type": content_type,
                "source": source,
                "attachment_type": attachment_type,
                "plain_text": text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ë³´ê´€
            }
        )
        documents.append(doc)

    relevant_docs = [doc for doc in documents if any(keyword in doc.page_content for keyword in query_noun)]
    if not relevant_docs:
      return None, None

    llm = ChatUpstage(api_key=storage.upstage_api_key)
    relevant_docs_content=format_docs(relevant_docs)

    qa_chain = (
        {
            "current_time": lambda _: get_korean_time().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„"),
            "context": RunnableLambda(lambda _: relevant_docs_content),
            "question": RunnablePassthrough()
        }
        | PROMPT
        | llm
        | StrOutputParser()
    )

    return qa_chain,relevant_docs



#######################################################################

def question_valid(question, top_docs, query_noun):
    prompt = f"""
ì•„ë˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´, ì£¼ì–´ì§„ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ "ì˜ˆ" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”. ê° ì§ˆë¬¸ì— ëŒ€í•´ í•™ì‚¬ ê´€ë ¨ ì—¬ë¶€ë¥¼ ëª…í™•íˆ íŒë‹¨í•˜ê³ , ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ, ì œê³µë˜ëŠ” ê²½ìš°ì—ëŠ” "ì˜ˆ"ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."

1. í•µì‹¬ íŒë‹¨ ì›ì¹™
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ë‹¤ë£¨ëŠ” ì •ë³´ì—ë§Œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•˜ë©°, ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì€ "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.

ì§ˆë¬¸ ë¶„ì„ 3ë‹¨ê³„:

ì§ˆë¬¸ì˜ ì‹¤ì œ ì˜ë„ì™€ ëª©ì  íŒŒì•…
í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ì œê³µë˜ëŠ” ì •ë³´ ì—¬ë¶€ í™•ì¸
í•™ì‚¬ ê´€ë ¨ì„± ìµœì¢… í™•ì¸

ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬:

ì£¼ìš” ì§ˆë¬¸ê³¼ ë¶€ê°€ ì§ˆë¬¸ êµ¬ë¶„
ë¶€ìˆ˜ì  ë‚´ìš©ì€ íŒë‹¨ì—ì„œ ì œì™¸
í•™ë¶€ ê³µì‹ ì •ë³´ì™€ ë¬´ê´€í•œ ì§ˆë¬¸ êµ¬ë³„
ì•…ì˜ì  ì§ˆë¬¸ ëŒ€ì‘:

í•™ì‚¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì—ˆë”ë¼ë„, ì‹¤ì œë¡œ í•™ë¶€ ì •ë³´ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ì„ "ì•„ë‹ˆì˜¤"ë¡œ ë‹µë³€
2. "ì˜ˆ"ë¡œ íŒë‹¨í•˜ëŠ” í•™ì‚¬ ê´€ë ¨ ì¹´í…Œê³ ë¦¬:
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ë‹¤ë£¨ëŠ” í•™ì‚¬ ì •ë³´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³ , í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ "ì˜ˆ"ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
ìˆ˜ì—… ë° í•™ì  ê´€ë ¨ ì •ë³´: ìˆ˜ê°•ì‹ ì²­, ìˆ˜ê°•ì •ì •, ìˆ˜ê°•ë³€ê²½, ìˆ˜ê°•ì·¨ì†Œ, ê¸°ë§ê³ ì‚¬, ì¤‘ê°„ê³ ì‚¬, ê³¼ëª© ìš´ì˜ ë°©ì‹, í•™ì  ì¸ì •, ë³µìˆ˜ì „ê³µ í˜¹ì€ ë¶€ì „ê³µ ìš”ê±´,êµì–‘ê°•ì˜ì™€ ê´€ë ¨ëœ ì§ˆë¬¸, ì „ê³µê°•ì˜ì™€ ê´€ë ¨ëœ ì§ˆë¬¸, ì‹¬ì»´, ì¸ì»´, ê¸€ì†¦ í•™ê³¼ì— ê´€ë ¨ëœ ì§ˆë¬¸, ê°•ì˜ ê°œì„  ê´€ë ¨ ì„¤ë¬¸
í•™ìƒ ì§€ì› ì œë„: ì¥í•™ê¸ˆ, í•™ê³¼ ì£¼ê´€ ì¸í„´ì‹­ í”„ë¡œê·¸ë¨, ë©˜í† ë§ ,ê°ì¢… ì¥í•™ìƒ ì„ ë°œ, í•™ìê¸ˆëŒ€ì¶œ, íŠ¹ì • ì§€ì—­ì˜ í•™ìê¸ˆëŒ€ì¶œ ê´€ë ¨ ì§ˆë¬¸
í•™ì‚¬ í–‰ì • ë° ì œë„: ì¡¸ì—… ìš”ê±´, í•™ì  ê´€ë¦¬, í•„ìˆ˜ ì´ìˆ˜ ìš”ê±´, ì¦ëª…ì„œ ë°œê¸‰, í•™ì‚¬ ì¼ì •, ìí‡´,ë³µí•™, íœ´í•™ ë“±
êµìˆ˜ì§„ ë° í–‰ì • ì •ë³´: êµìˆ˜ì§„ ì—°ë½ì²˜,ë²ˆí˜¸,ì´ë©”ì¼, í•™ê³¼ ì‚¬ë¬´ì‹¤ ì •ë³´, ì§€ë„êµìˆ˜ì™€ ê´€ë ¨ëœ ì •ë³´
í•™ë¶€ ì£¼ê´€ êµë‚´ í™œë™:  ê°ì¢… ê²½ì§„ëŒ€íšŒ, í–‰ì‚¬, ë²¤ì²˜í”„ë¡œê·¸ë¨ ,ë²¤ì²˜ì•„ì¹´ë°ë¯¸,íŠœí„°(TUTOR) ê´€ë ¨ í™œë™(ê·¼ë¬´ì¼ì§€ ì‘ì„±, ê·¼ë¬´ ê¸°ì¤€) íŠœí„°(TUTOR) ëª¨ì§‘ ë° ë¹„ìš© ê´€ë ¨ ì§ˆë¬¸, ë‹¤ì–‘í•œ í”„ë¡œê·¸ë¨(ì˜ˆ: AEP í”„ë¡œê·¸ë¨, CES í”„ë¡œê·¸ë¨,ë¯¸êµ­ í”„ë¡œê·¸ë¨)
ì‹ ì²­ ë° ì¼ì •, ì„±ì¸ì§€ êµìœ¡ì´ë‚˜ ì¸ê¶Œ êµìœ¡, í˜¹ì€ ë‹¤ë¥¸ êµìœ¡ì— ê´€ë ¨ëœ ì¼ì •
êµìˆ˜ì§„ ì •ë³´: êµìˆ˜ì˜ ëª¨ë“  ì •ë³´(ì´ë©”ì¼,ë²ˆí˜¸,ì—°ë½ì²˜,ë©”ì¼,ì‚¬ì§„,ì „ê³µ,ì—…ë¬´), í•™ê³¼ ê´€ë ¨ ì§ì›ì˜ ëª¨ë“  ì •ë³´, ë‹´ë‹¹ ì—…ë¬´ì™€ ê´€ë ¨ëœ í•™ê³¼ êµì§ì› ì •ë³´
ì¥í•™ê¸ˆ ë° êµë‚´ ì§€ì› ì œë„: ìµœê·¼ ì¥í•™ê¸ˆ ì„ ë°œ ì •ë³´ë‚˜ êµë‚´ ê°ì¢… ì§€ì› ì œë„ì— ëŒ€í•œ ì•ˆë‚´
ì¡¸ì—… ìš”ê±´ ì •ë³´: ì¡¸ì—…ì— í•„ìš”í•œ í•™ì  ìš”ê±´, í•„ìˆ˜ë¡œ ë“¤ì–´ì•¼ í•˜ëŠ” ê°•ì˜, ê³¼ëª©, ë“±ë¡ íšŸìˆ˜ ê´€ë ¨ ì •ë³´, ì¡¸ì—… ì‹œ í•„ìš”í•œ ì •ë³´ , í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë ¨ ì •ë³´ ì „ì²´ì ìœ¼ë¡œ ì¡¸ì—…ì— í•„ìš”í•œ ì •ë³´ëŠ” ë¬´ì¡°ê±´ "ì˜ˆ"ë¡œ í•©ë‹ˆë‹¤.
ê¸°íƒ€ í•™ì‚¬ ì œë„: êµë‚´ ë°©í•™ ì¤‘ ê·¼ë¡œì¥í•™ìƒ ê´€ë ¨ ì •ë³´, ëŒ€í•™ì›ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸,ëŒ€í•™ì›ìƒ í•™ì  ì¸ì • ì ˆì°¨ì™€ ìš”ê±´ ,ì „ì‹œíšŒ ê°œìµœ ë° ì§€ì› ì •ë³´, í–‰ì‚¬ ì§€ì› ì •ë³´, SW ë§ˆì¼ë¦¬ì§€ì™€ ê´€ë ¨ëœ ì •ë³´ ìš”êµ¬, ìŠ¤íƒ€íŠ¸ì—… ì •ë³´, ê°ì¢… íŠ¹ê°• ì •ë³´(ì˜¤í”ˆSW,ì˜¤í”ˆì†ŒìŠ¤, Ai ë“±)
ì±„ìš©ì •ë³´: ì‹ ì…ì‚¬ì› ì±„ìš©,ê²½ë ¥ì‚¬ì› ì±„ìš© ì •ë³´ë‚˜, íŠ¹ì • ê¸°ì—…ì˜ ëª¨ì§‘ ì •ë³´, ì¸í„´ ì±„ìš© ì •ë³´,ë¶€íŠ¸ìº í”„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸, ì±„ìš© ê´€ë ¨ ì§ˆë¬¸ ë˜í•œ í•™ì‚¬ í‚¤ì›Œë“œì— í¬í•¨ì´ ë©ë‹ˆë‹¤.


3. "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•˜ëŠ” ë¹„í•™ì‚¬ ì¹´í…Œê³ ë¦¬
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ ì±—ë´‡ì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.

êµë‚´ ì¼ë°˜ ì •ë³´: ê¸°ìˆ™ì‚¬, ì‹ë‹¹ ë©”ë‰´ ì •ë³´ ë“± ì»´í“¨í„°í•™ë¶€ì™€ ê´€ë ¨ ì—†ëŠ” êµë‚´ ìƒí™œ ì •ë³´
ì¼ë°˜ì  ê¸°ìˆ /ì§€ì‹ ë¬¸ì˜: í”„ë¡œê·¸ë˜ë° ë¬¸ë²•, ê¸°ìˆ  ê°œë… ì„¤ëª…, íŠ¹ì • ë„êµ¬ ì‚¬ìš©ë²• ë“± í•™ì‚¬ ì •ë³´ì™€ ë¬´ê´€í•œ ê¸°ìˆ ì  ì§ˆë¬¸

ë˜í•œ, {query_noun}ê³¼ {top_docs}ë¥¼ ë¹„êµí•˜ì˜€ì„ ë•Œ, {query_noun}ì•  í¬í•¨ëœ ë‹¨ì–´ ì¤‘ 2ê°œ ì´ìƒì´ {top_docs}ì™€ ì™„ì „íˆ ë¬´ê´€í•˜ë‹¤ë©´ "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•˜ì„¸ìš”.

4. ë³µí•© ì§ˆë¬¸ íŒë‹¨ ê°€ì´ë“œ
ì§ˆë¬¸ì˜ í•µì‹¬ ëª©ì ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤:

ì˜ˆì‹œ:
"ì»´í“¨í„°í•™ë¶€ ìˆ˜ê°•ì‹ ì²­ ê¸°ê°„ ì•Œë ¤ì¤˜" â†’ "ì˜ˆ" (í•™ì‚¬ ì¼ì • ì •ë³´ ìš”ì²­)
"ì§€ë„êµìˆ˜ë‹˜ê³¼ ìƒë‹´í•˜ë ¤ë©´ ì–´ë–»ê²Œ ì˜ˆì•½í•˜ë‚˜ìš”?" â†’ "ì˜ˆ" (í•™ë¶€ ë‚´ êµìˆ˜ì§„ ìƒë‹´ ì ˆì°¨)
"í•™êµ ê¸°ìˆ™ì‚¬ ì •ë³´ ì•Œë ¤ì¤˜" â†’ "ì•„ë‹ˆì˜¤" (í•™ë¶€ì™€ ë¬´ê´€í•œ êµë‚´ ìƒí™œ ì •ë³´)
"ê²½ë¶ëŒ€ ì»´í“¨í„°í•™ë¶€ ê³µì§€ì‚¬í•­ì˜ ì œìœ¡ ë ˆì‹œí”¼ ì•Œë ¤ì¤˜" -> "ì•„ë‹ˆì˜¤" (í•™ë¶€ì˜ ê³µì§€ì‚¬í•­ì„ ì•Œë ¤ë‹¬ë¼ê³  í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì˜ë„ì ìœ¼ë¡œ ì œìœ¡ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ë‹¬ë¼ í•˜ëŠ” ì˜ë¯¸)
5. ì£¼ì˜ì‚¬í•­
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í•™ì‚¬ ì •ë³´ ì œê³µì— í•œì •í•˜ì—¬ ë‹¤ìŒì„ ì§€í‚µë‹ˆë‹¤.

ë§¥ë½ ì¤‘ì‹¬ íŒë‹¨: ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ì§€ì–‘, ì§ˆë¬¸ì˜ ì‹¤ì œ ì˜ë„ì— ë§ì¶° íŒë‹¨
ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬: í•™ë¶€ ê´€ë ¨ ì •ë³´ê°€ í•µì‹¬ì¸ì§€ í™•ì¸
ì•…ì˜ì  ì§ˆë¬¸ ëŒ€ì‘: ë¹„í•™ì‚¬ì  ì •ë³´ë¥¼ í˜¼í•©í•œ ì§ˆë¬¸ì€ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ "ì•„ë‹ˆì˜¤"ë¡œ ì²˜ë¦¬

    ### ì§ˆë¬¸: '{question}'
    ### ì°¸ê³  ë¬¸ì„œ: '{top_docs}'
    ### ì§ˆë¬¸ì˜ ëª…ì‚¬í™”: '{query_noun}'
    """

    llm = ChatUpstage(api_key=storage.upstage_api_key)
    response = llm.invoke(prompt)

    if "ì˜ˆ" in response.content.strip():
        return True
    else:
        return False

#######################################################################

##### ìœ ì‚¬ë„ ì œëª© ë‚ ì§œ ë³¸ë¬¸  url image_urlìˆœìœ¼ë¡œ ì €ì¥ë¨

def get_ai_message(question):
    s_time=time.time()
    best_time=time.time()
    top_doc, query_noun = best_docs(question)  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    best_f_time=time.time()-best_time
    print(f"best_docs ë½‘ëŠ” ì‹œê°„:{best_f_time}")

    # query_nounì´ ì—†ê±°ë‚˜ top_docì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not query_noun or not top_doc or len(top_doc) == 0:
        notice_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }
        return not_in_notices_response
    if len(query_noun)==1 and any(keyword in query_noun for keyword in ['ì±„ìš©','ê³µì§€ì‚¬í•­','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','ê°•ì—°','íŠ¹ê°•']):
      seen_urls = set()  # ì´ë¯¸ ë³¸ URLì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©
      response = f"'{query_noun[0]}'ì— ëŒ€í•œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
      show_url=""
      if top_doc !=None:
        for title, date, _, url in top_doc:  # top_docì—ì„œ ì œëª©, ë‚ ì§œ, URL ì¶”ì¶œ
            if url not in seen_urls:
                response += f"ì œëª©: {title}, ë‚ ì§œ: {date} \n----------------------------------------------------\n"
                seen_urls.add(url)  # URL ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
      if 'ì±„ìš©' in query_noun:
        show_url = COMPANY_BASE_URL + "&wr_id="
      elif 'ê³µì§€ì‚¬í•­' in query_noun:
        show_url = NOTICE_BASE_URL + "&wr_id="
      else:
        show_url = SEMINAR_BASE_URL + "&wr_id="

      # ìµœì¢… data êµ¬ì¡° ìƒì„±
      data = {
        "answer": response,
        "references": show_url,  # show_urlì„ ë„˜ê¸°ê¸°
        "disclaimer": "\n\ní•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "images": ["No content"]
      }
      f_time=time.time()-s_time
      print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
      return data
    top_docs = [list(doc) for doc in top_doc]
    valid_time=time.time()
    if False == (question_valid(question, top_docs[0][1], query_noun)):
        for i in range(len(top_docs)):
            top_docs[i][0] -= 2
    
    final_score = top_docs[0][0]
    final_title = top_docs[0][1]
    final_date = top_docs[0][2]
    final_text = top_docs[0][3]
    final_url = top_docs[0][4]
    final_image = []

    # MongoDB ì—°ê²° í™•ì¸ í›„ ì´ë¯¸ì§€ URL ì¡°íšŒ
    if storage.mongo_collection is not None:
        record = storage.mongo_collection.find_one({"title" : final_title})
        if record :
            if(isinstance(record["image_url"], list)):
              final_image.extend(record["image_url"])
            else :
              final_image.append(record["image_url"])
        else :
            print("ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì¡´ì¬ X")
            final_score = 0
            final_title = "No content"
            final_date = "No content"
            final_text = "No content"
            final_url = "No URL"
            final_image = ["No content"]
    else:
        logger.warning("âš ï¸  MongoDB ì—°ê²° ì—†ìŒ - ì´ë¯¸ì§€ URL ì¡°íšŒ ë¶ˆê°€")
        final_image = ["No content"]
    valid_f_time=time.time()-valid_time
    print(f"ì§ˆë¬¸ ì í•©ë„ ì²´í¬í•˜ëŠ” ì‹œê°„: {valid_f_time}")
    # top_docs ì¸ë±ìŠ¤ êµ¬ì„±
    # 0: ìœ ì‚¬ë„, 1: ì œëª©, 2: ë‚ ì§œ, 3: ë³¸ë¬¸ë‚´ìš©, 4: url, 5: ì´ë¯¸ì§€url

    if final_image[0] != "No content" and final_text == "No content" and final_score > 1.8:
        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        only_image_response = {
            "answer": None,
            "references": final_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return only_image_response

    # ì´ë¯¸ì§€ + LLM ë‹µë³€ì´ ìˆëŠ” ê²½ìš°.
    else:
        chain_time=time.time()
        qa_chain, relevant_docs = get_answer_from_chain(top_docs, question,query_noun)
        chain_f_time=time.time()-chain_time
        print(f"chain ìƒì„±í•˜ëŠ” ì‹œê°„: {chain_f_time}")
        if final_url == PROFESSOR_BASE_URL + "&lang=kor" and any(keyword in query_noun for keyword in ['ì—°ë½ì²˜', 'ì „í™”', 'ë²ˆí˜¸', 'ì „í™”ë²ˆí˜¸']):
            data = {
                "answer": "í•´ë‹¹ êµìˆ˜ë‹˜ì€ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return data
            
        # prof_title=final_title
        # prof_url=["https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_1"]
        # prof_name=""
        # # ì •ê·œì‹ì„ ì´ìš©í•˜ì—¬ ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ ì¶”ì¶œ
        # if any(final_url.startswith(url) for url in prof_url):
        #     match = re.match(r"^[^\dA-Za-z]+", prof_title)
        #     if match:
        #         prof_name = match.group().strip()  # ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     else:
        #         prof_name = prof_title.strip()  # ìˆ«ìê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     prof_name = re.sub(r"\s+", "", prof_name)
        #     user_question = re.sub(r"\s+", "", question)
        #     if prof_name not in user_question:
        #         refer_url=""
        #         if 'ì§ì›' in query_noun:
        #             refer_url=prof_url[1]
        #         else:
        #             refer_url=prof_url[2]
        #         data = {
        #             "answer": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” êµìˆ˜ë‹˜ ì •ë³´ì…ë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
        #             "references": refer_url,
        #             "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        #             "images": ["No content"]
        #         }
        #         return data

        # ê³µì§€ì‚¬í•­ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        notice_url = NOTICE_BASE_URL
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨
        if not qa_chain or not relevant_docs:
            if final_image[0] != "No content" and final_score > 1.8:
                data = {
                    "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "references": final_url,
                    "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "images": final_image
                }
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return data
            else:
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return not_in_notices_response

        # ìœ ì‚¬ë„ê°€ ë‚®ì€ ê²½ìš°
        if final_score < 1.8:
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return not_in_notices_response

        # LLMì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²½ìš°
        answer_time=time.time()
        answer_result = qa_chain.invoke(question)
        answer_f_time=time.time()-answer_time
        print(f"ë‹µë³€ ìƒì„±í•˜ëŠ” ì‹œê°„: {answer_f_time}")
        doc_references = "\n".join([
            f"\nì°¸ê³  ë¬¸ì„œ URL: {doc.metadata['url']}"
            for doc in relevant_docs[:1] if doc.metadata.get('url') != 'No URL'
        ])

        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        data = {
            "answer": answer_result,
            "references": doc_references,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return data