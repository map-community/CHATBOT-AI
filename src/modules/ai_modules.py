import os
import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain_upstage import UpstageEmbeddings
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from pinecone import Pinecone
from langchain_upstage import ChatUpstage
from langchain import hub
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import Document
import re
from datetime import datetime
import pytz
from langchain.schema.runnable import Runnable
from langchain.chains import RetrievalQAWithSourcesChain, RetrievalQA
from langchain.schema.runnable import RunnableSequence, RunnableMap
from langchain_core.runnables import RunnableLambda
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from collections import defaultdict
import numpy as np
from IPython.display import display, HTML
from rank_bm25 import BM25Okapi
from difflib import SequenceMatcher
from pymongo import MongoClient
from pinecone import Index
import redis
import pickle
#ì‹œê°„ ì¸¡ì •ìš©
import time
import logging
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
pinecone_api_key = os.getenv('PINECONE_API_KEY')
index_name = os.getenv('PINECONE_INDEX_NAME', 'info')  # ê¸°ë³¸ê°’ 'info'
upstage_api_key = os.getenv('UPSTAGE_API_KEY')

# API í‚¤ ê²€ì¦
if not pinecone_api_key:
    logger.error("âŒ PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    raise ValueError("PINECONE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

if not upstage_api_key:
    logger.error("âŒ UPSTAGE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    raise ValueError("UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

logger.info("âœ… API í‚¤ë¥¼ .env íŒŒì¼ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

# Mecab import (logger ì •ì˜ ì´í›„)
try:
    from konlpy.tag import Mecab
    MECAB_AVAILABLE = True
    logger.info("âœ… Mecab ì‚¬ìš© ê°€ëŠ¥ (30-50ë°° ë¹ ë¥¸ í˜•íƒœì†Œ ë¶„ì„)")
except Exception as e:
    logger.warning(f"âš ï¸  Mecabì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    logger.warning("âš ï¸  Mecab ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì •í™•ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    MECAB_AVAILABLE = False
    Mecab = None

# Pinecone API ì„¤ì • ë° ì´ˆê¸°í™”
try:
    logger.info("ğŸ”„ Pineconeì— ì—°ê²° ì¤‘...")
    pc = Pinecone(api_key=pinecone_api_key)
    index = pc.Index(index_name)
    logger.info(f"âœ… Pinecone ì¸ë±ìŠ¤ '{index_name}'ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    logger.error(f"âŒ Pinecone ì—°ê²° ì‹¤íŒ¨: {e}")
    raise

def get_korean_time():
    return datetime.now(pytz.timezone('Asia/Seoul'))

# MongoDB ì—°ê²°
try:
    logger.info("ğŸ”„ MongoDBì— ì—°ê²° ì¤‘...")
    mongodb_uri = os.getenv('MONGODB_URI', 'mongodb://localhost:27017/')
    client = MongoClient(mongodb_uri, serverSelectionTimeoutMS=5000)
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    client.admin.command('ping')
    db = client["knu_chatbot"]
    collection = db["notice_collection"]
    logger.info("âœ… MongoDBì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    logger.error(f"âŒ MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
    logger.warning("âš ï¸  MongoDB ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    client = None
    db = None
    collection = None

# Redis ì—°ê²°
try:
    logger.info("ğŸ”„ Redisì— ì—°ê²° ì¤‘...")
    redis_host = os.getenv('REDIS_HOST', 'localhost')
    redis_port = int(os.getenv('REDIS_PORT', 6379))
    redis_client = redis.StrictRedis(host=redis_host, port=redis_port, db=0, socket_connect_timeout=5)
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    redis_client.ping()
    logger.info("âœ… Redisì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    logger.error(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
    logger.warning("âš ï¸  Redis ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. ìºì‹± ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    redis_client = None

# ì „ì—­ ìºì‹œ ë³€ìˆ˜ ì´ˆê¸°í™”
cached_titles = []
cached_texts = []
cached_urls = []
cached_dates = []

# ë‹¨ì–´ ëª…ì‚¬í™” í•¨ìˆ˜.
def transformed_query(content):
    # ì¤‘ë³µëœ ë‹¨ì–´ë¥¼ ì œê±°í•œ ëª…ì‚¬ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    query_nouns = []

    # 1. ìˆ«ìì™€ íŠ¹ì • ë‹¨ì–´ê°€ ê²°í•©ëœ íŒ¨í„´ ì¶”ì¶œ (ì˜ˆ: '2024í•™ë…„ë„', '1ì›”' ë“±)
    pattern = r'\d+(?:í•™ë…„ë„|ë…„|í•™ë…„|ì›”|ì¼|í•™ê¸°|ì‹œ|ë¶„|ì´ˆ|ê¸°|ê°œ|ì°¨)?'
    number_matches = re.findall(pattern, content)
    query_nouns += number_matches
    # ì¶”ì¶œëœ ë‹¨ì–´ë¥¼ contentì—ì„œ ì œê±°
    for match in number_matches:
        content = content.replace(match, '')


    # 1. ì˜ì–´ ë‹¨ì–´ë¥¼ ë‹¨ë…ìœ¼ë¡œ ë˜ëŠ” í•œê¸€ê³¼ ê²°í•©ëœ ê²½ìš° ì¶”ì¶œ (ì˜ì–´ë§Œ ì¶”ì¶œ)
    english_pattern = r'[a-zA-Z]+'
    english_matches = re.findall(english_pattern, content)

    # ëŒ€ë¬¸ìë¡œ ë³€í™˜ í›„ query_nounsì— ì¶”ê°€
    english_matches_upper = [match.upper() for match in english_matches]
    query_nouns += english_matches_upper

    # contentì—ì„œ ì˜ì–´ ë‹¨ì–´ ì œê±°
    for match in english_matches:
        content = re.sub(rf'\b{re.escape(match)}\b', '', content)

    if 'ì‹œê°„í‘œ' in content:
        content=content.replace('ì‹œê°„í‘œ','')
    if 'EXIT' in query_nouns:
        query_nouns.append('ì¶œêµ¬')
    if any(keyword in content for keyword in ['ë²¤ì²˜ì•„ì¹´ë°ë¯¸','ë²¤ì²˜ì•„ì¹´ë°ë¯¸']):
      query_nouns.append("ë²¤ì²˜ì•„ì¹´ë°ë¯¸")
    if 'êµ°' in content:
        query_nouns.append('êµ°')
    if 'ì¸ì»´' in content:
        query_nouns.append('ì¸ê³µì§€ëŠ¥ì»´í“¨íŒ…')
    if 'ì¸ê³µ' in content and 'ì§€ëŠ¥' in content and 'ì»´í“¨íŒ…' in content:
        query_nouns.append('ì¸ê³µì§€ëŠ¥ì»´í“¨íŒ…')
    if 'í•™ë¶€ìƒ' in content:
        query_nouns.append('í•™ë¶€ìƒ')
    ## ì§ì› E9í˜¸ê´€ ìˆëŠ”ê±° ì¶”ê°€í•˜ë ¤ê³ í•¨.
    if 'ê³µëŒ€' in content:
        query_nouns.append('E')
    if 'ì„¤ëª…íšŒ' in content:
        query_nouns.append('ì„¤ëª…íšŒ')
    if 'ì»´í•™' in content:
        query_nouns.append('ì»´í“¨í„°í•™ë¶€')
    if 'ì»´í“¨í„°' in content and 'ë¹„ì „' in content:
        query_nouns.append('ì»´í“¨í„°ë¹„ì „')
        content = content.replace('ì»´í“¨í„° ë¹„ì „', 'ì»´í“¨í„°ë¹„ì „')
        content = content.replace('ì»´í“¨í„°ë¹„ì „', '')
    if 'ì»´í“¨í„°' in content and 'í•™ë¶€' in content:
        query_nouns.append('ì»´í“¨í„°í•™ë¶€')
        content = content.replace('ì»´í“¨í„° í•™ë¶€', 'ì»´í“¨í„°í•™ë¶€')
        content = content.replace('ì»´í“¨í„°í•™ë¶€', '')
    if 'ì„ ë°œ' in content:
        content=content.replace('ì„ ë°œ','')
    if 'ì°¨' in content:
        query_nouns.append('ì°¨')
    if 'êµ­ê°€ ì¥í•™ê¸ˆ' in content:
        query_nouns.append('êµ­ê°€ì¥í•™ê¸ˆ')
        content=content.replace('êµ­ê°€ ì¥í•™ê¸ˆ','')
    if 'ì¢…í”„' in content:
        query_nouns.append('ì¢…í•©ì„¤ê³„í”„ë¡œì íŠ¸')
    if 'ì¢…í•©ì„¤ê³„í”„ë¡œì íŠ¸' in content:
        query_nouns.append('ì¢…í•©ì„¤ê³„í”„ë¡œì íŠ¸')
    if 'ëŒ€íšŒ' in content:
        query_nouns.append('ê²½ì§„ëŒ€íšŒ')
        content=content.replace('ëŒ€íšŒ','')
    if 'íŠœí„°' in content:
        query_nouns.append('TUTOR')
        content = content.replace('íŠœí„°', '')  # 'íŠœí„°' ì œê±°
    if 'íƒ‘ì‹¯' in content:
        query_nouns.append('TOPCIT')
        content=content.replace('íƒ‘ì‹¯','')
    if 'ì‹œí—˜' in content:
        query_nouns.append('ì‹œí—˜')
    if 'í•˜ê³„' in content:
        query_nouns.append('ì—¬ë¦„')
        query_nouns.append('í•˜ê³„')
    if 'ë™ê³„' in content:
        query_nouns.append('ê²¨ìš¸')
        query_nouns.append('ë™ê³„')
    if 'ê²¨ìš¸' in content:
        query_nouns.append('ê²¨ìš¸')
        query_nouns.append('ë™ê³„')
    if 'ì—¬ë¦„' in content:
        query_nouns.append('ì—¬ë¦„')
        query_nouns.append('í•˜ê³„')
    if 'ì„±ì¸ì§€' in content:
        query_nouns.append('ì„±ì¸ì§€')
    if 'ì²¨ì„±ì¸' in content:
        query_nouns.append('ì²¨ì„±ì¸')
    if 'ê¸€ì†¦' in content:
        query_nouns.append('ê¸€ì†')
    if 'ìˆ˜ê¾¸' in content:
        query_nouns.append('ìˆ˜ê°•ê¾¸ëŸ¬ë¯¸')
    if 'ì¥í•™ê¸ˆ' in content:
        query_nouns.append('ì¥í•™ìƒ')
        query_nouns.append('ì¥í•™')
    if 'ì¥í•™ìƒ' in content:
        query_nouns.append('ì¥í•™ê¸ˆ')
        query_nouns.append('ì¥í•™')
    if 'ëŒ€í•´' in content:
        content=content.replace('ëŒ€í•´','')
    if 'ì—ì´ë¹…' in content:
        query_nouns.append('ì—ì´ë¹…')
        query_nouns.append('ABEEK')
        content=content.replace('ì—ì´ë¹…','')
    if 'ì„ ì´ìˆ˜' in content:
        query_nouns.append('ì„ ì´ìˆ˜')
        content=content.replace('ì„ ì´ìˆ˜','')
    if 'ì„ í›„ìˆ˜' in content:
        query_nouns.append('ì„ ì´ìˆ˜')
        content=content.replace('ì„ í›„ìˆ˜','')
    if 'í•™ìê¸ˆ' in content:
        query_nouns.append('í•™ìê¸ˆ')
        content=content.replace('í•™ìê¸ˆ','')
    if  any(keyword in content for keyword in ['ì˜¤í”ˆ ì†ŒìŠ¤','ì˜¤í”ˆì†ŒìŠ¤']):
        query_nouns.append('ì˜¤í”ˆì†ŒìŠ¤')
        content=content.replace('ì˜¤í”ˆ ì†ŒìŠ¤','')
        content=content.replace('ì˜¤í”ˆì†ŒìŠ¤','')
    if any(keyword in content for keyword in ['êµ°','êµ°ëŒ€']) and 'íœ´í•™' in content:
        query_nouns.append('êµ°')
        query_nouns.append('êµ°íœ´í•™')
        query_nouns.append('êµ°ì…ëŒ€')
    if 'ì¹´í…Œìº ' in content:
        query_nouns.append('ì¹´ì¹´ì˜¤')
        query_nouns.append('í…Œí¬')
        query_nouns.append('ìº í¼ìŠ¤')
    re_keyword = ['ì¬ì´ìˆ˜', 'ì¬ ì´ìˆ˜', 'ì¬ ìˆ˜ê°•', 'ì¬ìˆ˜ê°•']
    # ê° í‚¤ì›Œë“œë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì¹˜í™˜
    if any(key in content for key in re_keyword):
      for keyword in re_keyword:
        query_nouns.append('ì¬ì´ìˆ˜')
        content = content.replace(keyword, '')
    if 'ê³¼ëª©' in content:
        query_nouns.append('ê°•ì˜')
    if 'ê°•ì˜' in content:
        query_nouns.append('ê³¼ëª©')
        query_nouns.append('ê°•ì¢Œ')
    if 'ê°•ì¢Œ' in content:
        query_nouns.append('ê°•ì¢Œ')
        contnet=content.replace('ê°•ì¢Œ','')
    if 'ì™¸êµ­ì–´' in content:
        query_nouns.append('ì™¸êµ­ì–´') 
        contnet=content.replace('ì™¸êµ­ì–´','')
    if 'ë¶€' in content and 'ì „ê³µ' in content:
        query_nouns.append('ë¶€ì „ê³µ') 
    if 'ìˆ˜ê¾¸' in content:
        query_nouns.append('ìˆ˜ê°•ê¾¸ëŸ¬ë¯¸')
    if 'ê³„ì ˆ' in content and 'í•™ê¸°' in content:
        query_nouns.append('ìˆ˜ì—…')
    if 'ì±„ìš©' in content and any(keyword in content for keyword in ['ëª¨ì§‘','ê³µê³ ']):
        if 'ëª¨ì§‘' in content:
          content=content.replace('ëª¨ì§‘','')
        if 'ê³µê³ ' in content:
          content=content.replace('ê³µê³ ','')
    # ë¹„ìŠ·í•œ ì˜ë¯¸ ëª¨ë‘ ì¶”ê°€ (ì„¸ë¯¸ë‚˜)
    related_keywords = ['ì„¸ë¯¸ë‚˜','íŠ¹ê°•', 'ê°•ì—°']
    if any(keyword in content for keyword in related_keywords):
        for keyword in related_keywords:
            query_nouns.append(keyword)
    # "ê³µì§€", "ì‚¬í•­", "ê³µì§€ì‚¬í•­"ì„ query_nounsì—ì„œ 'ê³µì§€ì‚¬í•­'ì´ë¼ê³  ê³ ì •í•˜ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ ì‚­ì œ
    keywords=['ê³µì§€','ì‚¬í•­','ê³µì§€ì‚¬í•­']
    if any(keyword in content for keyword in keywords):
      # í‚¤ì›Œë“œ ì œê±°
      for keyword in keywords:
          content = content.replace(keyword, '')
          query_nouns.append('ê³µì§€ì‚¬í•­')

    keywords=['ì‚¬ì›','ì‹ ì…ì‚¬ì›']
    if any(keyword in content for keyword in keywords):
        for keyword in keywords:
          content = content.replace(keyword, '')
          query_nouns.append('ì‹ ì…')
    # 5. Mecab í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì´ìš©í•œ ì¶”ê°€ ëª…ì‚¬ ì¶”ì¶œ
    if MECAB_AVAILABLE:
        mecab = Mecab()
        additional_nouns = [noun for noun in mecab.nouns(content) if len(noun) > 1]
        query_nouns += additional_nouns
    else:
        # Mecab ì—†ì´ ê°„ë‹¨í•œ í† í°í™” (ì •í™•ë„ëŠ” ë‚®ì§€ë§Œ ì‘ë™í•¨)
        logger.debug("âš ï¸  Mecab ì—†ì´ ê°„ë‹¨í•œ í† í°í™” ì‚¬ìš©")
        simple_tokens = content.split()
        additional_nouns = [token for token in simple_tokens if len(token) > 1]
        query_nouns += additional_nouns
    if 'ì¸ë„' not in query_nouns and  'ì¸í„´ì‹­' in query_nouns:
        query_nouns.append('ë² íŠ¸ë‚¨')

    # 6. "ìˆ˜ê°•" ë‹¨ì–´ì™€ ê´€ë ¨ëœ í‚¤ì›Œë“œ ê²°í•© ì¶”ê°€
    if 'ìˆ˜ê°•' in content:
        related_keywords = ['ë³€ê²½', 'ì‹ ì²­', 'ì •ì •', 'ì·¨ì†Œ','ê¾¸ëŸ¬ë¯¸']
        for keyword in related_keywords:
            if keyword in content:
                # 'ìˆ˜ê°•'ê³¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ í‚¤ì›Œë“œ ì¶”ê°€
                combined_keyword = 'ìˆ˜ê°•' + keyword
                query_nouns.append(combined_keyword)
                if ('ìˆ˜ê°•' in query_nouns):
                  query_nouns.remove('ìˆ˜ê°•')
                for keyword in related_keywords:
                  if keyword in query_nouns:
                    query_nouns.remove(keyword)
    # ìµœì¢… ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ë‹¨ì–´ ì œê±°
    if 'ê¾¸ëŸ¬ë¯¸' in content and 'ìˆ˜ê°•ì‹ ì²­' in query_nouns:
      query_nouns.append('ì‹ ì²­')

    query_nouns = list(set(query_nouns))
    return query_nouns
###################################################################################################


# Dense Retrieval (Upstage ì„ë² ë”©)
embeddings = UpstageEmbeddings(
  api_key=upstage_api_key,
  model="solar-embedding-1-large-query"  # ì§ˆë¬¸ ì„ë² ë”©ìš© ëª¨ë¸
) # Upstage API í‚¤ ì‚¬ìš©
# dense_doc_vectors = np.array(embeddings.embed_documents(texts))  # ë¬¸ì„œ ì„ë² ë”©


def fetch_titles_from_pinecone():
    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ì„ì˜ ì¿¼ë¦¬
    query_results = index.query(
        vector=[0] * 4096,  # Pineconeì—ì„œ ì‚¬ìš© ì¤‘ì¸ ë²¡í„° í¬ê¸°ì— ë§ê²Œ 0ìœ¼ë¡œ ì±„ìš´ ë²¡í„°
        top_k=1400,         # ìµœëŒ€ 1400ê°œì˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        include_metadata=True  # ë©”íƒ€ë°ì´í„° í¬í•¨
    )

    # ë©”íƒ€ë°ì´í„°ì—ì„œ í•„ìš”í•œ ê°’ë“¤ ì¶”ì¶œ.
    titles = [match["metadata"]["title"] for match in query_results["matches"]]
    texts = [match["metadata"]["text"] for match in query_results["matches"]]
    urls = [match["metadata"]["url"] for match in query_results["matches"]]
    dates = [match["metadata"]["date"] for match in query_results["matches"]]

    return titles, texts, urls, dates


# ìºì‹± ë°ì´í„° ì´ˆê¸°í™” í•¨ìˆ˜

def initialize_cache():
    global cached_titles, cached_texts, cached_urls, cached_dates

    try:
        logger.info("ğŸ”„ ìºì‹œ ì´ˆê¸°í™” ì‹œì‘...")

        # Pineconeì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        cached_titles, cached_texts, cached_urls, cached_dates = fetch_titles_from_pinecone()
        logger.info(f"âœ… Pineconeì—ì„œ {len(cached_titles)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

        # Redisì— ì €ì¥ ì‹œë„
        if redis_client is not None:
            try:
                redis_client.set('pinecone_metadata', pickle.dumps((cached_titles, cached_texts, cached_urls, cached_dates)))
                logger.info("âœ… Redisì— ìºì‹œ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"âš ï¸  Redis ì €ì¥ ì‹¤íŒ¨ (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©): {e}")
        else:
            logger.warning("âš ï¸  Redis ë¯¸ì‚¬ìš© (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©)")

        logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(cached_titles)}, texts: {len(cached_texts)})")

    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì•±ì´ í¬ë˜ì‹œí•˜ì§€ ì•Šë„ë¡ í•¨
        cached_titles = []
        cached_texts = []
        cached_urls = []
        cached_dates = []
        logger.warning("âš ï¸  ìºì‹œë¥¼ ë¹ˆ ìƒíƒœë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

                    #################################   24.11.16ê¸°ì¤€ ì •í™•ë„ ì¸¡ì •ì™„ë£Œ #####################################################
######################################################################################################################

# ë‚ ì§œë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜

def parse_date_change_korea_time(date_str):
    clean_date_str = date_str.replace("ì‘ì„±ì¼", "").strip()
    naive_date = datetime.strptime(clean_date_str, "%y-%m-%d %H:%M")
    # í•œêµ­ ì‹œê°„ëŒ€ ì¶”ê°€
    korea_timezone = pytz.timezone('Asia/Seoul')
    return korea_timezone.localize(naive_date)


def calculate_weight_by_days_difference(post_date, current_date, query_nouns):

    # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
    days_diff = (current_date - post_date).days

    # ê¸°ì¤€ ë‚ ì§œ (24-01-01 00:00) ì„¤ì •
    baseline_date_str = "24-01-01 00:00"
    baseline_date = parse_date_change_korea_time(baseline_date_str)
    graduate_weight = 1.0 if any(keyword in query_nouns for keyword in ['ì¡¸ì—…', 'ì¸í„°ë·°']) else 0
    scholar_weight = 1.0 if 'ì¥í•™' in query_nouns else 0
    # ì‘ì„±ì¼ì´ ê¸°ì¤€ ë‚ ì§œ ì´ì „ì´ë©´ ê°€ì¤‘ì¹˜ë¥¼ 1.35ë¡œ ê³ ì •
    if post_date <= baseline_date:
        return 1.35 + graduate_weight / 5

    # 'ìµœê·¼', 'ìµœì‹ ' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°, ìµœê·¼ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€
    add_recent_weight = 1.5 if any(keyword in query_nouns for keyword in ['ìµœê·¼', 'ìµœì‹ ', 'ì§€ê¸ˆ', 'í˜„ì¬']) else 0

    # **10ì¼ ë‹¨ìœ„ êµ¬ë¶„**: ìµœê·¼ ë¬¸ì„œì— ëŒ€í•œ ì„¸ë°€í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    if days_diff <= 6:
        return 1.355 + add_recent_weight + graduate_weight + scholar_weight
    elif days_diff <= 12:
        return 1.330 + add_recent_weight / 3.0 + graduate_weight / 1.2 + scholar_weight / 1.5
    elif days_diff <= 18:
        return 1.321 + add_recent_weight / 5.0 + graduate_weight / 1.3 + scholar_weight / 2.0
    elif days_diff <= 24:
        return 1.310 + add_recent_weight / 7.0 + graduate_weight / 1.4 + scholar_weight / 2.5
    elif days_diff <= 30:
        return 1.290 + add_recent_weight / 9.0 + graduate_weight / 1.5 + scholar_weight / 3.0
    elif days_diff <= 36:
        return 1.270 + graduate_weight / 1.6 + scholar_weight / 3.5
    elif days_diff <= 45:
        return 1.250 +graduate_weight / 1.7 + scholar_weight / 4.0
    elif days_diff <= 60:
        return 1.230 +graduate_weight / 1.8 + scholar_weight / 4.5
    elif days_diff <= 90:
        return 1.210 +graduate_weight / 2.0 + scholar_weight / 5.0

    # **ì›” ë‹¨ìœ„ êµ¬ë¶„**: 2ê°œì›” ì´í›„ëŠ” ì›” ë‹¨ìœ„ë¡œ ë‹¨ìˆœí™”
    month_diff = (days_diff - 90) // 30
    month_weight_map = {
        0: 1.19,
        1: 1.17 - add_recent_weight / 6 - scholar_weight / 10,
        2: 1.15 - add_recent_weight / 5 - scholar_weight / 9,
        3: 1.13 - add_recent_weight / 4 - scholar_weight / 7,
        4: 1.11 - add_recent_weight / 3  - scholar_weight / 5,
    }

    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë°˜í™˜ (6ê°œì›” ì´í›„)
    return month_weight_map.get(month_diff, 0.88 - add_recent_weight /2  - scholar_weight / 5)


# ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜

def adjust_date_similarity(similarity, date_str,query_nouns):
    # í˜„ì¬ í•œêµ­ ì‹œê°„
    current_time = get_korean_time()
    # ì‘ì„±ì¼ íŒŒì‹±
    post_date = parse_date_change_korea_time(date_str)
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    weight = calculate_weight_by_days_difference(post_date, current_time,query_nouns)
    # ì¡°ì •ëœ ìœ ì‚¬ë„ ë°˜í™˜
    return similarity * weight

# ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ ëª…ì‚¬ì™€ ê° ë¬¸ì„œ ì œëª©ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜
'''
def adjust_similarity_scores(query_noun, title,texts,similarities):

    for idx, titl in enumerate(title):
        # ì œëª©ì— í¬í•¨ëœ query_noun ìš”ì†Œì˜ ê°œìˆ˜ë¥¼ ì„¼ë‹¤

        matching_noun = [noun for noun in query_noun if noun in titl]
        if texts[idx] == "No content":
            if "êµ­ê°€ì¥í•™ê¸ˆ" in titl and "êµ­ê°€ì¥í•™ê¸ˆ" in query_noun:
              similarities[idx]*=5.0
            else:
              similarities[idx] *=1.5 # ë³¸ë¬¸ì´ "No content"ì¸ ê²½ìš° ìœ ì‚¬ë„ë¥¼ ë†’ì„
        for noun in matching_noun:
            similarities[idx] += len(noun)*0.21
            if re.search(r'\d', noun):  # ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ì–´ í™•ì¸
                if noun in title:  # ë³¸ë¬¸ì—ë„ ìˆ«ì í¬í•¨ ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì¡°ì •
                    similarities[idx] += len(noun)*0.22
                else:
                    similarities[idx]+=len(noun)*0.19
        # query_nounì— "ëŒ€í•™ì›"ì´ ì—†ê³  ì œëª©ì— "ëŒ€í•™ì›"ì´ í¬í•¨ëœ ê²½ìš° ìœ ì‚¬ë„ë¥¼ 0.1 ê°ì†Œ
        keywords = ['ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ']
        # ì¡°ê±´ 1: ë‘˜ ë‹¤ í‚¤ì›Œë“œ í¬í•¨
        if any(keyword in query_noun for keyword in keywords) and any(keyword in titl for keyword in keywords):
            similarities[idx] += 2.0
        # ì¡°ê±´ 2: query_nounì— ì—†ê³ , titleì—ë§Œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
        if not any(keyword in query_noun for keyword in keywords) and any(keyword in titl for keyword in keywords):
            similarities[idx] -= 2.0
        if not any(keyword in query_noun for keyword in["í˜„ì¥", "ì‹¤ìŠµ", "í˜„ì¥ì‹¤ìŠµ"]) and any(keyword in titl for keyword in ["í˜„ì¥ì‹¤ìŠµ","ëŒ€ì²´","ê¸°ì¤€"]):
            similarities[idx]-=2
        if 'ì™¸êµ­ì–´' in query_noun and 'ê°•ì¢Œ' in query_noun and 'ì‹ ì²­' in titl:
            similarities[idx]-=1.0
        if "ì™¸êµ­ì¸" not in query_noun and "ì™¸êµ­ì¸" in titl:
            similarities[idx]-=2.0
        if texts[idx] == "No content":
            similarities[idx] *=1.45# ë³¸ë¬¸ì´ "No content"ì¸ ê²½ìš° ìœ ì‚¬ë„ë¥¼ ë†’ì„
        if 'ë§ˆì¼ë¦¬ì§€' in query_noun and 'ë§ˆì¼ë¦¬ì§€' in texts[idx]:
            similarities[idx]+=2
        if 'ì¸ì»´' in query_noun and any(keyword in titl for keyword in ['ì¸ì»´','ì¸ê³µì§€ëŠ¥ì»´í“¨íŒ…']):
          similarities[idx]+=3
        if 'ì‹ ì…ìƒ' in query_noun and 'ìˆ˜ê°•ì‹ ì²­' in query_noun and 'ì‹ ì…ìƒ' in titl and 'ìˆ˜ê°•ì‹ ì²­' in titl:
          similarities[idx]+=1.5
    return similarities
'''

def adjust_similarity_scores(query_noun, title, texts, similarities):
    query_noun_set = set(query_noun)
    title_tokens = [set(titl.split()) for titl in title]

    for idx, titl_tokens in enumerate(title_tokens):
        matching_noun = query_noun_set.intersection(titl_tokens)
        
        if texts[idx] == "No content":
            similarities[idx] *= 1.5
            if "êµ­ê°€ì¥í•™ê¸ˆ" in query_noun_set and "êµ­ê°€ì¥í•™ê¸ˆ" in titl_tokens:
                similarities[idx] *= 5.0
        
        for noun in matching_noun:
            len_adjustment = len(noun) * 0.21
            similarities[idx] += len_adjustment
            if re.search(r'\d', noun):  # ìˆ«ì í¬í•¨ ì—¬ë¶€
                similarities[idx] += len(noun) * (0.22 if noun in titl_tokens else 0.19)

        if query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and titl_tokens.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}):
            similarities[idx] += 2.0
        if not query_noun_set.intersection({'ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ'}) and 'ëŒ€í•™ì›' in titl_tokens:
            similarities[idx] -= 2.0

    return similarities


#############################################################################################

def last_filter_keyword(DOCS,query_noun,user_question):
        # í•„í„°ë§ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        Final_best=DOCS
        # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ê³ , ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        for idx, doc in enumerate(DOCS):
            score, title, date, text, url = doc
            if not any(keyword in query_noun for keyword in["í˜„ì¥", "ì‹¤ìŠµ", "í˜„ì¥ì‹¤ìŠµ"]) and any(keyword in title for keyword in ["í˜„ì¥ì‹¤ìŠµ","ëŒ€ì²´","ê¸°ì¤€"]):
              score-=1.0
            # wr_id ë’¤ì— ì˜¤ëŠ” ìˆ«ì ì¶”ì¶œ
            target_numbers = [27510, 27047, 27614, 27246, 25900, 27553, 25896, 28183,27807,25817,25804]

            match = re.search(r"wr_id=(\d+)", url)
            if match:
                extracted_number = int(match.group(1))
                # ìˆ«ìê°€ target_numbersì— í¬í•¨ë˜ë©´ score ì¦ê°€
                if extracted_number in target_numbers:
                    if any(keyword in query_noun for keyword in ['ì—ì´ë¹…','ABEEK']) and any(keyword in text for keyword in ['ì—ì´ë¹…','ABEEK']):
                        if extracted_number==27047:
                           score+=0.3
                        else:
                           score+=1.5
                    else:
                        if 'íê°•' not in query_noun:
                          score+=0.8
                        if 'ê³„ì ˆ' in query_noun:
                            score-=2.0
                        if 'ì „ê³¼' in query_noun:
                          score-=1.0
                        if 'ìœ ì˜ˆ' in query_noun and 'í•™ì‚¬' in query_noun and extracted_number==28183:
                          score+=0.45
            if 'ê¸°ë…' in query_noun and 'ê¸°ë…' in title and url=="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4&wr_id=354":
              score+=0.5
            if 'ìŠ¤íƒ¬í”„' not in query_noun and 'ìŠ¤íƒ¬í”„' in title:
              score-=0.5
            if 'ê¸°ë§' in query_noun and 'ê¸°ë§' in title:
                score+=1.0
            if 'ì¤‘ê°„' in query_noun and 'ì¤‘ê°„' in title:
                score+=1.0
            if 'ì¡¸ì—…' in query_noun and 'ì¡¸ì—…' not in title and 'í¬íŠ¸í´ë¦¬ì˜¤' in query_noun and 'í¬íŠ¸í´ë¦¬ì˜¤' in title:
              score-=1.0
            if 'ì¡¸ì—…' in query_noun and 'í¬íŠ¸í´ë¦¬ì˜¤' in title and 'ì¡¸ì—…' in title and 'í¬íŠ¸í´ë¦¬ì˜¤' in query_noun:
              score+=1.0
            if 'TUTOR' in title and 'TUTOR' not in query_noun:
                score-=1.0
            class_word = ['ì‹ ì²­', 'ì·¨ì†Œ', 'ë³€ê²½']
            for keyword in class_word:
              if keyword in query_noun and 'ê³„ì ˆ' in query_noun and keyword in title:
                  score += 1.3
                  break
            if 'ìí‡´' in title and 'ìí‡´' in query_noun:
                score+=1.0
            if 'ì „ê³¼' in title and 'ì „ê³¼' in query_noun:
              score+=1.0
            if 'ì¡°ê¸°' in title and 'ì¡°ê¸°' not in query_noun:
              score-=0.5    
            if 'ìˆ˜ê°•' in title:
              if url=="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1&wr_id=28180":
                score-=3.0
              if any(keyword in query_noun for keyword in ['íê°•','ì¬ì´ìˆ˜']):
                if 'íê°•' in query_noun and any(keyword in title for keyword in ['ì‹ ì²­', 'ì •ì •']):
                  score+=2.0
                else:
                  score+=0.8  
                if 'ì¬ì´ìˆ˜' in query_noun:
                  if 'ê¾¸ëŸ¬ë¯¸' in title:
                    score+=1.0
                  elif 'ì‹ ì²­' in title:
                    score+=2.0
                  else:
                    score+=1.5
            if 'ì„¤ë¬¸' not in query_noun and 'ì„¤ë¬¸' in title:
                score-=0.5
            if any(keyword in query_noun for keyword in ['êµ°','êµ°ëŒ€']) and 'êµ°' in title:
              if 'í•™ì ' in title and 'í•™ì ' not in query_noun:
                score-=1.0
              else:
                score+=1.5
            if 'êµ°' not in query_noun and 'êµ°' in title:
              score-=1.0
            if 'ë³µí•™' in query_noun and 'ë³µí•™' in title:
                score+=1.0
            if 'íœ´í•™' in query_noun and 'íœ´í•™' in title:
                score+=1.0
            if 'ì¹´ì¹´ì˜¤' in title and 'ì¹´ì¹´ì˜¤' in query_noun:
                score+=0.6
            if 'ì„¤ê³„' in title:
                score-=0.4
            if 'ì˜¤í”ˆì†ŒìŠ¤' in query_noun and 'ì˜¤í”ˆì†ŒìŠ¤' in title:
                score+=0.5
            if 'SDG' in query_noun and 'SDG' in title:
                score+=2.9
            if any(keyword in query_noun for keyword in ['ì¸í„´','ì¸í„´ì‹­'])  and any(keyword in query_noun for keyword in ['ì¸ë„','ë² íŠ¸ë‚¨']):
                score+=1.0
            if any(keyword in title for keyword in ['ìˆ˜ìš”','ì¡°ì‚¬']) and not any(keyword in query_noun for keyword in ['ìˆ˜ìš”','ì¡°ì‚¬']):
                score-=0.6
            if 'ì—¬ë¦„' in query_noun and any(keyword in title for keyword in['ê²¨ìš¸',"ë™ê³„"]):
                score-=1.0
            if 'ê²¨ìš¸' in query_noun and any(keyword in title for keyword in['í•˜ê³„',"ì—¬ë¦„"]):
                score-=1.0
            if 'ì—¬ë¦„' in query_noun and any(keyword in title for keyword in['í•˜ê³„',"ì—¬ë¦„"]):
                score+=0.7
                if 'ë²¤ì²˜ì•„ì¹´ë°ë¯¸' in query_noun:
                  score+=2.0
            if 'ê²¨ìš¸' in query_noun and any(keyword in title for keyword in['ê²¨ìš¸',"ë™ê³„"]):
                score+=0.7
                if 'ë²¤ì²˜ì•„ì¹´ë°ë¯¸' in query_noun:
                  score+=2.0
 
            if '1í•™ê¸°' in query_noun and '1í•™ê¸°' in title:
                score+=1.0
            if '2í•™ê¸°' in query_noun and '2í•™ê¸°' in title:
                score+=1.0
            if '1í•™ê¸°' in query_noun and '2í•™ê¸°' in title:
                score-=1.0
            if '2í•™ê¸°' in query_noun and '1í•™ê¸°' in title:
                score-=1.0
            if any(keyword in text for keyword in ['ì¢…í”„','ì¢…í•©ì„¤ê³„í”„ë¡œì íŠ¸']) and any(keyword in user_question for keyword in ['ì¢…í”„','ì¢…í•©ì„¤ê³„í”„ë¡œì íŠ¸']):
                score+=0.7
                if 'ì„¤ëª…íšŒ' in query_noun and 'ì„¤ëª…íšŒ' in title:
                  score+=0.7
                else:
                  score-=1.0
            if 'ë¶€ì „ê³µ' in query_noun and 'ë¶€ì „ê³µ' in title:
                score+=1.0
            if any(keyword in query_noun for keyword in ['ë³µì „','ë³µìˆ˜','ë³µìˆ˜ì „ê³µ']) and  any(keyword in title for keyword in ['ë³µìˆ˜']):
                score+=0.7
            if not any(keyword in query_noun for keyword in ['ë³µì „','ë³µìˆ˜','ë³µìˆ˜ì „ê³µ']) and any(keyword in title for keyword in ['ë³µìˆ˜']):
                score-=1.4
            if any(keyword in title for keyword in ['ì‹¬ì»´','ì‹¬í™”ì»´í“¨í„°ì „ê³µ','ì‹¬í™” ì»´í“¨í„°ê³µí•™','ì‹¬í™”ì»´í“¨í„°ê³µí•™']):
              if any(keyword in user_question for keyword in['ì‹¬ì»´','ì‹¬í™”ì»´í“¨í„°ì „ê³µ']):
                score+=0.7
              else:
                if not "ì»´í“¨í„°ë¹„ì „" in query_noun:
                  score-=0.7
            elif any(keyword in title for keyword in ['ê¸€ë¡œë²Œì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ','ê¸€ë¡œë²ŒSWì „ê³µ','ê¸€ë¡œë²Œì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ì „ê³µ','ê¸€ì†','ê¸€ì†¦']):
              if any(keyword in user_question for keyword in ['ê¸€ë¡œë²Œì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ì „ê³µ','ê¸€ë¡œë²Œì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ','ê¸€ë¡œë²ŒSWì „ê³µ','ê¸€ì†','ê¸€ì†¦']):
                score+=0.7
              else:
                score-=0.8
            elif any(keyword in title for keyword in['ì¸ì»´','ì¸ê³µì§€ëŠ¥ì»´í“¨íŒ…']):
              if any(keyword in user_question for keyword in ['ì¸ì»´','ì¸ê³µì§€ëŠ¥ì»´í“¨íŒ…']):
                score+=0.7
                if url=="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1&wr_id=27553":
                  score+=1.0
              else:
                score-=0.8
            if any(keyword in user_question for keyword in ['ë²¤ì²˜','ì•„ì¹´ë°ë¯¸']) and any(keyword in title for keyword in ['ë²¤ì²˜ì•„ì¹´ë°ë¯¸','ë²¤ì²˜ìŠ¤íƒ€íŠ¸ì—…ì•„ì¹´ë°ë¯¸','ë²¤ì²˜ìŠ¤íƒ€íŠ¸ì—…']):
                if any(keyword in user_question for keyword in ['ìŠ¤íƒ€íŠ¸ì—…']) and any(keyword in title for keyword in ['ìŠ¤íƒ€íŠ¸ì—…']):
                  score+=0.5
                elif not any(keyword in user_question for keyword in ['ìŠ¤íƒ€íŠ¸ì—…']) and any(keyword in title for keyword in ['ë²¤ì²˜ìŠ¤íƒ€íŠ¸ì—…ì•„ì¹´ë°ë¯¸','ë²¤ì²˜ìŠ¤íƒ€íŠ¸ì—…ì•„ì¹´ë°ë¯¸','ìŠ¤íƒ€íŠ¸ì—…','ìŠ¤íƒ€íŠ¸','ë²¤ì²˜ìŠ¤íƒ€íŠ¸ì—…']):
                  score-=2.5
                else:
                  score+=2.0
            if any(keyword in text for keyword in ['ê³„ì•½í•™ê³¼', 'ëŒ€í•™ì›', 'íƒ€ëŒ€í•™ì›']) and not any(keyword in query_noun for keyword in ['ê³„ì•½í•™ê³¼', 'ëŒ€í•™ì›', 'íƒ€ëŒ€í•™ì›']):
                score -= 0.8  # ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ 0.4 ë‚®ì¶”ê¸°
            keywords = ['ëŒ€í•™ì›', 'ëŒ€í•™ì›ìƒ']

            # ì¡°ê±´ 1: ë‘˜ ë‹¤ í‚¤ì›Œë“œ í¬í•¨
            if any(keyword in query_noun for keyword in keywords) and any(keyword in title for keyword in keywords):
                score += 2.0
            # ì¡°ê±´ 2: query_nounì— ì—†ê³ , titleì—ë§Œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
            elif not any(keyword in query_noun for keyword in keywords) and any(keyword in title for keyword in keywords):
                if 'í•™ë¶€ìƒ' in query_noun and 'ì—°êµ¬' in query_noun:
                  score+=1.0
                else:
                  score -= 2.0
            if any(keyword in query_noun for keyword in ['ëŒ€í•™ì›','ëŒ€í•™ì›ìƒ']) and any (keyword in title for keyword in ['ëŒ€í•™ì›','ëŒ€í•™ì›ìƒ']):
                score+=2.0

            if any(keyword in user_question for keyword in ['ë‹´ë‹¹','ì—…ë¬´','ì¼','ê·¼ë¬´','ê´€ë ¨']) and any(keyword in query_noun for keyword in ['ì§ì›','ì„ ìƒ','ì„ ìƒë‹˜']):
                if url!= "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5&lang=kor":
                    score-=3.0
                else:
                    score+=1.0
                    # ITì™€ E ëª¨ë‘ ì²˜ë¦¬
                    for keyword in ['IT', 'E']:
                        if keyword in query_noun:
                            # 'IT'ì˜ ê²½ìš° ìˆ«ì 4, 5 / 'E'ì˜ ê²½ìš° ìˆ«ì 9 í™•ì¸
                            valid_numbers = ['4', '5'] if keyword == 'IT' else ['9']
                            building_number = [num for num in query_noun if num in valid_numbers]
                            if building_number:
                                # IT4, IT5, E9 í˜•ì‹ìœ¼ë¡œ ê²°í•©
                                combined_building = f"{keyword}{building_number[0]}"
                                # í…ìŠ¤íŠ¸ì— í•´ë‹¹ ê±´ë¬¼ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
                                if combined_building in text:
                                    score += 0.5  # ì •í™•íˆ ë§¤ì¹­ëœ ê²½ìš° ê°€ì¤‘ì¹˜ ë¶€ì—¬
                                else:
                                    score -= 0.8  # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ íŒ¨ë„í‹°
                    if 'ëŒ€í•™ì›' in query_noun:
                      if not any(keyword in query_noun for keyword in ['ì§€ì›','ê³„ì•½']) and any(keyword in text for keyword in ['ì§€ì›','ê³„ì•½']):
                        score-=0.8
                      else:
                        score+=0.5



            if (any(keyword in query_noun for keyword in ['ë‹´ë‹¹','ì—…ë¬´','ì¼','ê·¼ë¬´']) or any(keyword in query_noun for keyword in ['ì§ì›','êµìˆ˜','ì„ ìƒ','ì„ ìƒë‹˜'])) and date=="ì‘ì„±ì¼24-01-01 00:00":
                ### ì¢…í”„ íŒ€ê³¼ì œ ë‹´ë‹¹ êµìˆ˜ ëˆ„êµ¬ì•¼ì™€ ê°™ì€ ì§ˆë¬¸ì¸ë° ì—‰ëš±í•˜ê²Œ íŒŒì¸ì½˜ì—ì„œ ì§ì›ì´ ìœ ì‚¬ë„ ë†’ê²Œ ì¸¡ì •ëœ ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•¨.
                if (any(keys in query_noun for keys in ['êµìˆ˜'])):
                  check=0
                  compare_url="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5&lang=kor" ## ì§ì›ì— í•´ë‹¹í•˜ëŠ” URLì„.
                  if compare_url==url:
                    check=1
                  if check==0:
                    score+=0.5
                  else:
                    score-=0.9 ###ì§ì›ì´ë‹ˆê¹Œ ìœ ì‚¬ë„ ë‚˜ê°€ë¼..
                else:
                  score+=4.0

            if not any(keys in query_noun for keys in['êµìˆ˜']) and any(keys in title for keys in ['ë‹´ë‹¹êµìˆ˜','êµìˆ˜']):
              score-=0.7

            match = re.search(r"(?<![\[\(])\bìˆ˜ê°•\w*\b(?![\]\)])", title)
            if match:
                full_keyword = match.group(0)
                # query_nounsì— í¬í•¨ ì—¬ë¶€ í™•ì¸
                if full_keyword not in query_noun:
                  match = re.search(r"wr_id=(\d+)", url)
                  if match:
                      extracted_number = int(match.group(1))
                      if extracted_number in target_numbers:
                          score-=0.2
                      else:
                          score-=0.7
                else:
                  score+=0.8
            # ì¡°ì •ëœ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            Final_best[idx] = (score, title, date, text,  url)
            #print(Final_best[idx])
        return Final_best

#################################################################################################

def find_url(url, title, doc_date, text, doc_url, number):
    return_docs = []
    for i, urls in enumerate(doc_url):
        if urls.startswith(url):  # indexsì™€ ì‹œì‘ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            return_docs.append((title[i], doc_date[i], text[i], doc_url[i]))
    
    # doc_url[i] ìˆœì„œëŒ€ë¡œ ì •ë ¬
    return_docs.sort(key=lambda x: x[3],reverse=True) 

    # ê³ ìœ  ìˆ«ìë¥¼ ì¶”ì í•˜ë©° numberê°œì˜ ë¬¸ì„œ ì„ íƒ
    unique_numbers = set()
    filtered_docs = []

    for doc in return_docs:
        # ìˆ«ìê°€ ì„œë¡œ ë‹¤ë¥¸ numberê°œê°€ ëª¨ì´ë©´ ì¢…ë£Œ
        if len(unique_numbers) >= number:
            break
        url_number = ''.join(filter(str.isdigit, doc[3]))  # URLì—ì„œ ìˆ«ì ì¶”ì¶œ
        unique_numbers.add(url_number)
        filtered_docs.append(doc)


    return filtered_docs


########################################################################################  best_docs ì‹œì‘ ##########################################################################################


def best_docs(user_question):
      # ì‚¬ìš©ì ì§ˆë¬¸
      noun_time=time.time()
      query_noun=transformed_query(user_question)
      query_noun_time=time.time()-noun_time
      print(f"ëª…ì‚¬í™” ë³€í™˜ ì‹œê°„ : {query_noun_time}")
      titles_from_pinecone, texts_from_pinecone, urls_from_pinecone, dates_from_pinecone = cached_titles, cached_texts, cached_urls, cached_dates
      if not query_noun:
        return None,None
      #######  ìµœê·¼ ê³µì§€ì‚¬í•­, ì±„ìš©, ì„¸ë¯¸ë‚˜, í–‰ì‚¬, íŠ¹ê°•ì˜ ë‹¨ìˆœí•œ ì •ë³´ë¥¼ ìš”êµ¬í•˜ëŠ” ê²½ìš°ë¥¼ í•„í„°ë§ í•˜ê¸° ìœ„í•œ ë§¤ì»¤ë‹ˆì¦˜ ########
      remove_noticement = ['ëª©ë¡','ë¦¬ìŠ¤íŠ¸','ë‚´ìš©','ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì—…ë°ì´íŠ¸',
                           'ì»´í“¨í„°í•™ë¶€','ì»´í•™','ìƒìœ„','ì •ë³´','ê´€ë ¨','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ ','ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬']
      query_nouns = [noun for noun in query_noun if noun not in remove_noticement]
      return_docs=[]
      key=None
      numbers=5 ## ê¸°ë³¸ìœ¼ë¡œ 5ê°œ ë¬¸ì„œ ë°˜í™˜í•  ê²ƒ.
      check_num=0
      recent_time=time.time()
      for noun in query_nouns:
        if 'ê°œ' in noun:
            # ìˆ«ì ì¶”ì¶œ
            num = re.findall(r'\d+', noun)
            if num:
                numbers=int(num[0])
                check_num=1
      if (any(keyword in query_noun for keyword in ['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©','ê³µê³ '])and any(keyword in query_noun for keyword in ['ìµœê·¼','ìµœì‹ ','ì§€ê¸ˆ','í˜„ì¬'])and len(query_nouns)<1 or check_num==1):    
        if numbers ==0:
          #### 0ê°œì˜ keywordì— ëŒ€í•´ì„œ ì§ˆë¬¸í•œë‹¤ë©´? ex) ê°€ì¥ ìµœê·¼ ê³µì§€ì‚¬í•­ 0ê°œ ì•Œë ¤ì¤˜######
          keys=['ì„¸ë¯¸ë‚˜','í–‰ì‚¬','íŠ¹ê°•','ê°•ì—°','ê³µì§€ì‚¬í•­','ì±„ìš©']
          return None,[keyword for keyword in keys if keyword in user_question]
        if 'ê³µì§€ì‚¬í•­' in query_noun:
          key=['ê³µì§€ì‚¬í•­']
          notice_url="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1&wr_id="
          return_docs=find_url(notice_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        if 'ì±„ìš©' in query_noun:
          key=['ì±„ìš©']
          company_url="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b&wr_id="
          return_docs=find_url(company_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        other_key = ['ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°']
        if any(keyword in query_noun for keyword in other_key):
          seminar_url="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4&wr_id="
          key = [keyword for keyword in other_key if keyword in user_question]
          return_docs=find_url(seminar_url,titles_from_pinecone,dates_from_pinecone,texts_from_pinecone,urls_from_pinecone,numbers)
        recent_finish_time=time.time()-recent_time
        print(f"ìµœê·¼ ê³µì§€ì‚¬í•­ ë¬¸ì„œ ë½‘ëŠ” ì‹œê°„ {recent_finish_time}")
        if (len(return_docs)>0):
          return return_docs,key


      remove_noticement = ['ì œì¼','ê°€ì¥','ê³µê³ ', 'ê³µì§€ì‚¬í•­','í•„ë…','ì²¨ë¶€íŒŒì¼','ìˆ˜ì—…','ì»´í•™','ìƒìœ„','ê´€ë ¨']

      bm_title_time=time.time()
      tokenized_titles = [transformed_query(title) for title in titles_from_pinecone]

      # ê¸°ì¡´ê³¼ ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸
      bm25_titles = BM25Okapi(tokenized_titles, k1=1.5, b=0.75)  # ê¸°ì¡´ íŒŒë¼ë¯¸í„° í™•ì¸

      title_question_similarities = bm25_titles.get_scores(query_noun)  # ì œëª©ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ ê°„ì˜ ìœ ì‚¬ë„
      title_question_similarities /= 24
      

      adjusted_similarities = adjust_similarity_scores(query_noun, titles_from_pinecone,texts_from_pinecone, title_question_similarities)
      # ìœ ì‚¬ë„ ê¸°ì¤€ ìƒìœ„ 15ê°œ ë¬¸ì„œ ì„ íƒ
      top_20_titles_idx = np.argsort(title_question_similarities)[-25:][::-1]

       # ê²°ê³¼ ì¶œë ¥
      # print("ìµœì¢… ì •ë ¬ëœ BM25 ë¬¸ì„œ:")
      # for idx in top_20_titles_idx:  # top_20_titles_idxì—ì„œ ê° ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜´
      #     print(f"  ì œëª©: {titles[idx]}")
      #     print(f"  ìœ ì‚¬ë„: {title_question_similarities[idx]}")
      #     print(f" URL: {doc_urls[idx]}")
      #     print("-" * 50)

      Bm25_best_docs = [(titles_from_pinecone[i], dates_from_pinecone[i], texts_from_pinecone[i], urls_from_pinecone[i]) for i in top_20_titles_idx]
      bm_title_f_time=time.time()-bm_title_time
      print(f"bm25 ë¬¸ì„œ ë½‘ëŠ”ì‹œê°„: {bm_title_f_time}")
      ####################################################################################################
      dense_time=time.time()
      # 1. Dense Retrieval - Text ì„ë² ë”© ê¸°ë°˜ 20ê°œ ë¬¸ì„œ ì¶”ì¶œ
      query_dense_vector = np.array(embeddings.embed_query(user_question))  # ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”©

      # Pineconeì—ì„œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„° 20ê°œ ì¶”ì¶œ
      pinecone_results_text = index.query(vector=query_dense_vector.tolist(), top_k=30, include_values=False, include_metadata=True)
      pinecone_similarities_text = [res['score'] for res in pinecone_results_text['matches']]
      pinecone_docs_text = [(res['metadata'].get('title', 'No Title'),
                            res['metadata'].get('date', 'No Date'),
                            res['metadata'].get('text', ''),
                            res['metadata'].get('url', 'No URL')) for res in pinecone_results_text['matches']]

     
      pinecone_time=time.time()-dense_time
      print(f"íŒŒì¸ì½˜ì—ì„œ top k ë½‘ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ {pinecone_time}")

      #####íŒŒì¸ì½˜ìœ¼ë¡œ êµ¬í•œ  ë¬¸ì„œ ì¶”ì¶œ ë°©ì‹ ê²°í•©í•˜ê¸°.
      combine_dense_docs = []

      # 1. ë³¸ë¬¸ ê¸°ë°˜ ë¬¸ì„œë¥¼ combine_dense_docsì— ë¨¼ì € ì¶”ê°€
      for idx, text_doc in enumerate(pinecone_docs_text):
          text_similarity = pinecone_similarities_text[idx]*3.26
          text_similarity=adjust_date_similarity(text_similarity,text_doc[1],query_noun)
          matching_noun = [noun for noun in query_noun if noun in text_doc[2]]

          # # ë³¸ë¬¸ì— í¬í•¨ëœ ëª…ì‚¬ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ë„ ì¡°ì •
          for noun in matching_noun:
              text_similarity += len(noun)*0.20
              if re.search(r'\d', noun):  # ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ì–´ í™•ì¸
                  if noun in text_doc[2]:  # ë³¸ë¬¸ì—ë„ ìˆ«ì í¬í•¨ ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì¡°ì •
                      text_similarity += len(noun)*0.24
                  else:
                      text_similarity+=len(noun)*0.20
          combine_dense_docs.append((text_similarity, text_doc))

      ####query_nounì— í¬í•¨ëœ í‚¤ì›Œë“œë¡œ ìœ ì‚¬ë„ë¥¼ ë³´ì •
      # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
      combine_dense_docs.sort(key=lambda x: x[0], reverse=True)

      # ## ê²°ê³¼ ì¶œë ¥
      # print("\ní†µí•©ëœ íŒŒì¸ì½˜ë¬¸ì„œ ìœ ì‚¬ë„:")
      # for score, doc in combine_dense_docs:
      #     title, date, text, url = doc
      #     print(f"ì œëª©: {title}\nìœ ì‚¬ë„: {score} {url}")
      #     print('---------------------------------')


      #################################################3#################################################3
      #####################################################################################################3

      # Step 1: combine_dense_docsì— ì œëª©, ë³¸ë¬¸, ë‚ ì§œ, URLì„ ë¯¸ë¦¬ ì €ì¥

      # combine_dense_docëŠ” (ìœ ì‚¬ë„, ì œëª©, ë³¸ë¬¸ ë‚´ìš©, ë‚ ì§œ, URL) í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
      combine_dense_doc = []
      combine_time=time.time()
      # combine_dense_docsì˜ ë‚´ë¶€ êµ¬ì¡°ì— ë§ê²Œ ë‘ ë‹¨ê³„ë¡œ ë¶„í•´
      for score, (title, date, text, url) in combine_dense_docs:
          combine_dense_doc.append((score, title, text, date, url))
        
      combine_dense_doc=last_filter_keyword(combine_dense_doc,query_noun,user_question)
      # Step 2: combine_dense_docsì™€ BM25 ê²°ê³¼ í•©ì¹˜ê¸°
      final_best_docs = []

      # combine_dense_docsì™€ BM25 ê²°ê³¼ë¥¼ í•©ì³ì„œ ì²˜ë¦¬
      for score, title, text, date, url in combine_dense_doc:
          matched = False
          for bm25_doc in Bm25_best_docs:
              if bm25_doc[0] == title:  # ì œëª©ì´ ì¼ì¹˜í•˜ë©´ ìœ ì‚¬ë„ë¥¼ í•©ì‚°
                  combined_similarity = score + adjusted_similarities[titles_from_pinecone.index(bm25_doc[0])]
                  final_best_docs.append((combined_similarity, bm25_doc[0], bm25_doc[1], bm25_doc[2], bm25_doc[3]))
                  matched = True
                  break
          if not matched:

              # ì œëª©ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ combine_dense_docsì—ì„œë§Œ ìœ ì‚¬ë„ ì‚¬ìš©
              final_best_docs.append((score,title, date, text, url))


      # ì œëª©ì´ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” BM25 ë¬¸ì„œë„ ì¶”ê°€
      for bm25_doc in Bm25_best_docs:
          matched = False
          for score, title, text, date, url in combine_dense_doc:
              if bm25_doc[0] == title and bm25_doc[2]==text:  # ì œëª©ì´ ì¼ì¹˜í•˜ë©´ matched = Trueë¡œ ì²˜ë¦¬ë¨
                  matched = True
                  break
          if not matched:
              # ì œëª©ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ BM25 ë¬¸ì„œë§Œ final_best_docsì— ì¶”ê°€
              combined_similarity = adjusted_similarities[titles_from_pinecone.index(bm25_doc[0])]  # BM25 ìœ ì‚¬ë„ ê°€ì ¸ì˜¤ê¸°
              combined_similarity= adjust_date_similarity(combined_similarity,bm25_doc[1],query_noun)
              final_best_docs.append((combined_similarity, bm25_doc[0], bm25_doc[1], bm25_doc[2], bm25_doc[3]))
      final_best_docs.sort(key=lambda x: x[0], reverse=True)
      final_best_docs=final_best_docs[:20]


      # print("\n\n\n\ní•„í„°ë§ ì „ ìµœì¢…ë¬¸ì„œ (ìœ ì‚¬ë„ í° ìˆœ):")
      # for idx, (scor, titl, dat, tex, ur, image_ur) in enumerate(final_best_docs):
      #     print(f"ìˆœìœ„ {idx+1}: ì œëª©: {titl}, ìœ ì‚¬ë„: {scor},ë³¸ë¬¸ {len(tex)} ë‚ ì§œ: {dat}, URL: {ur}")
      #     print("-" * 50)
      
      final_best_docs=last_filter_keyword(final_best_docs,query_noun,user_question)
      final_best_docs.sort(key=lambda x: x[0], reverse=True)
      combine_f_time=time.time()-combine_time
      print(f"Bm25ë‘ pinecone ê²°í•© ì‹œê°„: {combine_f_time}")
      # print("\n\n\n\nì¤‘ê°„í•„í„° ìµœì¢…ë¬¸ì„œ (ìœ ì‚¬ë„ í° ìˆœ):")
      # for idx, (scor, titl, dat, tex, ur, image_ur) in enumerate(final_best_docs):
      #     print(f"ìˆœìœ„ {idx+1}: ì œëª©: {titl}, ìœ ì‚¬ë„: {scor},ë³¸ë¬¸ {len(tex)} ë‚ ì§œ: {dat}, URL: {ur}")
      #     print("-" * 50)

      def cluster_documents_by_similarity(docs, threshold=0.89):
          clusters = []

          for doc in docs:
              title = doc[1]
              added_to_cluster = False
              # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì™€ ë¹„êµ
              for cluster in clusters:
                  # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì œëª©ê³¼ í˜„ì¬ ë¬¸ì„œì˜ ì œëª©ì„ ë¹„êµí•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
                  cluster_title = cluster[0][1]
                  similarity = SequenceMatcher(None, cluster_title, title).ratio()
                  # ìœ ì‚¬ë„ê°€ threshold ì´ìƒì´ë©´ í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€
                  if similarity >= threshold:
                      #print(f"{doc[0]} {cluster[0][0]}  {title} {cluster_title}")
                      cluster_date=parse_date_change_korea_time(cluster[0][2])
                      doc_in_date=parse_date_change_korea_time(doc[2])
                      compare_date=abs(cluster_date-doc_in_date).days
                      if cluster_title==title or(-doc[0]+cluster[0][0]<0.6 and cluster[0][3]!=doc[2] and compare_date<60):
                        cluster.append(doc)
                      added_to_cluster = True
                      break

              # ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„° ìƒì„±
              if not added_to_cluster:
                  clusters.append([doc])

          return clusters

      # Step 1: Cluster documents by similarity
      cluster_time=time.time()
      clusters = cluster_documents_by_similarity(final_best_docs)
      # print(clusters[0])
      # print(clusters[1])
      # ë‚ ì§œ í˜•ì‹ì„ datetime ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
      def parse_date(date_str):
          # 'ì‘ì„±ì¼'ì„ ì œê±°í•˜ê³  ê³µë°±ì„ ì œê±°í•œ ë’¤ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
          clean_date_str = date_str.replace("ì‘ì„±ì¼", "").strip()
          return datetime.strptime(clean_date_str, "%y-%m-%d %H:%M")
      # Step 2: Compare cluster[0] cluster[1] top similarity and check condition
      top_0_cluster_similar=clusters[0][0][0]
      top_1_cluster_similar=clusters[1][0][0]
      keywords = ["ìµœê·¼", "ìµœì‹ ", "í˜„ì¬", "ì§€ê¸ˆ"]
      #print(f"{top_0_cluster_similar} {top_1_cluster_similar}")
      if (top_0_cluster_similar-top_1_cluster_similar<=0.3): ## ì§ˆë¬¸ì´ ëª¨í˜¸í–ˆë‹¤ëŠ” ì˜ë¯¸ì¼ ìˆ˜ ìˆìŒ.. (ì˜ˆë¥¼ ë“¤ë©´ ìˆ˜ê°•ì‹ ì²­ ì–¸ì œì•¼? ì¸ë° êµ¬ì²´ì ìœ¼ë¡œ 1í•™ê¸°ì¸ì§€, 2í•™ê¸°ì¸ì§€, ê²¨ìš¸, ì—¬ë¦„ì¸ì§€ ëª¨ë¥´ê²Œ..)
          # ë‚ ì§œë¥¼ ë¹„êµí•´ ë” ìµœê·¼ ë‚ ì§œë¥¼ ê°€ì§„ í´ëŸ¬ìŠ¤í„° ì„ íƒ
          #ì¡°ê¸ˆë” ì„¸ë°€í•˜ê²Œ ë“¤ì–´ê°€ìë©´?
          #print("ì„¸ë°€í•˜ê²Œ..")
          if (any(keyword in word for word in query_noun for keyword in keywords) or top_0_cluster_similar-clusters[len(clusters)-1][0][0]<=0.3):
            #print("ìµœê·¼ì´ê±°ë‚˜ ë½‘ì€ ë¬¸ì„œë“¤ì´ ìœ ì‚¬ë„ 0.3ì´ë‚´")
            if (top_0_cluster_similar-clusters[len(clusters)-1][0][0]<=0.3):
              #print("ìµœê·¼ì´ë©´ì„œ ë½‘ì€ ë¬¸ì„œë“¤ì´ ìœ ì‚¬ë„ 0.3ì´ë‚´ real")
              sorted_cluster=sorted(clusters, key=lambda doc: doc[0][2], reverse=True)
              sorted_cluster=sorted_cluster[0]
            else:
              #print("ìµœê·¼ì´ë©´ì„œ ë½‘ì€ ë¬¸ì„œë“¤ì´ ìœ ì‚¬ë„ 0.3ì´ìƒ")
              if (top_0_cluster_similar-top_1_cluster_similar<=0.3):
                #print("ìµœê·¼ì´ë©´ì„œ ë½‘ì€ ë‘ë¬¸ì„œì˜ ìœ ì‚¬ë„ 0.3ì´í•˜ì´ë¼ì„œ ë‘ ë¬¸ì„œë¡œ ì¤„ì„")
                date1 = parse_date_change_korea_time(clusters[0][0][2])
                date2 = parse_date_change_korea_time(clusters[1][0][2])
                result_date=(date1-date2).days
                if result_date<0:
                  result_docs=clusters[1]
                else:
                  result_docs=clusters[0]
                sorted_cluster = sorted(result_docs, key=lambda doc: doc[2], reverse=True)

              else:
                sorted_cluster=sorted(clusters, key=lambda doc: doc[0][0], reverse=True)
                sorted_cluster=sorted_cluster[0]
          else:
           # print("ë‘ í´ëŸ¬ìŠ¤í„°ë¡œ íŒë‹¨í•´ë³´ì..")
            if (top_0_cluster_similar-top_1_cluster_similar<=0.1):
             # print("ì§„ì§œ ì°¨ì´ê°€ ì—†ëŠ”ë“¯..?")
              date1 =parse_date_change_korea_time(clusters[0][0][2])
              date2 = parse_date_change_korea_time(clusters[1][0][2])
              result_date=(date1-date2).days
              if result_date<0:
                #print("ë‘ë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ê°€ ë” í¬ë„¤..?")
                result_docs=clusters[1]
              else:
                #print("ì²«ë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ê°€ ë” í¬ë„¤..?")
                result_docs=clusters[0]
              sorted_cluster = sorted(result_docs, key=lambda doc: doc[2], reverse=True)
            else:
              #print("ì—ì´ ê·¸ë˜ë„ ìœ ì‚¬ë„ ì°¨ì´ê°€ ìˆê¸´í•˜ë„¤..")
              result_docs=clusters[0]
              sorted_cluster=sorted(result_docs,key=lambda doc: doc[0],reverse=True)
      else: #ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„± ì—…ì—…
          number_pattern = r"\d"
          period_word=["ì—¬ë¦„","ê²¨ìš¸"]
          if (any(keyword in word for word in query_noun for keyword in keywords) or not any(re.search(number_pattern, word) for word in query_noun) or not any(key in word for word in query_noun for key in period_word)):
              #print("ìµœê·¼ ìµœì‹ ì´ë¼ëŠ” ë§ ë“œê°€ê±°ë‚˜ 2ê°€ì§€ ëª¨í˜¸í•œ íŒë‹¨ ê¸°ì¤€")
              if (any(re.search(number_pattern, word) for word in query_noun) or any(key in word for word in query_noun for key in period_word)):
                #print("ìµœì‹ ì¸ì¤„ ì•Œì•˜ì§€ë§Œ ìœ ì‚¬ë„ìˆœ..")
                result_docs=clusters[0]
                num=0
                for doc in result_docs:
                  if re.search(r'\d+ì°¨', doc[1]):
                    num+=1
                if num>1:
                  sorted_cluster=sorted(result_docs,key=lambda doc:doc[2],reverse=True)
                else:
                  sorted_cluster=sorted(result_docs,key=lambda doc:doc[0],reverse=True)
              else:
                #print("ë„ˆëŠ” ê·¸ëƒ¥ ìµœì‹ ìˆœì´ ë§ëŠ”ê±°ì—¬..")
                result_docs=clusters[0]
                sorted_cluster=sorted(result_docs,key=lambda doc: doc[2],reverse=True)
          else:
            #print("ì§„ì§œ ìœ ì‚¬ë„ìˆœëŒ€ë¡œ")
            result_docs=clusters[0]
            sorted_cluster = sorted(clusters[0], key=lambda doc: doc[0], reverse=True)
      cluster_f_time=time.time()-cluster_time
      print(f"clusterë¡œ ë¬¸ì„œ ì¶”ì¶œí•˜ëŠ” ì‹œê°„:{cluster_f_time}")
      # print("\n\n\n\nadd_similarë„£ê¸°ì „ ìƒìœ„ ë¬¸ì„œ (ìœ ì‚¬ë„ ë° ë‚ ì§œ ê¸°ì¤€ ì •ë ¬):")
      # for idx, (scor, titl, dat, tex, ur, image_ur) in enumerate(sorted_cluster):
      #     print(f"ìˆœìœ„ {idx+1}: ì œëª©: {titl}, ìœ ì‚¬ë„: {scor}, ë‚ ì§œ: {dat}, URL: {ur} ë‚´ìš©: {len(tex)}   ì´ë¯¸ì§€{len(image_ur)}")
      #     print("-" * 50)
      # print("\n\n\n")

      def organize_documents_v2(sorted_cluster, titles, doc_dates, texts, doc_urls):
          # ì²« ë²ˆì§¸ ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ˆê¸° ì„¤ì •
          top_doc = sorted_cluster[0]
          top_title = top_doc[1]

          # new_sorted_cluster ì´ˆê¸°í™” ë° ì²« ë²ˆì§¸ ë¬¸ì„œì™€ ë™ì¼í•œ ì œëª©ì„ ê°€ì§„ ë¬¸ì„œë“¤ì„ ëª¨ë‘ ì¶”ê°€
          new_sorted_cluster = []
          # titlesì—ì„œ top_titleê³¼ ê°™ì€ ì œëª©ì„ ê°€ì§„ ëª¨ë“  ë¬¸ì„œë¥¼ new_sorted_clusterì— ì¶”ê°€
          count=0
          for i, title in enumerate(titles):
              if title == top_title:
                  new_similar=top_doc[0]
                  count+=1
                  new_doc = (top_doc[0], titles[i], doc_dates[i], texts[i], doc_urls[i])
                  new_sorted_cluster.append(new_doc)
          for i in range(count-1):
            fix_similar=list(new_sorted_cluster[i])
            fix_similar[0]=fix_similar[0]+0.2*count
            new_sorted_cluster[i]=tuple(fix_similar)
          # sorted_clusterì—ì„œ new_sorted_clusterì— ì—†ëŠ” ì œëª©ë§Œ ì¶”ê°€
          for doc in sorted_cluster:
              doc_title = doc[1]
              # ì´ë¯¸ new_sorted_clusterì— ì¶”ê°€ëœ ì œëª©ì€ ì œì™¸
              if doc_title != top_title:
                  new_sorted_cluster.append(doc)

          return new_sorted_cluster,count

      # ì˜ˆì‹œ ì‚¬ìš©
      final_cluster,count = organize_documents_v2(sorted_cluster, titles_from_pinecone, dates_from_pinecone, texts_from_pinecone, urls_from_pinecone)
      return final_cluster[:count], query_noun

prompt_template = """ë‹¹ì‹ ì€ ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ ê³µì§€ì‚¬í•­ì„ ì „ë‹¬í•˜ëŠ” ì§ì›ì´ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì •í™•í•˜ê²Œ ì „ë‹¬í•´ì•¼ í•  ì˜ë¬´ê°€ ìˆìŠµë‹ˆë‹¤.
í˜„ì¬ í•œêµ­ ì‹œê°„: {current_time}

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:

1. ì§ˆë¬¸ì˜ ë‚´ìš©ì´ ì´ë²¤íŠ¸ì˜ ê¸°ê°„ì— ëŒ€í•œ ê²ƒì¼ ê²½ìš°, ë¬¸ì„œì— ì£¼ì–´ì§„ ê¸°í•œê³¼ í˜„ì¬ í•œêµ­ ì‹œê°„ì„ ë¹„êµí•˜ì—¬ í•´ë‹¹ ì´ë²¤íŠ¸ê°€ ì˜ˆì •ëœ ê²ƒì¸ì§€, ì§„í–‰ ì¤‘ì¸ì§€, ë˜ëŠ” ì´ë¯¸ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
  ì˜ˆë¥¼ ë“¤ì–´, "2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì¼ì •ì€ ì–¸ì œì•¼?"ë¼ëŠ” ì§ˆë¬¸ì„ ë°›ì•˜ì„ ê²½ìš°, í˜„ì¬ ì‹œê°„ì€ 11ì›”ì´ë¼ê³  ê°€ì •í•˜ë©´ ìˆ˜ê°•ì‹ ì²­ì€ ê¸°ê°„ì€ 8ì›”ì´ì—ˆìœ¼ë¯€ë¡œ ì´ë¯¸ ì¢…ë£Œëœ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤.
  ë”°ë¼ì„œ, "2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ì€ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ê³ , 2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
  ë˜ ë‹¤ë¥¸ ì˜ˆì‹œë¡œ í˜„ì¬ ì‹œê°„ì´ 11ì›” 12ì¼ì´ë¼ê³  ê°€ì •í•˜ì˜€ì„ ë•Œ, "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì€ ì–¸ì œì•¼?"ë¼ëŠ” ì§ˆë¬¸ì„ ë°›ì•˜ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì´ 11ì›” 13ì¼ì´ë¼ë©´ ì•„ì§ ì‹œì‘ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤.
  ë”°ë¼ì„œ, "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ì€ ì•„ì§ ì‹œì‘ ì „ì…ë‹ˆë‹¤."ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ ì¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
  ë˜ ë‹¤ë¥¸ ì˜ˆì‹œë¡œ í˜„ì¬ ì‹œê°„ì´ 11ì›” 13ì¼ì´ë¼ê³  ê°€ì •í•˜ì˜€ì„ ë•Œ, "ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì€ ì–¸ì œì•¼?"ë¼ëŠ” ì§ˆë¬¸ì„ ë°›ì•˜ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì´ 11ì›” 13ì¼ì´ë¼ë©´ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ì…ë‹ˆë‹¤.
  ë”°ë¼ì„œ, "í˜„ì¬ ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ê¸°ê°„ì…ë‹ˆë‹¤."ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ê³ , ê²¨ìš¸ ê³„ì ˆ ì‹ ì²­ ì¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
2. ì§ˆë¬¸ì—ì„œ í•µì‹¬ì ì¸ í‚¤ì›Œë“œë“¤ì„ ê³¨ë¼ í‚¤ì›Œë“œë“¤ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì•„ì„œ í•´ë‹¹ ë¬¸ì„œë¥¼ ì½ê³  ì •í™•í•œ ë‚´ìš©ì„ ë‹µë³€í•´ì£¼ì„¸ìš”.
3. ë¬¸ì„œì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ê¸¸ê²Œ ì „ë‹¬í•˜ê¸°ë³´ë‹¤ëŠ” ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ëŠ” ë‚´ìš©ì— í•´ë‹¹í•˜ëŠ” ë‹µë³€ë§Œì„ ì œê³µí•¨ìœ¼ë¡œì¨ ìµœëŒ€í•œ ë‹µë³€ì„ ê°„ê²°í•˜ê³  ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
4. ì—ì´ë¹…ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ì„ì˜ë¡œ íŒë‹¨í•´ì„œ ë„¤ ì•„ë‹ˆì˜¤ í•˜ì§€ ë§ê³  ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.
5. ë‹µë³€ì€ ì¹œì ˆí•˜ê²Œ ì¡´ëŒ“ë§ë¡œ ì œê³µí•˜ì„¸ìš”.
6. ì§ˆë¬¸ì´ ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ë‹¤ê³  íŒë‹¨í•˜ë©´ ì‘ë‹µí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ë©´ "ë„ˆëŠ” ë¬´ì—‡ì„ ì•Œê¹Œ", "ì ì‹¬ë©”ë‰´ ì¶”ì²œ"ê³¼ ê°™ì´ ì¼ë°˜ ìƒì‹ì„ ìš”êµ¬í•˜ëŠ” ì§ˆë¬¸ì€ ê±°ì ˆí•´ì£¼ì„¸ìš”.
7. ì—ì´ë¹… ì¸ì • ê´€ë ¨ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ê³„ì ˆí•™ê¸°ì¸ì§€ ê·¸ëƒ¥ í•™ê¸°ë¥¼ ë¬»ëŠ”ê²ƒì¸ì§€ ì§ˆë¬¸ì„ ì²´í¬í•´ì•¼í•©ë‹ˆë‹¤. ê³„ì ˆí•™ê¸°ê°€ ì•„ë‹Œ ê²½ìš°ì— ì‹¬ì»´,ê¸€ì†,ì¸ì»´ ê°œì„¤ì´ ì•„ë‹ˆë©´ ì—ì´ë¹… ì¸ì •ì´ ì•ˆë©ë‹ˆë‹¤.
ë‹µë³€:"""

# PromptTemplate ê°ì²´ ìƒì„±
PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["current_time", "context", "question"]
)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def get_answer_from_chain(best_docs, user_question,query_noun):

    documents = []
    doc_titles = []
    doc_dates = []
    doc_texts = []
    doc_urls = []
    for doc in best_docs:
        tit = doc[1]
        date = doc[2]
        text = doc[3]
        url = doc[4]
        # score,tit, date, text, url,im_url = doc
        doc_titles.append(tit)  # ì œëª©
        doc_dates.append(date)    # ë‚ ì§œ
        doc_texts.append(text)    # ë³¸ë¬¸
        doc_urls.append(url)     # URL

    documents = [
        Document(page_content=text, metadata={"title": title, "url": url, "doc_date": datetime.strptime(date, 'ì‘ì„±ì¼%y-%m-%d %H:%M')})
        for title, text, url, date in zip(doc_titles, doc_texts, doc_urls, doc_dates)
    ]

    relevant_docs = [doc for doc in documents if any(keyword in doc.page_content for keyword in query_noun)]
    if not relevant_docs:
      return None, None

    llm = ChatUpstage(api_key=upstage_api_key)
    relevant_docs_content=format_docs(relevant_docs)
    
    qa_chain = (
        {
            "current_time": lambda _: get_korean_time().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„"),
            "context": RunnableLambda(lambda _: relevant_docs_content),
            "question": RunnablePassthrough()
        }
        | PROMPT
        | llm
        | StrOutputParser()
    )

    return qa_chain,relevant_docs



#######################################################################

def question_valid(question, top_docs, query_noun):
    prompt = f"""
ì•„ë˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´, ì£¼ì–´ì§„ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ "ì˜ˆ" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”. ê° ì§ˆë¬¸ì— ëŒ€í•´ í•™ì‚¬ ê´€ë ¨ ì—¬ë¶€ë¥¼ ëª…í™•íˆ íŒë‹¨í•˜ê³ , ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ, ì œê³µë˜ëŠ” ê²½ìš°ì—ëŠ” "ì˜ˆ"ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤."

1. í•µì‹¬ íŒë‹¨ ì›ì¹™
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ë‹¤ë£¨ëŠ” ì •ë³´ì—ë§Œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•˜ë©°, ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì€ "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.

ì§ˆë¬¸ ë¶„ì„ 3ë‹¨ê³„:

ì§ˆë¬¸ì˜ ì‹¤ì œ ì˜ë„ì™€ ëª©ì  íŒŒì•…
í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ì œê³µë˜ëŠ” ì •ë³´ ì—¬ë¶€ í™•ì¸
í•™ì‚¬ ê´€ë ¨ì„± ìµœì¢… í™•ì¸

ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬:

ì£¼ìš” ì§ˆë¬¸ê³¼ ë¶€ê°€ ì§ˆë¬¸ êµ¬ë¶„
ë¶€ìˆ˜ì  ë‚´ìš©ì€ íŒë‹¨ì—ì„œ ì œì™¸
í•™ë¶€ ê³µì‹ ì •ë³´ì™€ ë¬´ê´€í•œ ì§ˆë¬¸ êµ¬ë³„
ì•…ì˜ì  ì§ˆë¬¸ ëŒ€ì‘:

í•™ì‚¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì—ˆë”ë¼ë„, ì‹¤ì œë¡œ í•™ë¶€ ì •ë³´ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ì§ˆë¬¸ì„ "ì•„ë‹ˆì˜¤"ë¡œ ë‹µë³€
2. "ì˜ˆ"ë¡œ íŒë‹¨í•˜ëŠ” í•™ì‚¬ ê´€ë ¨ ì¹´í…Œê³ ë¦¬:
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ ë‹¤ë£¨ëŠ” í•™ì‚¬ ì •ë³´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³ , í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ "ì˜ˆ"ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
ìˆ˜ì—… ë° í•™ì  ê´€ë ¨ ì •ë³´: ìˆ˜ê°•ì‹ ì²­, ìˆ˜ê°•ì •ì •, ìˆ˜ê°•ë³€ê²½, ìˆ˜ê°•ì·¨ì†Œ, ê¸°ë§ê³ ì‚¬, ì¤‘ê°„ê³ ì‚¬, ê³¼ëª© ìš´ì˜ ë°©ì‹, í•™ì  ì¸ì •, ë³µìˆ˜ì „ê³µ í˜¹ì€ ë¶€ì „ê³µ ìš”ê±´,êµì–‘ê°•ì˜ì™€ ê´€ë ¨ëœ ì§ˆë¬¸, ì „ê³µê°•ì˜ì™€ ê´€ë ¨ëœ ì§ˆë¬¸, ì‹¬ì»´, ì¸ì»´, ê¸€ì†¦ í•™ê³¼ì— ê´€ë ¨ëœ ì§ˆë¬¸, ê°•ì˜ ê°œì„  ê´€ë ¨ ì„¤ë¬¸
í•™ìƒ ì§€ì› ì œë„: ì¥í•™ê¸ˆ, í•™ê³¼ ì£¼ê´€ ì¸í„´ì‹­ í”„ë¡œê·¸ë¨, ë©˜í† ë§ ,ê°ì¢… ì¥í•™ìƒ ì„ ë°œ, í•™ìê¸ˆëŒ€ì¶œ, íŠ¹ì • ì§€ì—­ì˜ í•™ìê¸ˆëŒ€ì¶œ ê´€ë ¨ ì§ˆë¬¸
í•™ì‚¬ í–‰ì • ë° ì œë„: ì¡¸ì—… ìš”ê±´, í•™ì  ê´€ë¦¬, í•„ìˆ˜ ì´ìˆ˜ ìš”ê±´, ì¦ëª…ì„œ ë°œê¸‰, í•™ì‚¬ ì¼ì •, ìí‡´,ë³µí•™, íœ´í•™ ë“±
êµìˆ˜ì§„ ë° í–‰ì • ì •ë³´: êµìˆ˜ì§„ ì—°ë½ì²˜,ë²ˆí˜¸,ì´ë©”ì¼, í•™ê³¼ ì‚¬ë¬´ì‹¤ ì •ë³´, ì§€ë„êµìˆ˜ì™€ ê´€ë ¨ëœ ì •ë³´
í•™ë¶€ ì£¼ê´€ êµë‚´ í™œë™:  ê°ì¢… ê²½ì§„ëŒ€íšŒ, í–‰ì‚¬, ë²¤ì²˜í”„ë¡œê·¸ë¨ ,ë²¤ì²˜ì•„ì¹´ë°ë¯¸,íŠœí„°(TUTOR) ê´€ë ¨ í™œë™(ê·¼ë¬´ì¼ì§€ ì‘ì„±, ê·¼ë¬´ ê¸°ì¤€) íŠœí„°(TUTOR) ëª¨ì§‘ ë° ë¹„ìš© ê´€ë ¨ ì§ˆë¬¸, ë‹¤ì–‘í•œ í”„ë¡œê·¸ë¨(ì˜ˆ: AEP í”„ë¡œê·¸ë¨, CES í”„ë¡œê·¸ë¨,ë¯¸êµ­ í”„ë¡œê·¸ë¨)
ì‹ ì²­ ë° ì¼ì •, ì„±ì¸ì§€ êµìœ¡ì´ë‚˜ ì¸ê¶Œ êµìœ¡, í˜¹ì€ ë‹¤ë¥¸ êµìœ¡ì— ê´€ë ¨ëœ ì¼ì •
êµìˆ˜ì§„ ì •ë³´: êµìˆ˜ì˜ ëª¨ë“  ì •ë³´(ì´ë©”ì¼,ë²ˆí˜¸,ì—°ë½ì²˜,ë©”ì¼,ì‚¬ì§„,ì „ê³µ,ì—…ë¬´), í•™ê³¼ ê´€ë ¨ ì§ì›ì˜ ëª¨ë“  ì •ë³´, ë‹´ë‹¹ ì—…ë¬´ì™€ ê´€ë ¨ëœ í•™ê³¼ êµì§ì› ì •ë³´
ì¥í•™ê¸ˆ ë° êµë‚´ ì§€ì› ì œë„: ìµœê·¼ ì¥í•™ê¸ˆ ì„ ë°œ ì •ë³´ë‚˜ êµë‚´ ê°ì¢… ì§€ì› ì œë„ì— ëŒ€í•œ ì•ˆë‚´
ì¡¸ì—… ìš”ê±´ ì •ë³´: ì¡¸ì—…ì— í•„ìš”í•œ í•™ì  ìš”ê±´, í•„ìˆ˜ë¡œ ë“¤ì–´ì•¼ í•˜ëŠ” ê°•ì˜, ê³¼ëª©, ë“±ë¡ íšŸìˆ˜ ê´€ë ¨ ì •ë³´, ì¡¸ì—… ì‹œ í•„ìš”í•œ ì •ë³´ , í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë ¨ ì •ë³´ ì „ì²´ì ìœ¼ë¡œ ì¡¸ì—…ì— í•„ìš”í•œ ì •ë³´ëŠ” ë¬´ì¡°ê±´ "ì˜ˆ"ë¡œ í•©ë‹ˆë‹¤.
ê¸°íƒ€ í•™ì‚¬ ì œë„: êµë‚´ ë°©í•™ ì¤‘ ê·¼ë¡œì¥í•™ìƒ ê´€ë ¨ ì •ë³´, ëŒ€í•™ì›ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸,ëŒ€í•™ì›ìƒ í•™ì  ì¸ì • ì ˆì°¨ì™€ ìš”ê±´ ,ì „ì‹œíšŒ ê°œìµœ ë° ì§€ì› ì •ë³´, í–‰ì‚¬ ì§€ì› ì •ë³´, SW ë§ˆì¼ë¦¬ì§€ì™€ ê´€ë ¨ëœ ì •ë³´ ìš”êµ¬, ìŠ¤íƒ€íŠ¸ì—… ì •ë³´, ê°ì¢… íŠ¹ê°• ì •ë³´(ì˜¤í”ˆSW,ì˜¤í”ˆì†ŒìŠ¤, Ai ë“±)
ì±„ìš©ì •ë³´: ì‹ ì…ì‚¬ì› ì±„ìš©,ê²½ë ¥ì‚¬ì› ì±„ìš© ì •ë³´ë‚˜, íŠ¹ì • ê¸°ì—…ì˜ ëª¨ì§‘ ì •ë³´, ì¸í„´ ì±„ìš© ì •ë³´,ë¶€íŠ¸ìº í”„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸, ì±„ìš© ê´€ë ¨ ì§ˆë¬¸ ë˜í•œ í•™ì‚¬ í‚¤ì›Œë“œì— í¬í•¨ì´ ë©ë‹ˆë‹¤.


3. "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•˜ëŠ” ë¹„í•™ì‚¬ ì¹´í…Œê³ ë¦¬
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ ì±—ë´‡ì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.

êµë‚´ ì¼ë°˜ ì •ë³´: ê¸°ìˆ™ì‚¬, ì‹ë‹¹ ë©”ë‰´ ì •ë³´ ë“± ì»´í“¨í„°í•™ë¶€ì™€ ê´€ë ¨ ì—†ëŠ” êµë‚´ ìƒí™œ ì •ë³´
ì¼ë°˜ì  ê¸°ìˆ /ì§€ì‹ ë¬¸ì˜: í”„ë¡œê·¸ë˜ë° ë¬¸ë²•, ê¸°ìˆ  ê°œë… ì„¤ëª…, íŠ¹ì • ë„êµ¬ ì‚¬ìš©ë²• ë“± í•™ì‚¬ ì •ë³´ì™€ ë¬´ê´€í•œ ê¸°ìˆ ì  ì§ˆë¬¸

ë˜í•œ, {query_noun}ê³¼ {top_docs}ë¥¼ ë¹„êµí•˜ì˜€ì„ ë•Œ, {query_noun}ì•  í¬í•¨ëœ ë‹¨ì–´ ì¤‘ 2ê°œ ì´ìƒì´ {top_docs}ì™€ ì™„ì „íˆ ë¬´ê´€í•˜ë‹¤ë©´ "ì•„ë‹ˆì˜¤"ë¡œ íŒë‹¨í•˜ì„¸ìš”.

4. ë³µí•© ì§ˆë¬¸ íŒë‹¨ ê°€ì´ë“œ
ì§ˆë¬¸ì˜ í•µì‹¬ ëª©ì ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤:

ì˜ˆì‹œ:
"ì»´í“¨í„°í•™ë¶€ ìˆ˜ê°•ì‹ ì²­ ê¸°ê°„ ì•Œë ¤ì¤˜" â†’ "ì˜ˆ" (í•™ì‚¬ ì¼ì • ì •ë³´ ìš”ì²­)
"ì§€ë„êµìˆ˜ë‹˜ê³¼ ìƒë‹´í•˜ë ¤ë©´ ì–´ë–»ê²Œ ì˜ˆì•½í•˜ë‚˜ìš”?" â†’ "ì˜ˆ" (í•™ë¶€ ë‚´ êµìˆ˜ì§„ ìƒë‹´ ì ˆì°¨)
"í•™êµ ê¸°ìˆ™ì‚¬ ì •ë³´ ì•Œë ¤ì¤˜" â†’ "ì•„ë‹ˆì˜¤" (í•™ë¶€ì™€ ë¬´ê´€í•œ êµë‚´ ìƒí™œ ì •ë³´)
"ê²½ë¶ëŒ€ ì»´í“¨í„°í•™ë¶€ ê³µì§€ì‚¬í•­ì˜ ì œìœ¡ ë ˆì‹œí”¼ ì•Œë ¤ì¤˜" -> "ì•„ë‹ˆì˜¤" (í•™ë¶€ì˜ ê³µì§€ì‚¬í•­ì„ ì•Œë ¤ë‹¬ë¼ê³  í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì˜ë„ì ìœ¼ë¡œ ì œìœ¡ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ë‹¬ë¼ í•˜ëŠ” ì˜ë¯¸)
5. ì£¼ì˜ì‚¬í•­
ê²½ë¶ëŒ€í•™êµ ì»´í“¨í„°í•™ë¶€ í•™ì‚¬ ì •ë³´ ì œê³µì— í•œì •í•˜ì—¬ ë‹¤ìŒì„ ì§€í‚µë‹ˆë‹¤.

ë§¥ë½ ì¤‘ì‹¬ íŒë‹¨: ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ì§€ì–‘, ì§ˆë¬¸ì˜ ì‹¤ì œ ì˜ë„ì— ë§ì¶° íŒë‹¨
ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬: í•™ë¶€ ê´€ë ¨ ì •ë³´ê°€ í•µì‹¬ì¸ì§€ í™•ì¸
ì•…ì˜ì  ì§ˆë¬¸ ëŒ€ì‘: ë¹„í•™ì‚¬ì  ì •ë³´ë¥¼ í˜¼í•©í•œ ì§ˆë¬¸ì€ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ "ì•„ë‹ˆì˜¤"ë¡œ ì²˜ë¦¬

    ### ì§ˆë¬¸: '{question}'
    ### ì°¸ê³  ë¬¸ì„œ: '{top_docs}'
    ### ì§ˆë¬¸ì˜ ëª…ì‚¬í™”: '{query_noun}'
    """

    llm = ChatUpstage(api_key=upstage_api_key)
    response = llm.invoke(prompt)

    if "ì˜ˆ" in response.content.strip():
        return True
    else:
        return False

#######################################################################

##### ìœ ì‚¬ë„ ì œëª© ë‚ ì§œ ë³¸ë¬¸  url image_urlìˆœìœ¼ë¡œ ì €ì¥ë¨

def get_ai_message(question):
    s_time=time.time()
    best_time=time.time()
    top_doc, query_noun = best_docs(question)  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    best_f_time=time.time()-best_time
    print(f"best_docs ë½‘ëŠ” ì‹œê°„:{best_f_time}")

    # query_nounì´ ì—†ê±°ë‚˜ top_docì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not query_noun or not top_doc or len(top_doc) == 0:
        notice_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }
        return not_in_notices_response
    if len(query_noun)==1 and any(keyword in query_noun for keyword in ['ì±„ìš©','ê³µì§€ì‚¬í•­','ì„¸ë¯¸ë‚˜','í–‰ì‚¬','ê°•ì—°','íŠ¹ê°•']):
      seen_urls = set()  # ì´ë¯¸ ë³¸ URLì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©
      response = f"'{query_noun[0]}'ì— ëŒ€í•œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
      show_url=""
      if top_doc !=None:
        for title, date, _, url in top_doc:  # top_docì—ì„œ ì œëª©, ë‚ ì§œ, URL ì¶”ì¶œ
            if url not in seen_urls:
                response += f"ì œëª©: {title}, ë‚ ì§œ: {date} \n----------------------------------------------------\n"
                seen_urls.add(url)  # URL ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
      if 'ì±„ìš©' in query_noun:
        show_url="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b&wr_id="
      elif 'ê³µì§€ì‚¬í•­' in query_noun:
        show_url="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1&wr_id="         
      else:
        show_url="https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4&wr_id="

      # ìµœì¢… data êµ¬ì¡° ìƒì„±
      data = {
        "answer": response,
        "references": show_url,  # show_urlì„ ë„˜ê¸°ê¸°
        "disclaimer": "\n\ní•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "images": ["No content"]
      }
      f_time=time.time()-s_time
      print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
      return data
    top_docs = [list(doc) for doc in top_doc]
    valid_time=time.time()
    if False == (question_valid(question, top_docs[0][1], query_noun)):
        for i in range(len(top_docs)):
            top_docs[i][0] -= 2
    
    final_score = top_docs[0][0]
    final_title = top_docs[0][1]
    final_date = top_docs[0][2]
    final_text = top_docs[0][3]
    final_url = top_docs[0][4]
    final_image = []

    record = collection.find_one({"title" : final_title})
    if record :
        if(isinstance(record["image_url"], list)):
          final_image.extend(record["image_url"])
        else :
          final_image.append(record["image_url"])
    else :
        print("ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì¡´ì¬ X")
        final_score = 0
        final_title = "No content"
        final_date = "No content"
        final_text = "No content"
        final_url = "No URL"
        final_image = ["No content"]
    valid_f_time=time.time()-valid_time
    print(f"ì§ˆë¬¸ ì í•©ë„ ì²´í¬í•˜ëŠ” ì‹œê°„: {valid_f_time}")
    # top_docs ì¸ë±ìŠ¤ êµ¬ì„±
    # 0: ìœ ì‚¬ë„, 1: ì œëª©, 2: ë‚ ì§œ, 3: ë³¸ë¬¸ë‚´ìš©, 4: url, 5: ì´ë¯¸ì§€url

    if final_image[0] != "No content" and final_text == "No content" and final_score > 1.8:
        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        only_image_response = {
            "answer": None,
            "references": final_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return only_image_response

    # ì´ë¯¸ì§€ + LLM ë‹µë³€ì´ ìˆëŠ” ê²½ìš°.
    else:
        chain_time=time.time()
        qa_chain, relevant_docs = get_answer_from_chain(top_docs, question,query_noun)
        chain_f_time=time.time()-chain_time
        print(f"chain ìƒì„±í•˜ëŠ” ì‹œê°„: {chain_f_time}")
        if final_url == "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2&lang=kor" and any(keyword in query_noun for keyword in ['ì—°ë½ì²˜', 'ì „í™”', 'ë²ˆí˜¸', 'ì „í™”ë²ˆí˜¸']):
            data = {
                "answer": "í•´ë‹¹ êµìˆ˜ë‹˜ì€ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return data
            
        # prof_title=final_title
        # prof_url=["https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5",
        #           "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_1"]
        # prof_name=""
        # # ì •ê·œì‹ì„ ì´ìš©í•˜ì—¬ ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ ì¶”ì¶œ
        # if any(final_url.startswith(url) for url in prof_url):
        #     match = re.match(r"^[^\dA-Za-z]+", prof_title)
        #     if match:
        #         prof_name = match.group().strip()  # ìˆ«ì ì´ì „ì˜ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     else:
        #         prof_name = prof_title.strip()  # ìˆ«ìê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¬¸ìì—´ì„ êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        #     prof_name = re.sub(r"\s+", "", prof_name)
        #     user_question = re.sub(r"\s+", "", question)
        #     if prof_name not in user_question:
        #         refer_url=""
        #         if 'ì§ì›' in query_noun:
        #             refer_url=prof_url[1]
        #         else:
        #             refer_url=prof_url[2]
        #         data = {
        #             "answer": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” êµìˆ˜ë‹˜ ì •ë³´ì…ë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
        #             "references": refer_url,
        #             "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        #             "images": ["No content"]
        #         }
        #         return data

        # ê³µì§€ì‚¬í•­ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        notice_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨
        if not qa_chain or not relevant_docs:
            if final_image[0] != "No content" and final_score > 1.8:
                data = {
                    "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "references": final_url,
                    "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "images": final_image
                }
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return data
            else:
                f_time=time.time()-s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return not_in_notices_response

        # ìœ ì‚¬ë„ê°€ ë‚®ì€ ê²½ìš°
        if final_score < 1.8:
            f_time=time.time()-s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return not_in_notices_response

        # LLMì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²½ìš°
        answer_time=time.time()
        answer_result = qa_chain.invoke(question)
        answer_f_time=time.time()-answer_time
        print(f"ë‹µë³€ ìƒì„±í•˜ëŠ” ì‹œê°„: {answer_f_time}")
        doc_references = "\n".join([
            f"\nì°¸ê³  ë¬¸ì„œ URL: {doc.metadata['url']}"
            for doc in relevant_docs[:1] if doc.metadata.get('url') != 'No URL'
        ])

        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        data = {
            "answer": answer_result,
            "references": doc_references,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
        f_time=time.time()-s_time
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return data