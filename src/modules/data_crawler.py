# %pip install -U langchain-community
# %pip install beautifulsoup4 requests scikit-learn pinecone-client numpy langchain-upstage faiss-cpu
# %pip install langchain
# %pip install nltk
# !pip install spacy
# !python -m spacy download ko_core_news_sm
# # í•„ìš”í•œ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# !apt-get install -y python3-dev
# !apt-get install -y libmecab-dev
# !apt-get install -y mecab mecab-ko mecab-ko-dic
# %pip install rank_bm25
# # konlpy ì„¤ì¹˜
# !pip install konlpy
# !pip install python-Levenshtein
# !pip install sentence-transformers
# !pip install pymongo

import os
import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain_upstage import UpstageEmbeddings
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from pinecone import Pinecone
from langchain_upstage import ChatUpstage
from langchain import hub
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import Document
from langchain.vectorstores import FAISS
import re
from datetime import datetime
import pytz
from langchain.schema.runnable import Runnable
from langchain.chains import RetrievalQAWithSourcesChain, RetrievalQA
from langchain.schema.runnable import RunnableSequence, RunnableMap
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from konlpy.tag import Okt
from collections import defaultdict
import Levenshtein
import numpy as np
from IPython.display import display, HTML
from rank_bm25 import BM25Okapi
from difflib import SequenceMatcher
from pymongo import MongoClient
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Pinecone API í‚¤ì™€ ì¸ë±ìŠ¤ ì´ë¦„ (.envì—ì„œ ë¡œë“œ)
pinecone_api_key = os.getenv('PINECONE_API_KEY')
index_name = os.getenv('PINECONE_INDEX_NAME', 'info')
# Upstage API í‚¤ (.envì—ì„œ ë¡œë“œ)
upstage_api_key = os.getenv('UPSTAGE_API_KEY')


# Pinecone API ì„¤ì • ë° ì´ˆê¸°í™”
pc = Pinecone(api_key=pinecone_api_key)
index = pc.Index(index_name)
def get_korean_time():
    return datetime.now(pytz.timezone('Asia/Seoul'))

# mongodb ì—°ê²°, clientë¡œ
client = MongoClient("mongodb://mongodb:27017/")

db = client["knu_chatbot"]
collection = db["notice_collection"]

####ê³µì§€ì‚¬í•­ í¬ë¡¤ë§í•˜ëŠ” ì½”ë“œ ################
def get_latest_wr_id():
    url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
    response = requests.get(url)
    if response.status_code == 200:
        match = re.search(r'wr_id=(\d+)', response.text)
        if match:
            return int(match.group(1))
    return None


# ìŠ¤í¬ë˜í•‘í•  URL ëª©ë¡ ìƒì„±
now_number = get_latest_wr_id()
base_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1&wr_id="

# ê¸°ë³¸ URL ëª©ë¡ ìƒì„±
urls = [f"{base_url}{number}" for number in range(now_number, 27726, -1)]

# ì¶”ê°€ë¡œ í•„ìš”í•œ URL ëª©ë¡
add_urls = [
    27510, 27047, 27614, 27246, 25900,
    27553, 25896, 25817, 25560, 27445,25804
]

# ì¶”ê°€ URLì„ `urls` ë¦¬ìŠ¤íŠ¸ì— í™•ì¥
urls.extend(f"{base_url}{wr_id}" for wr_id in add_urls)

# URLì—ì„œ ì œëª©, ë‚ ì§œ, ë‚´ìš©(ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URL) ì¶”ì¶œí•˜ëŠ” ê³µì§€ì‚¬í•­ í•¨ìˆ˜
def extract_text_and_date_from_url(urls):
    all_data = []

    def fetch_text_and_date(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('span', class_='bo_v_tit')
            title = title_element.get_text(strip=True) if title_element else "Unknown Title"

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URLì„ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            text_content = "Unknown Content"  # í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            image_content = []  # ì´ë¯¸ì§€ URLì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            paragraphs = soup.find('div', id='bo_v_con')
            if paragraphs:
                # paragraphs ë‚´ë¶€ì—ì„œ 'p', 'div', 'li' íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = "\n".join([element.get_text(strip=True) for element in paragraphs.find_all(['p', 'div', 'li'])])
                #print(text_content)
                if text_content.strip() == "":
                    text_content = ""
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                for img in paragraphs.find_all('img'):
                    img_src = img.get('src')
                    if img_src:
                        image_content.append(img_src)

            # ë‚ ì§œ ì¶”ì¶œ
            date_element = soup.select_one("strong.if_date")  # ìˆ˜ì •ëœ ì„ íƒì
            date = date_element.get_text(strip=True) if date_element else "Unknown Date"

            # ì œëª©ì´ Unknown Titleì´ ì•„ë‹ ë•Œë§Œ ë°ì´í„° ì¶”ê°€
            if title != "Unknown Title":
                return title, text_content, image_content, date, url  # ë¬¸ì„œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸, ë‚ ì§œ, URL ë°˜í™˜
            else:
                return None, None, None, None, None  # ì œëª©ì´ Unknownì¼ ê²½ìš° None ë°˜í™˜
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None, None, None, None, url

    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_text_and_date, urls)

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
    all_data = [(title, text_content, image_content, date, url) for title, text_content, image_content, date, url in results if title is not None]
    return all_data

#### í¬ë¡¤ë§í•œ ê³µì§€ì‚¬í•­ ì •ë³´ document_dataì— ì €ì¥
print(f"\n{'='*80}")
print(f"ğŸŒ ê²½ë¶ëŒ€ ì»´í“¨í„°í•™ë¶€ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘")
print(f"ğŸ“‹ í¬ë¡¤ë§í•  URL ê°œìˆ˜: {len(urls)}ê°œ")
print(f"{'='*80}\n")
print("ğŸ”„ ì›¹ í¬ë¡¤ë§ ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n")

document_data = extract_text_and_date_from_url(urls)

print(f"\n{'='*80}")
print(f"âœ… ì›¹ í¬ë¡¤ë§ ì™„ë£Œ! {len(document_data)}ê°œ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ë¨")
print(f"{'='*80}\n")
################################################################################################

# í…ìŠ¤íŠ¸ ë¶„ë¦¬ê¸° ì´ˆê¸°í™”
class CharacterTextSplitter:
    def __init__(self, chunk_size=850, chunk_overlap=100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    def split_text(self, text):
        chunks = []
        if len(text) <= self.chunk_size:
            return [text]
        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
            chunk = text[i:i + self.chunk_size]
            if chunk:
                chunks.append(chunk)
        return chunks

text_splitter = CharacterTextSplitter(chunk_size=850, chunk_overlap=100)

################################################################################################

# í…ìŠ¤íŠ¸ ë¶„ë¦¬ ë° URLê³¼ ë‚ ì§œ ë§¤í•‘
texts = []
image_url=[]
titles = []
doc_urls = []
doc_dates = []

for title, doc, image, date, url in document_data:
    if isinstance(doc, str) and doc.strip():  # docê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        split_texts = text_splitter.split_text(doc)
        texts.extend(split_texts)
        titles.extend([title] * len(split_texts))  # ì œëª©ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        doc_urls.extend([url] * len(split_texts))
        doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©
        
        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            image_url.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            image_url.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"

    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        # í…ìŠ¤íŠ¸ëŠ” "No content"ë¡œ ì¶”ê°€
        texts.append("No content")
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image_url.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        texts.append("No content")
        image_url.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image = "No content"
    
    temp_data = {
        "title" : title,
        "image_url" : image
    	}
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")



######################################################################################################
########################### ì§€ê¸ˆê¹Œì§€ ê³µì§€ì‚¬í•­ ì •ë³´ ###################################################
######################################################################################################


######   ì •êµìˆ˜ì§„ì˜ ì •ë³´ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ ##########

def extract_professor_info_from_urls(urls):
    all_data = []

    def fetch_professor_info(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # êµìˆ˜ ì •ë³´ê°€ ë‹´ê¸´ ìš”ì†Œë“¤ ì„ íƒ
            professor_elements = soup.find("div", id="dr").find_all("li")

            for professor in professor_elements:
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                image_element = professor.find("div", class_="dr_img").find("img")
                image_content = image_element["src"] if image_element else "Unknown Image URL"

                # ì´ë¦„ ì¶”ì¶œ
                name_element = professor.find("div", class_="dr_txt").find("h3")
                title = name_element.get_text(strip=True) if name_element else "Unknown Name"

                # ì—°ë½ì²˜ì™€ ì´ë©”ì¼ ì¶”ì¶œ í›„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
                contact_info = professor.find("div", class_="dr_txt").find_all("dd")
                contact_number = contact_info[0].get_text(strip=True) if len(contact_info) > 0 else "Unknown Contact Number"
                email = contact_info[1].get_text(strip=True) if len(contact_info) > 1 else "Unknown Email"
                text_content = f"{title}, {contact_number}, {email}"

                # ë‚ ì§œì™€ URL ì„¤ì •
                date = "ì‘ì„±ì¼24-01-01 00:00"

                prof_url_element = professor.find("a")
                prof_url = prof_url_element["href"] if prof_url_element else "Unknown URL"

                # ê° êµìˆ˜ì˜ ì •ë³´ë¥¼ all_dataì— ì¶”ê°€
                all_data.append((title, text_content, image_content, date, prof_url))
                

        except Exception as e:
            print(f"Error processing {url}: {e}")

    # ThreadPoolExecutorë¥¼ ì´ìš©í•˜ì—¬ ë³‘ë ¬ í¬ë¡¤ë§
    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_professor_info, urls)

    return all_data


######   ì´ˆë¹™êµìˆ˜ì§„ì˜ ì •ë³´ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ ##########

def extract_professor_info_from_urls_2(urls):
    all_data = []

    def fetch_professor_info(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # êµìˆ˜ ì •ë³´ê°€ ë‹´ê¸´ ìš”ì†Œë“¤ ì„ íƒ
            professor_elements = soup.find("div", id="Student").find_all("li")

            for professor in professor_elements:
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                image_element = professor.find("div", class_="img").find("img")
                image_content = image_element["src"] if image_element else "Unknown Image URL"

                # ì´ë¦„ ì¶”ì¶œ
                name_element = professor.find("div", class_="cnt").find("div", class_="name")
                title = name_element.get_text(strip=True) if name_element else "Unknown Name"

                # ì—°ë½ì²˜ì™€ ì´ë©”ì¼ ì¶”ì¶œ
                contact_place = professor.find("div", class_="dep").get_text(strip=True) if professor.find("div", class_="dep") else "Unknown Contact Place"
                email_element = professor.find("dl", class_="email").find("dd").find("a")
                email = email_element.get_text(strip=True) if email_element else "Unknown Email"

                # í…ìŠ¤íŠ¸ ë‚´ìš© ì¡°í•©
                text_content = f"ì„±í•¨(ì´ë¦„):{title}, ì—°êµ¬ì‹¤(ì¥ì†Œ):{contact_place}, ì´ë©”ì¼:{email}"

                # ë‚ ì§œì™€ URL ì„¤ì •
                date = "ì‘ì„±ì¼24-01-01 00:00"
                prof_url = url

                # ê° êµìˆ˜ì˜ ì •ë³´ë¥¼ all_dataì— ì¶”ê°€
                all_data.append((title, text_content, image_content, date, prof_url))
                
               

        except Exception as e:
            print(f"Error processing {url}: {e}")

    # ThreadPoolExecutorë¥¼ ì´ìš©í•˜ì—¬ ë³‘ë ¬ í¬ë¡¤ë§
    with ThreadPoolExecutor() as executor:
        executor.map(fetch_professor_info, urls)

    return all_data

######   ì§ì›ì˜ ì •ë³´ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ ##########

def extract_professor_info_from_urls_3(urls):
    all_data = []

    def fetch_professor_info(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # êµìˆ˜ ì •ë³´ê°€ ë‹´ê¸´ ìš”ì†Œë“¤ ì„ íƒ
            professor_elements = soup.find("div", id="Student").find_all("li")

            for professor in professor_elements:
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                image_element = professor.find("div", class_="img").find("img")
                image_content = image_element["src"] if image_element else "Unknown Image URL"

                # ì´ë¦„ ì¶”ì¶œ
                name_element = professor.find("div", class_="cnt").find("h1")
                title = name_element.get_text(strip=True) if name_element else "Unknown Name"

                # ì—°ë½ì²˜ ì¶”ì¶œ
                contact_number_element = professor.find("span", class_="period")
                contact_number = contact_number_element.get_text(strip=True) if contact_number_element else "Unknown Contact Number"

                # ì—°êµ¬ì‹¤ ìœ„ì¹˜ ì¶”ì¶œ
                contact_info = professor.find_all("dl", class_="dep")
                contact_place = contact_info[0].find("dd").get_text(strip=True) if len(contact_info) > 0 else "Unknown Contact Place"

                # ì´ë©”ì¼ ì¶”ì¶œ
                email = contact_info[1].find("dd").find("a").get_text(strip=True) if len(contact_info) > 1 else "Unknown Email"

                # ë‹´ë‹¹ ì—…ë¬´ ì¶”ì¶œ
                role = contact_info[2].find("dd").get_text(strip=True) if len(contact_info) > 2 else "Unknown Role"

                # í…ìŠ¤íŠ¸ ë‚´ìš© ì¡°í•©
                text_content = f"ì„±í•¨(ì´ë¦„):{title}, ì—°ë½ì²˜(ì „í™”ë²ˆí˜¸):{contact_number}, ì‚¬ë¬´ì‹¤(ì¥ì†Œ):{contact_place}, ì´ë©”ì¼:{email}, ë‹´ë‹¹ì—…ë¬´:{role}"

                # ë‚ ì§œì™€ URL ì„¤ì •
                date = "ì‘ì„±ì¼24-01-01 00:00"
                prof_url = url

                # ê° êµìˆ˜ì˜ ì •ë³´ë¥¼ all_dataì— ì¶”ê°€
                all_data.append((title, text_content, image_content, date, prof_url))
                
                

        except Exception as e:
            print(f"Error processing {url}: {e}")

    # ThreadPoolExecutorë¥¼ ì´ìš©í•˜ì—¬ ë³‘ë ¬ í¬ë¡¤ë§
    with ThreadPoolExecutor() as executor:
        executor.map(fetch_professor_info, urls)

    return all_data


# êµìˆ˜ì§„ í˜ì´ì§€ URL ëª©ë¡
urls2 = [
    "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_1&lang=kor",
]
urls3 = [
    "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2&lang=kor",
]
urls4 = [
    "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5&lang=kor",
]

prof_data = extract_professor_info_from_urls(urls2)
prof_data_2 = extract_professor_info_from_urls_2(urls3)
prof_data_3 = extract_professor_info_from_urls_3(urls4)

combined_prof_data = prof_data + prof_data_2 + prof_data_3

# êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ ë°ì´í„° ë¶„ë¦¬ ë° ì €ì¥
professor_texts = []
professor_image_urls = []
professor_titles = []
professor_doc_urls = []
professor_doc_dates = []

# prof_dataëŠ” extract_professor_info_from_urls í•¨ìˆ˜ì˜ ë°˜í™˜ê°’
for title, doc, image, date, url in combined_prof_data :
    if isinstance(doc, str) and doc.strip():  # êµìˆ˜ ì •ë³´ê°€ ë¬¸ìì—´ë¡œ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œ
        split_texts = text_splitter.split_text(doc)
        professor_texts.extend(split_texts)
        professor_titles.extend([title] * len(split_texts))  # êµìˆ˜ ì´ë¦„ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        professor_doc_urls.extend([url] * len(split_texts))
        professor_doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©
	
        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            professor_image_urls.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:
            professor_image_urls.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"
            

    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        professor_texts.append("No content")
        professor_titles.append(title)
        professor_doc_urls.append(url)
        professor_doc_dates.append(date)
        professor_image_urls.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        professor_texts.append("No content")
        professor_image_urls.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        professor_titles.append(title)
        professor_doc_urls.append(url)
        professor_doc_dates.append(date)
        image = "No content"
    
    temp_data = {
        "title" : title,
        "image_url" : image
    }
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")
    
# êµìˆ˜ ì •ë³´ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
texts.extend(professor_texts)
image_url.extend(professor_image_urls)
titles.extend(professor_titles)
doc_urls.extend(professor_doc_urls)
doc_dates.extend(professor_doc_dates)


######################################################################################################
###########################êµìˆ˜ ë° ì§ì›ì •ë³´ì„ ìœ„ëŠ”  ##################################################
######################################################################################################



####### ì·¨ì—…ì •ë³´ë¥¼ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ #######

def get_latest_wr_id_1():
    url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b"
    response = requests.get(url)
    if response.status_code == 200:
        # re.findallì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  wr_id ê°’ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        match = re.findall(r'wr_id=(\d+)', response.text)
        if match:
        # wr_ids ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ëª¨ë“  wr_id ê°’ì„ ì¶œë ¥
          max_wr_id = max(int(wr_id) for wr_id in match)
          return max_wr_id
    return None


now_company_number=get_latest_wr_id_1()

company_urls=[]
for number in range(now_company_number,1149,-1):
  company_urls.append("https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b&wr_id="+str(number))


def extract_company_from_url(urls):
    all_data = []

    def fetch_text_and_date(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('span', class_='bo_v_tit')
            title = title_element.get_text(strip=True) if title_element else "Unknown Title"

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URLì„ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            text_content = "Unknown Content"  # í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            image_content = []  # ì´ë¯¸ì§€ URLì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            paragraphs = soup.find('div', id='bo_v_con')
            if paragraphs:
                # paragraphs ë‚´ë¶€ì—ì„œ 'p', 'div', 'li' íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = "\n".join([element.get_text(strip=True) for element in paragraphs.find_all(['p', 'div', 'li'])])
                #print(text_content)
                if text_content.strip() == "":
                    text_content = ""
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                for img in paragraphs.find_all('img'):
                    img_src = img.get('src')
                    if img_src:
                        image_content.append(img_src)

            # ë‚ ì§œ ì¶”ì¶œ
            date_element = soup.select_one("strong.if_date")  # ìˆ˜ì •ëœ ì„ íƒì
            date = date_element.get_text(strip=True) if date_element else "Unknown Date"

            # ì œëª©ì´ Unknown Titleì´ ì•„ë‹ ë•Œë§Œ ë°ì´í„° ì¶”ê°€
            if title != "Unknown Title":
                return title, text_content, image_content, date, url  # ë¬¸ì„œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸, ë‚ ì§œ, URL ë°˜í™˜
            else:
                return None, None, None, None, None  # ì œëª©ì´ Unknownì¼ ê²½ìš° None ë°˜í™˜
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None, None, None, None, url

    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_text_and_date, urls)

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
    all_data = [(title, text_content, image_content, date, url) for title, text_content, image_content, date, url in results if title is not None]
    return all_data

company_data= extract_company_from_url(company_urls)

for title, doc, image, date, url in company_data:
    if isinstance(doc, str) and doc.strip():  # docê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        split_texts = text_splitter.split_text(doc)
        texts.extend(split_texts)
        titles.extend([title] * len(split_texts))  # ì œëª©ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        doc_urls.extend([url] * len(split_texts))
        doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©
        
        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            image_url.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            image_url.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"
            
    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        # í…ìŠ¤íŠ¸ëŠ” "No content"ë¡œ ì¶”ê°€
        texts.append("No content")
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image_url.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        texts.append("No content")
        image_url.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        image = "No content"
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)

    temp_data = {
        "title" : title,
        "image_url" : image
    }
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")

######################################################################################################
###########################ì·¨ì—…ì •ë³´ì„ ìœ„ëŠ”  ##########################################################
######################################################################################################



def get_latest_wr_id_2():
    url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4"
    response = requests.get(url)
    if response.status_code == 200:
        # re.findallì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  wr_id ê°’ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        match = re.findall(r'wr_id=(\d+)', response.text)
        if match:
        # wr_ids ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ëª¨ë“  wr_id ê°’ì„ ì¶œë ¥
          max_wr_id = max(int(wr_id) for wr_id in match)
          return max_wr_id
    return None


now_seminar_number=get_latest_wr_id_2()

seminar_urls=[]
for number in range(now_seminar_number,246,-1):
  seminar_urls.append("https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4&wr_id="+str(number))


def extract_seminar_from_url(urls):
    all_data = []

    def fetch_text_and_date(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('span', class_='bo_v_tit')
            title = title_element.get_text(strip=True) if title_element else "Unknown Title"

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URLì„ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            text_content = "Unknown Content"  # í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            image_content = []  # ì´ë¯¸ì§€ URLì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            paragraphs = soup.find('div', id='bo_v_con')
            if paragraphs:
                # paragraphs ë‚´ë¶€ì—ì„œ 'p', 'div', 'li' íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = "\n".join([element.get_text(strip=True) for element in paragraphs.find_all(['p', 'div', 'li'])])
                #print(text_content)
                if text_content.strip() == "":
                    text_content = ""
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                for img in paragraphs.find_all('img'):
                    img_src = img.get('src')
                    if img_src:
                        image_content.append(img_src)

            # ë‚ ì§œ ì¶”ì¶œ
            date_element = soup.select_one("strong.if_date")  # ìˆ˜ì •ëœ ì„ íƒì
            date = date_element.get_text(strip=True) if date_element else "Unknown Date"

            # ì œëª©ì´ Unknown Titleì´ ì•„ë‹ ë•Œë§Œ ë°ì´í„° ì¶”ê°€
            if title != "Unknown Title":
                return title, text_content, image_content, date, url  # ë¬¸ì„œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸, ë‚ ì§œ, URL ë°˜í™˜
            else:
                return None, None, None, None, None  # ì œëª©ì´ Unknownì¼ ê²½ìš° None ë°˜í™˜
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None, None, None, None, url

    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_text_and_date, urls)

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
    all_data = [(title, text_content, image_content, date, url) for title, text_content, image_content, date, url in results if title is not None]
    return all_data

seminar_data= extract_seminar_from_url(seminar_urls)



for title, doc, image, date, url in seminar_data:
    if isinstance(doc, str) and doc.strip():  # docê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        split_texts = text_splitter.split_text(doc)
        texts.extend(split_texts)
        titles.extend([title] * len(split_texts))  # ì œëª©ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        doc_urls.extend([url] * len(split_texts))
        doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©

        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            image_url.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            image_url.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"
    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        # í…ìŠ¤íŠ¸ëŠ” "No content"ë¡œ ì¶”ê°€
        texts.append("No content")
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image_url.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        texts.append("No content")
        image_url.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image = "No content"
    
    temp_data = {
        "title" : title,
        "image_url" : image
    }
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")

######################################################################################################
############################ì„¸ë¯¸ë‚˜ì„ ìœ„ëŠ”  ##########################################################
######################################################################################################

# ë°‘ì— ì½”ë“œëŠ” ì´ˆê¸°ì— í•œ ë²ˆë§Œ ëŒë¦¼. ì‹¤ì œ ì„œë²„ ëŒë¦´ ë•ŒëŠ” ì‚¬ìš© X
# Dense Retrieval (Upstage ì„ë² ë”©)
print(f"\n{'='*80}")
print(f"ğŸ“Š ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)}ê°œ ë¬¸ì„œ")
print(f"{'='*80}\n")

embeddings = UpstageEmbeddings(
  api_key=upstage_api_key,
  model="solar-embedding-1-large-passage"  # ë¬¸ì„œ ì„ë² ë”©ìš© ëª¨ë¸
) # Upstage API í‚¤ ì‚¬ìš©

print("ğŸ”„ Upstage APIë¡œ ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
dense_doc_vectors = np.array(embeddings.embed_documents(texts))  # ë¬¸ì„œ ì„ë² ë”©
print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ! {len(dense_doc_vectors)}ê°œ ë²¡í„° ìƒì„±ë¨\n")

# Pineconeì— ë¬¸ì„œ ì„ë² ë”© ì €ì¥ (ë¬¸ì„œ í…ìŠ¤íŠ¸ì™€ URL, ë‚ ì§œë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨)
print(f"{'='*80}")
print(f"ğŸ“¤ Pinecone ì—…ë¡œë“œ ì‹œì‘: {len(dense_doc_vectors)}ê°œ ë²¡í„°")
print(f"{'='*80}\n")

for i, embedding in enumerate(dense_doc_vectors):
    metadata = {
        "title": titles[i],
        "text": texts[i],
        "url": doc_urls[i],  # URL ë©”íƒ€ë°ì´í„°
        "date": doc_dates[i]  # ë‚ ì§œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    }
    index.upsert([(str(i), embedding.tolist(), metadata)])  # ë¬¸ì„œ ID, ì„ë² ë”© ë²¡í„°, ë©”íƒ€ë°ì´í„° ì¶”ê°€

    # ì§„í–‰ ìƒí™© ì¶œë ¥ (50ê°œë§ˆë‹¤)
    if (i + 1) % 50 == 0:
        progress = (i + 1) / len(dense_doc_vectors) * 100
        print(f"â³ ì§„í–‰: {i + 1}/{len(dense_doc_vectors)} ({progress:.1f}%)")

print(f"\n{'='*80}")
print(f"âœ… Pinecone ì—…ë¡œë“œ ì™„ë£Œ! ì´ {len(dense_doc_vectors)}ê°œ ë²¡í„° ì—…ë¡œë“œë¨")
print(f"{'='*80}\n")# %pip install -U langchain-community
# %pip install beautifulsoup4 requests scikit-learn pinecone-client numpy langchain-upstage faiss-cpu
# %pip install langchain
# %pip install nltk
# !pip install spacy
# !python -m spacy download ko_core_news_sm
# # í•„ìš”í•œ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# !apt-get install -y python3-dev
# !apt-get install -y libmecab-dev
# !apt-get install -y mecab mecab-ko mecab-ko-dic
# %pip install rank_bm25
# # konlpy ì„¤ì¹˜
# !pip install konlpy
# !pip install python-Levenshtein
# !pip install sentence-transformers
# !pip install pymongo

import os
import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain_upstage import UpstageEmbeddings
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from pinecone import Pinecone
from langchain_upstage import ChatUpstage
from langchain import hub
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import Document
from langchain.vectorstores import FAISS
import re
from datetime import datetime
import pytz
from langchain.schema.runnable import Runnable
from langchain.chains import RetrievalQAWithSourcesChain, RetrievalQA
from langchain.schema.runnable import RunnableSequence, RunnableMap
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from konlpy.tag import Okt
from collections import defaultdict
import Levenshtein
import numpy as np
from IPython.display import display, HTML
from rank_bm25 import BM25Okapi
from difflib import SequenceMatcher
from pymongo import MongoClient
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Pinecone API í‚¤ì™€ ì¸ë±ìŠ¤ ì´ë¦„ (.envì—ì„œ ë¡œë“œ)
pinecone_api_key = os.getenv('PINECONE_API_KEY')
index_name = os.getenv('PINECONE_INDEX_NAME', 'info')
# Upstage API í‚¤ (.envì—ì„œ ë¡œë“œ)
upstage_api_key = os.getenv('UPSTAGE_API_KEY')


# Pinecone API ì„¤ì • ë° ì´ˆê¸°í™”
pc = Pinecone(api_key=pinecone_api_key)
index = pc.Index(index_name)
def get_korean_time():
    return datetime.now(pytz.timezone('Asia/Seoul'))

# mongodb ì—°ê²°, clientë¡œ
client = MongoClient("mongodb://mongodb:27017/")

db = client["knu_chatbot"]
collection = db["notice_collection"]

####ê³µì§€ì‚¬í•­ í¬ë¡¤ë§í•˜ëŠ” ì½”ë“œ ################
def get_latest_wr_id():
    url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1"
    response = requests.get(url)
    if response.status_code == 200:
        match = re.search(r'wr_id=(\d+)', response.text)
        if match:
            return int(match.group(1))
    return None


# ìŠ¤í¬ë˜í•‘í•  URL ëª©ë¡ ìƒì„±
now_number = get_latest_wr_id()
base_url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_1&wr_id="

# ê¸°ë³¸ URL ëª©ë¡ ìƒì„±
urls = [f"{base_url}{number}" for number in range(now_number, 27726, -1)]

# ì¶”ê°€ë¡œ í•„ìš”í•œ URL ëª©ë¡
add_urls = [
    27510, 27047, 27614, 27246, 25900,
    27553, 25896, 25817, 25560, 27445,25804
]

# ì¶”ê°€ URLì„ `urls` ë¦¬ìŠ¤íŠ¸ì— í™•ì¥
urls.extend(f"{base_url}{wr_id}" for wr_id in add_urls)

# URLì—ì„œ ì œëª©, ë‚ ì§œ, ë‚´ìš©(ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URL) ì¶”ì¶œí•˜ëŠ” ê³µì§€ì‚¬í•­ í•¨ìˆ˜
def extract_text_and_date_from_url(urls):
    all_data = []

    def fetch_text_and_date(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('span', class_='bo_v_tit')
            title = title_element.get_text(strip=True) if title_element else "Unknown Title"

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URLì„ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            text_content = "Unknown Content"  # í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            image_content = []  # ì´ë¯¸ì§€ URLì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            paragraphs = soup.find('div', id='bo_v_con')
            if paragraphs:
                # paragraphs ë‚´ë¶€ì—ì„œ 'p', 'div', 'li' íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = "\n".join([element.get_text(strip=True) for element in paragraphs.find_all(['p', 'div', 'li'])])
                #print(text_content)
                if text_content.strip() == "":
                    text_content = ""
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                for img in paragraphs.find_all('img'):
                    img_src = img.get('src')
                    if img_src:
                        image_content.append(img_src)

            # ë‚ ì§œ ì¶”ì¶œ
            date_element = soup.select_one("strong.if_date")  # ìˆ˜ì •ëœ ì„ íƒì
            date = date_element.get_text(strip=True) if date_element else "Unknown Date"

            # ì œëª©ì´ Unknown Titleì´ ì•„ë‹ ë•Œë§Œ ë°ì´í„° ì¶”ê°€
            if title != "Unknown Title":
                return title, text_content, image_content, date, url  # ë¬¸ì„œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸, ë‚ ì§œ, URL ë°˜í™˜
            else:
                return None, None, None, None, None  # ì œëª©ì´ Unknownì¼ ê²½ìš° None ë°˜í™˜
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None, None, None, None, url

    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_text_and_date, urls)

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
    all_data = [(title, text_content, image_content, date, url) for title, text_content, image_content, date, url in results if title is not None]
    return all_data

#### í¬ë¡¤ë§í•œ ê³µì§€ì‚¬í•­ ì •ë³´ document_dataì— ì €ì¥
print(f"\n{'='*80}")
print(f"ğŸŒ ê²½ë¶ëŒ€ ì»´í“¨í„°í•™ë¶€ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘")
print(f"ğŸ“‹ í¬ë¡¤ë§í•  URL ê°œìˆ˜: {len(urls)}ê°œ")
print(f"{'='*80}\n")
print("ğŸ”„ ì›¹ í¬ë¡¤ë§ ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n")

document_data = extract_text_and_date_from_url(urls)

print(f"\n{'='*80}")
print(f"âœ… ì›¹ í¬ë¡¤ë§ ì™„ë£Œ! {len(document_data)}ê°œ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ë¨")
print(f"{'='*80}\n")
################################################################################################

# í…ìŠ¤íŠ¸ ë¶„ë¦¬ê¸° ì´ˆê¸°í™”
class CharacterTextSplitter:
    def __init__(self, chunk_size=850, chunk_overlap=100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    def split_text(self, text):
        chunks = []
        if len(text) <= self.chunk_size:
            return [text]
        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
            chunk = text[i:i + self.chunk_size]
            if chunk:
                chunks.append(chunk)
        return chunks

text_splitter = CharacterTextSplitter(chunk_size=850, chunk_overlap=100)

################################################################################################

# í…ìŠ¤íŠ¸ ë¶„ë¦¬ ë° URLê³¼ ë‚ ì§œ ë§¤í•‘
texts = []
image_url=[]
titles = []
doc_urls = []
doc_dates = []

for title, doc, image, date, url in document_data:
    if isinstance(doc, str) and doc.strip():  # docê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        split_texts = text_splitter.split_text(doc)
        texts.extend(split_texts)
        titles.extend([title] * len(split_texts))  # ì œëª©ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        doc_urls.extend([url] * len(split_texts))
        doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©

        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            image_url.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            image_url.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"

    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        # í…ìŠ¤íŠ¸ëŠ” "No content"ë¡œ ì¶”ê°€
        texts.append("No content")
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image_url.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        texts.append("No content")
        image_url.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image = "No content"

    temp_data = {
        "title" : title,
        "image_url" : image
    	}
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")



######################################################################################################
########################### ì§€ê¸ˆê¹Œì§€ ê³µì§€ì‚¬í•­ ì •ë³´ ###################################################
######################################################################################################


######   ì •êµìˆ˜ì§„ì˜ ì •ë³´ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ ##########

def extract_professor_info_from_urls(urls):
    all_data = []

    def fetch_professor_info(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # êµìˆ˜ ì •ë³´ê°€ ë‹´ê¸´ ìš”ì†Œë“¤ ì„ íƒ
            professor_elements = soup.find("div", id="dr").find_all("li")

            for professor in professor_elements:
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                image_element = professor.find("div", class_="dr_img").find("img")
                image_content = image_element["src"] if image_element else "Unknown Image URL"

                # ì´ë¦„ ì¶”ì¶œ
                name_element = professor.find("div", class_="dr_txt").find("h3")
                title = name_element.get_text(strip=True) if name_element else "Unknown Name"

                # ì—°ë½ì²˜ì™€ ì´ë©”ì¼ ì¶”ì¶œ í›„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
                contact_info = professor.find("div", class_="dr_txt").find_all("dd")
                contact_number = contact_info[0].get_text(strip=True) if len(contact_info) > 0 else "Unknown Contact Number"
                email = contact_info[1].get_text(strip=True) if len(contact_info) > 1 else "Unknown Email"
                text_content = f"{title}, {contact_number}, {email}"

                # ë‚ ì§œì™€ URL ì„¤ì •
                date = "ì‘ì„±ì¼24-01-01 00:00"

                prof_url_element = professor.find("a")
                prof_url = prof_url_element["href"] if prof_url_element else "Unknown URL"

                # ê° êµìˆ˜ì˜ ì •ë³´ë¥¼ all_dataì— ì¶”ê°€
                all_data.append((title, text_content, image_content, date, prof_url))


        except Exception as e:
            print(f"Error processing {url}: {e}")

    # ThreadPoolExecutorë¥¼ ì´ìš©í•˜ì—¬ ë³‘ë ¬ í¬ë¡¤ë§
    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_professor_info, urls)

    return all_data


######   ì´ˆë¹™êµìˆ˜ì§„ì˜ ì •ë³´ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ ##########

def extract_professor_info_from_urls_2(urls):
    all_data = []

    def fetch_professor_info(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # êµìˆ˜ ì •ë³´ê°€ ë‹´ê¸´ ìš”ì†Œë“¤ ì„ íƒ
            professor_elements = soup.find("div", id="Student").find_all("li")

            for professor in professor_elements:
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                image_element = professor.find("div", class_="img").find("img")
                image_content = image_element["src"] if image_element else "Unknown Image URL"

                # ì´ë¦„ ì¶”ì¶œ
                name_element = professor.find("div", class_="cnt").find("div", class_="name")
                title = name_element.get_text(strip=True) if name_element else "Unknown Name"

                # ì—°ë½ì²˜ì™€ ì´ë©”ì¼ ì¶”ì¶œ
                contact_place = professor.find("div", class_="dep").get_text(strip=True) if professor.find("div", class_="dep") else "Unknown Contact Place"
                email_element = professor.find("dl", class_="email").find("dd").find("a")
                email = email_element.get_text(strip=True) if email_element else "Unknown Email"

                # í…ìŠ¤íŠ¸ ë‚´ìš© ì¡°í•©
                text_content = f"ì„±í•¨(ì´ë¦„):{title}, ì—°êµ¬ì‹¤(ì¥ì†Œ):{contact_place}, ì´ë©”ì¼:{email}"

                # ë‚ ì§œì™€ URL ì„¤ì •
                date = "ì‘ì„±ì¼24-01-01 00:00"
                prof_url = url

                # ê° êµìˆ˜ì˜ ì •ë³´ë¥¼ all_dataì— ì¶”ê°€
                all_data.append((title, text_content, image_content, date, prof_url))



        except Exception as e:
            print(f"Error processing {url}: {e}")

    # ThreadPoolExecutorë¥¼ ì´ìš©í•˜ì—¬ ë³‘ë ¬ í¬ë¡¤ë§
    with ThreadPoolExecutor() as executor:
        executor.map(fetch_professor_info, urls)

    return all_data

######   ì§ì›ì˜ ì •ë³´ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ ##########

def extract_professor_info_from_urls_3(urls):
    all_data = []

    def fetch_professor_info(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # êµìˆ˜ ì •ë³´ê°€ ë‹´ê¸´ ìš”ì†Œë“¤ ì„ íƒ
            professor_elements = soup.find("div", id="Student").find_all("li")

            for professor in professor_elements:
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                image_element = professor.find("div", class_="img").find("img")
                image_content = image_element["src"] if image_element else "Unknown Image URL"

                # ì´ë¦„ ì¶”ì¶œ
                name_element = professor.find("div", class_="cnt").find("h1")
                title = name_element.get_text(strip=True) if name_element else "Unknown Name"

                # ì—°ë½ì²˜ ì¶”ì¶œ
                contact_number_element = professor.find("span", class_="period")
                contact_number = contact_number_element.get_text(strip=True) if contact_number_element else "Unknown Contact Number"

                # ì—°êµ¬ì‹¤ ìœ„ì¹˜ ì¶”ì¶œ
                contact_info = professor.find_all("dl", class_="dep")
                contact_place = contact_info[0].find("dd").get_text(strip=True) if len(contact_info) > 0 else "Unknown Contact Place"

                # ì´ë©”ì¼ ì¶”ì¶œ
                email = contact_info[1].find("dd").find("a").get_text(strip=True) if len(contact_info) > 1 else "Unknown Email"

                # ë‹´ë‹¹ ì—…ë¬´ ì¶”ì¶œ
                role = contact_info[2].find("dd").get_text(strip=True) if len(contact_info) > 2 else "Unknown Role"

                # í…ìŠ¤íŠ¸ ë‚´ìš© ì¡°í•©
                text_content = f"ì„±í•¨(ì´ë¦„):{title}, ì—°ë½ì²˜(ì „í™”ë²ˆí˜¸):{contact_number}, ì‚¬ë¬´ì‹¤(ì¥ì†Œ):{contact_place}, ì´ë©”ì¼:{email}, ë‹´ë‹¹ì—…ë¬´:{role}"

                # ë‚ ì§œì™€ URL ì„¤ì •
                date = "ì‘ì„±ì¼24-01-01 00:00"
                prof_url = url

                # ê° êµìˆ˜ì˜ ì •ë³´ë¥¼ all_dataì— ì¶”ê°€
                all_data.append((title, text_content, image_content, date, prof_url))



        except Exception as e:
            print(f"Error processing {url}: {e}")

    # ThreadPoolExecutorë¥¼ ì´ìš©í•˜ì—¬ ë³‘ë ¬ í¬ë¡¤ë§
    with ThreadPoolExecutor() as executor:
        executor.map(fetch_professor_info, urls)

    return all_data


# êµìˆ˜ì§„ í˜ì´ì§€ URL ëª©ë¡
urls2 = [
    "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_1&lang=kor",
]
urls3 = [
    "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_2&lang=kor",
]
urls4 = [
    "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub2_5&lang=kor",
]

prof_data = extract_professor_info_from_urls(urls2)
prof_data_2 = extract_professor_info_from_urls_2(urls3)
prof_data_3 = extract_professor_info_from_urls_3(urls4)

combined_prof_data = prof_data + prof_data_2 + prof_data_3

# êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ ë°ì´í„° ë¶„ë¦¬ ë° ì €ì¥
professor_texts = []
professor_image_urls = []
professor_titles = []
professor_doc_urls = []
professor_doc_dates = []

# prof_dataëŠ” extract_professor_info_from_urls í•¨ìˆ˜ì˜ ë°˜í™˜ê°’
for title, doc, image, date, url in combined_prof_data :
    if isinstance(doc, str) and doc.strip():  # êµìˆ˜ ì •ë³´ê°€ ë¬¸ìì—´ë¡œ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œ
        split_texts = text_splitter.split_text(doc)
        professor_texts.extend(split_texts)
        professor_titles.extend([title] * len(split_texts))  # êµìˆ˜ ì´ë¦„ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        professor_doc_urls.extend([url] * len(split_texts))
        professor_doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©

        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            professor_image_urls.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:
            professor_image_urls.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"


    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        professor_texts.append("No content")
        professor_titles.append(title)
        professor_doc_urls.append(url)
        professor_doc_dates.append(date)
        professor_image_urls.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        professor_texts.append("No content")
        professor_image_urls.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        professor_titles.append(title)
        professor_doc_urls.append(url)
        professor_doc_dates.append(date)
        image = "No content"

    temp_data = {
        "title" : title,
        "image_url" : image
    }
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")

# êµìˆ˜ ì •ë³´ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
texts.extend(professor_texts)
image_url.extend(professor_image_urls)
titles.extend(professor_titles)
doc_urls.extend(professor_doc_urls)
doc_dates.extend(professor_doc_dates)


######################################################################################################
###########################êµìˆ˜ ë° ì§ì›ì •ë³´ì„ ìœ„ëŠ”  ##################################################
######################################################################################################



####### ì·¨ì—…ì •ë³´ë¥¼ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ #######

def get_latest_wr_id_1():
    url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b"
    response = requests.get(url)
    if response.status_code == 200:
        # re.findallì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  wr_id ê°’ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        match = re.findall(r'wr_id=(\d+)', response.text)
        if match:
        # wr_ids ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ëª¨ë“  wr_id ê°’ì„ ì¶œë ¥
          max_wr_id = max(int(wr_id) for wr_id in match)
          return max_wr_id
    return None


now_company_number=get_latest_wr_id_1()

company_urls=[]
for number in range(now_company_number,1149,-1):
  company_urls.append("https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_3_b&wr_id="+str(number))


def extract_company_from_url(urls):
    all_data = []

    def fetch_text_and_date(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('span', class_='bo_v_tit')
            title = title_element.get_text(strip=True) if title_element else "Unknown Title"

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URLì„ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            text_content = "Unknown Content"  # í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            image_content = []  # ì´ë¯¸ì§€ URLì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            paragraphs = soup.find('div', id='bo_v_con')
            if paragraphs:
                # paragraphs ë‚´ë¶€ì—ì„œ 'p', 'div', 'li' íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = "\n".join([element.get_text(strip=True) for element in paragraphs.find_all(['p', 'div', 'li'])])
                #print(text_content)
                if text_content.strip() == "":
                    text_content = ""
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                for img in paragraphs.find_all('img'):
                    img_src = img.get('src')
                    if img_src:
                        image_content.append(img_src)

            # ë‚ ì§œ ì¶”ì¶œ
            date_element = soup.select_one("strong.if_date")  # ìˆ˜ì •ëœ ì„ íƒì
            date = date_element.get_text(strip=True) if date_element else "Unknown Date"

            # ì œëª©ì´ Unknown Titleì´ ì•„ë‹ ë•Œë§Œ ë°ì´í„° ì¶”ê°€
            if title != "Unknown Title":
                return title, text_content, image_content, date, url  # ë¬¸ì„œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸, ë‚ ì§œ, URL ë°˜í™˜
            else:
                return None, None, None, None, None  # ì œëª©ì´ Unknownì¼ ê²½ìš° None ë°˜í™˜
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None, None, None, None, url

    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_text_and_date, urls)

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
    all_data = [(title, text_content, image_content, date, url) for title, text_content, image_content, date, url in results if title is not None]
    return all_data

company_data= extract_company_from_url(company_urls)

for title, doc, image, date, url in company_data:
    if isinstance(doc, str) and doc.strip():  # docê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        split_texts = text_splitter.split_text(doc)
        texts.extend(split_texts)
        titles.extend([title] * len(split_texts))  # ì œëª©ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        doc_urls.extend([url] * len(split_texts))
        doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©

        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            image_url.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            image_url.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"

    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        # í…ìŠ¤íŠ¸ëŠ” "No content"ë¡œ ì¶”ê°€
        texts.append("No content")
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image_url.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        texts.append("No content")
        image_url.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        image = "No content"
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)

    temp_data = {
        "title" : title,
        "image_url" : image
    }
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")

######################################################################################################
###########################ì·¨ì—…ì •ë³´ì„ ìœ„ëŠ”  ##########################################################
######################################################################################################



def get_latest_wr_id_2():
    url = "https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4"
    response = requests.get(url)
    if response.status_code == 200:
        # re.findallì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  wr_id ê°’ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        match = re.findall(r'wr_id=(\d+)', response.text)
        if match:
        # wr_ids ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ëª¨ë“  wr_id ê°’ì„ ì¶œë ¥
          max_wr_id = max(int(wr_id) for wr_id in match)
          return max_wr_id
    return None


now_seminar_number=get_latest_wr_id_2()

seminar_urls=[]
for number in range(now_seminar_number,246,-1):
  seminar_urls.append("https://cse.knu.ac.kr/bbs/board.php?bo_table=sub5_4&wr_id="+str(number))


def extract_seminar_from_url(urls):
    all_data = []

    def fetch_text_and_date(url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('span', class_='bo_v_tit')
            title = title_element.get_text(strip=True) if title_element else "Unknown Title"

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ URLì„ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            text_content = "Unknown Content"  # í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            image_content = []  # ì´ë¯¸ì§€ URLì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            paragraphs = soup.find('div', id='bo_v_con')
            if paragraphs:
                # paragraphs ë‚´ë¶€ì—ì„œ 'p', 'div', 'li' íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = "\n".join([element.get_text(strip=True) for element in paragraphs.find_all(['p', 'div', 'li'])])
                #print(text_content)
                if text_content.strip() == "":
                    text_content = ""
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                for img in paragraphs.find_all('img'):
                    img_src = img.get('src')
                    if img_src:
                        image_content.append(img_src)

            # ë‚ ì§œ ì¶”ì¶œ
            date_element = soup.select_one("strong.if_date")  # ìˆ˜ì •ëœ ì„ íƒì
            date = date_element.get_text(strip=True) if date_element else "Unknown Date"

            # ì œëª©ì´ Unknown Titleì´ ì•„ë‹ ë•Œë§Œ ë°ì´í„° ì¶”ê°€
            if title != "Unknown Title":
                return title, text_content, image_content, date, url  # ë¬¸ì„œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸, ë‚ ì§œ, URL ë°˜í™˜
            else:
                return None, None, None, None, None  # ì œëª©ì´ Unknownì¼ ê²½ìš° None ë°˜í™˜
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None, None, None, None, url

    with ThreadPoolExecutor() as executor:
        results = executor.map(fetch_text_and_date, urls)

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
    all_data = [(title, text_content, image_content, date, url) for title, text_content, image_content, date, url in results if title is not None]
    return all_data

seminar_data= extract_seminar_from_url(seminar_urls)



for title, doc, image, date, url in seminar_data:
    if isinstance(doc, str) and doc.strip():  # docê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        split_texts = text_splitter.split_text(doc)
        texts.extend(split_texts)
        titles.extend([title] * len(split_texts))  # ì œëª©ì„ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì¶”ê°€
        doc_urls.extend([url] * len(split_texts))
        doc_dates.extend([date] * len(split_texts))  # ë¶„ë¦¬ëœ ê° í…ìŠ¤íŠ¸ì— ë™ì¼í•œ ë‚ ì§œ ì ìš©

        # ì´ë¯¸ì§€ URLë„ ì €ì¥
        if image:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆì§€ ì•Šì€ ê²½ìš°
            image_url.extend([image] * len(split_texts))  # ë™ì¼í•œ ê¸¸ì´ë¡œ ì´ë¯¸ì§€ URL ì¶”ê°€
        else:  # ì´ë¯¸ì§€ URLì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            image_url.extend(["No content"] * len(split_texts))  # "No content" ì¶”ê°€
            image = "No content"
    elif image:  # docê°€ ë¹„ì–´ ìˆê³  ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
        # í…ìŠ¤íŠ¸ëŠ” "No content"ë¡œ ì¶”ê°€
        texts.append("No content")
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image_url.append(image)  # ì´ë¯¸ì§€ URL ì¶”ê°€

    else:  # docì™€ imageê°€ ëª¨ë‘ ë¹„ì–´ ìˆëŠ” ê²½ìš°
        texts.append("No content")
        image_url.append("No content")  # ì´ë¯¸ì§€ë„ "No content"ë¡œ ì¶”ê°€
        titles.append(title)
        doc_urls.append(url)
        doc_dates.append(date)
        image = "No content"

    temp_data = {
        "title" : title,
        "image_url" : image
    }
    if not collection.find_one(temp_data):
        collection.insert_one(temp_data)
        print("Document inserted.")
    else:
        print("Duplicate document. Skipping insertion.")

######################################################################################################
############################ì„¸ë¯¸ë‚˜ì„ ìœ„ëŠ”  ##########################################################
######################################################################################################

# ë°‘ì— ì½”ë“œëŠ” ì´ˆê¸°ì— í•œ ë²ˆë§Œ ëŒë¦¼. ì‹¤ì œ ì„œë²„ ëŒë¦´ ë•ŒëŠ” ì‚¬ìš© X
# Dense Retrieval (Upstage ì„ë² ë”©)
print(f"\n{'='*80}")
print(f"ğŸ“Š ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)}ê°œ ë¬¸ì„œ")
print(f"{'='*80}\n")

embeddings = UpstageEmbeddings(
  api_key=upstage_api_key,
  model="solar-embedding-1-large-passage"  # ë¬¸ì„œ ì„ë² ë”©ìš© ëª¨ë¸
) # Upstage API í‚¤ ì‚¬ìš©

print("ğŸ”„ Upstage APIë¡œ ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
dense_doc_vectors = np.array(embeddings.embed_documents(texts))  # ë¬¸ì„œ ì„ë² ë”©
print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ! {len(dense_doc_vectors)}ê°œ ë²¡í„° ìƒì„±ë¨\n")

# Pineconeì— ë¬¸ì„œ ì„ë² ë”© ì €ì¥ (ë¬¸ì„œ í…ìŠ¤íŠ¸ì™€ URL, ë‚ ì§œë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨)
print(f"{'='*80}")
print(f"ğŸ“¤ Pinecone ì—…ë¡œë“œ ì‹œì‘: {len(dense_doc_vectors)}ê°œ ë²¡í„°")
print(f"{'='*80}\n")

for i, embedding in enumerate(dense_doc_vectors):
    metadata = {
        "title": titles[i],
        "text": texts[i],
        "url": doc_urls[i],  # URL ë©”íƒ€ë°ì´í„°
        "date": doc_dates[i]  # ë‚ ì§œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    }
    index.upsert([(str(i), embedding.tolist(), metadata)])  # ë¬¸ì„œ ID, ì„ë² ë”© ë²¡í„°, ë©”íƒ€ë°ì´í„° ì¶”ê°€

    # ì§„í–‰ ìƒí™© ì¶œë ¥ (50ê°œë§ˆë‹¤)
    if (i + 1) % 50 == 0:
        progress = (i + 1) / len(dense_doc_vectors) * 100
        print(f"â³ ì§„í–‰: {i + 1}/{len(dense_doc_vectors)} ({progress:.1f}%)")

print(f"\n{'='*80}")
print(f"âœ… Pinecone ì—…ë¡œë“œ ì™„ë£Œ! ì´ {len(dense_doc_vectors)}ê°œ ë²¡í„° ì—…ë¡œë“œë¨")
print(f"{'='*80}\n")
