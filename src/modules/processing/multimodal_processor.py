"""
ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ì²˜ë¦¬
ì´ë¯¸ì§€, ì²¨ë¶€íŒŒì¼ ë“± ë¹„í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ RAGì— í™œìš© ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import logging
import hashlib
import requests
from typing import List, Tuple, Dict, Optional
from pymongo import MongoClient
from config import CrawlerConfig
from processing.upstage_client import UpstageClient

logger = logging.getLogger(__name__)


class CharacterTextSplitter:
    """
    í…ìŠ¤íŠ¸ ë¶„í• ê¸°

    ê¸´ í…ìŠ¤íŠ¸ë¥¼ chunk_size ë‹¨ìœ„ë¡œ ë¶„í• í•˜ë©°,
    chunk_overlap ë§Œí¼ ê²¹ì¹˜ë„ë¡ ë¶„í• 
    """

    def __init__(self, chunk_size: int = 850, chunk_overlap: int = 100):
        """
        Args:
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ ë¶„í• 

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸

        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []

        if len(text) <= self.chunk_size:
            return [text]

        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
            chunk = text[i:i + self.chunk_size]
            if chunk:
                chunks.append(chunk)

        return chunks


class MultimodalContent:
    """
    ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ë°ì´í„° í´ë˜ìŠ¤

    í•˜ë‚˜ì˜ ê²Œì‹œê¸€ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  ì½˜í…ì¸ ë¥¼ ë‹´ëŠ” ì»¨í…Œì´ë„ˆ
    """

    def __init__(self, title: str, url: str, date: str):
        self.title = title
        self.url = url
        self.date = date

        # í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
        self.text_chunks: List[str] = []

        # ì´ë¯¸ì§€ ì½˜í…ì¸ 
        self.image_contents: List[Dict] = []
        # [{"url": "...", "ocr_text": "...", "description": "..."}]

        # ì²¨ë¶€íŒŒì¼ ì½˜í…ì¸ 
        self.attachment_contents: List[Dict] = []
        # [{"url": "...", "type": "pdf", "text": "..."}]

    def add_text_chunk(self, text: str):
        """í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ê°€"""
        if text and text.strip():
            self.text_chunks.append(text)

    def add_image_content(self, url: str, ocr_text: str = "", ocr_html: str = "", ocr_elements: List = None, description: str = ""):
        """ì´ë¯¸ì§€ ì½˜í…ì¸  ì¶”ê°€ (HTML êµ¬ì¡° í¬í•¨)"""
        self.image_contents.append({
            "url": url,
            "ocr_text": ocr_text,
            "ocr_html": ocr_html,  # HTML êµ¬ì¡° (í‘œ, ë ˆì´ì•„ì›ƒ ë“±)
            "ocr_elements": ocr_elements or [],  # ìš”ì†Œ ì •ë³´
            "description": description
        })

    def add_attachment_content(self, url: str, file_type: str, text: str, html: str = "", elements: List = None):
        """ì²¨ë¶€íŒŒì¼ ì½˜í…ì¸  ì¶”ê°€ (HTML êµ¬ì¡° í¬í•¨)"""
        self.attachment_contents.append({
            "url": url,
            "type": file_type,
            "text": text,
            "html": html,  # HTML êµ¬ì¡° (í‘œ, ë ˆì´ì•„ì›ƒ ë“±)
            "elements": elements or []  # ìš”ì†Œ ì •ë³´
        })

    def to_embedding_items(self) -> List[Tuple[str, Dict]]:
        """
        ì„ë² ë”©í•  í•­ëª©ë“¤ë¡œ ë³€í™˜ (ì²­í‚¹ í¬í•¨)

        Returns:
            [(text, metadata), ...]
        """
        items = []

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” (ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤: 850ì ì²­í‚¹)
        text_splitter = CharacterTextSplitter(
            chunk_size=CrawlerConfig.CHUNK_SIZE,
            chunk_overlap=CrawlerConfig.CHUNK_OVERLAP
        )

        # 1. í…ìŠ¤íŠ¸ ì²­í¬ (ì´ë¯¸ ì²­í‚¹ë˜ì–´ ìˆìŒ)
        total_text_chunks = len(self.text_chunks)
        for idx, chunk in enumerate(self.text_chunks):
            items.append((
                chunk,
                {
                    "title": self.title,
                    "url": self.url,
                    "date": self.date,
                    "content_type": "text",
                    "chunk_index": idx,
                    "total_chunks": total_text_chunks,
                    "source": "original_post"  # ì›ë³¸ ê²Œì‹œê¸€
                }
            ))

        # 2. ì´ë¯¸ì§€ OCR ê²°ê³¼ (ğŸ”§ ì²­í‚¹ ì¶”ê°€!)
        for idx, img in enumerate(self.image_contents):
            if img["ocr_text"]:
                # OCR í…ìŠ¤íŠ¸ ì¤€ë¹„
                combined_text = f"[ì´ë¯¸ì§€ í…ìŠ¤íŠ¸]\n{img['ocr_text']}"

                # ì„¤ëª…ë„ ìˆìœ¼ë©´ ì¶”ê°€
                if img["description"]:
                    combined_text += f"\n\n[ì´ë¯¸ì§€ ì„¤ëª…]\n{img['description']}"

                # âœ… ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í‚¹! (ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤)
                if len(combined_text) > CrawlerConfig.CHUNK_SIZE:
                    chunks = text_splitter.split_text(combined_text)
                    for chunk_idx, chunk in enumerate(chunks):
                        items.append((
                            chunk,
                            {
                                "title": self.title,
                                "url": self.url,
                                "date": self.date,
                                "content_type": "image",
                                "image_url": img["url"],
                                "image_index": idx,
                                "chunk_index": chunk_idx,
                                "total_chunks": len(chunks),
                                "source": "image_ocr",  # OCR ê²°ê³¼
                                "html": img.get("ocr_html", ""),
                                "html_available": bool(img.get("ocr_html"))
                            }
                        ))
                else:
                    # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ
                    items.append((
                        combined_text,
                        {
                            "title": self.title,
                            "url": self.url,
                            "date": self.date,
                            "content_type": "image",
                            "image_url": img["url"],
                            "image_index": idx,
                            "source": "image_ocr",
                            "html": img.get("ocr_html", ""),
                            "html_available": bool(img.get("ocr_html"))
                        }
                    ))

        # 3. ì²¨ë¶€íŒŒì¼ ë‚´ìš© (ğŸ”§ ì²­í‚¹ ì¶”ê°€!)
        for idx, att in enumerate(self.attachment_contents):
            if att["text"]:
                full_text = f"[ì²¨ë¶€íŒŒì¼: {att['type'].upper()}]\n{att['text']}"

                # âœ… ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í‚¹! (ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤)
                if len(full_text) > CrawlerConfig.CHUNK_SIZE:
                    chunks = text_splitter.split_text(full_text)
                    for chunk_idx, chunk in enumerate(chunks):
                        items.append((
                            chunk,
                            {
                                "title": self.title,
                                "url": self.url,
                                "date": self.date,
                                "content_type": "attachment",
                                "attachment_url": att["url"],
                                "attachment_type": att["type"],
                                "attachment_index": idx,
                                "chunk_index": chunk_idx,
                                "total_chunks": len(chunks),
                                "source": "document_parse",  # Document Parse ê²°ê³¼
                                "html": att.get("html", ""),
                                "html_available": bool(att.get("html"))
                            }
                        ))
                else:
                    # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ
                    items.append((
                        full_text,
                        {
                            "title": self.title,
                            "url": self.url,
                            "date": self.date,
                            "content_type": "attachment",
                            "attachment_url": att["url"],
                            "attachment_type": att["type"],
                            "attachment_index": idx,
                            "source": "document_parse",
                            "html": att.get("html", ""),
                            "html_available": bool(att.get("html"))
                        }
                    ))

        return items


class MultimodalProcessor:
    """
    ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  í”„ë¡œì„¸ì„œ

    ì—­í• :
    - ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)
    - ì²¨ë¶€íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Document Parse)
    - ì²˜ë¦¬ëœ ì½˜í…ì¸ ë¥¼ RAGìš©ìœ¼ë¡œ ë³€í™˜
    """

    def __init__(
        self,
        upstage_api_key: Optional[str] = None,
        mongo_client: Optional[MongoClient] = None,
        enable_image_processing: bool = True,
        enable_attachment_processing: bool = True
    ):
        """
        Args:
            upstage_api_key: Upstage API í‚¤
            mongo_client: MongoDB í´ë¼ì´ì–¸íŠ¸ (ì²˜ë¦¬ ì´ë ¥ ì €ì¥ìš©)
            enable_image_processing: ì´ë¯¸ì§€ ì²˜ë¦¬ í™œì„±í™”
            enable_attachment_processing: ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ í™œì„±í™”
        """
        self.upstage_client = UpstageClient(api_key=upstage_api_key)
        self.enable_image = enable_image_processing
        self.enable_attachment = enable_attachment_processing

        # MongoDB ì—°ê²° (ì²˜ë¦¬ ì´ë ¥ ìºì‹œìš©)
        if mongo_client is None:
            mongo_client = MongoClient(CrawlerConfig.MONGODB_URI)

        self.client = mongo_client
        self.db = self.client[CrawlerConfig.MONGODB_DATABASE]
        self.cache_collection = self.db["multimodal_cache"]

        # ìºì‹œ ì¸ë±ìŠ¤ ìƒì„±
        self.cache_collection.create_index("url", unique=True)
        self.cache_collection.create_index("file_hash")  # íŒŒì¼ í•´ì‹œ ì¸ë±ìŠ¤ (ì¤‘ë³µ ì´ë¯¸ì§€ ê°ì§€ìš©)

        logger.info(f"MultimodalProcessor ì´ˆê¸°í™” - ì´ë¯¸ì§€: {self.enable_image}, ì²¨ë¶€íŒŒì¼: {self.enable_attachment}")

    def process_images(self, image_urls: List[str], logger=None, category: str = "notice") -> Dict:
        """
        ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (OCR)

        Args:
            image_urls: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
            logger: ì»¤ìŠ¤í…€ ë¡œê±°
            category: ì¹´í…Œê³ ë¦¬

        Returns:
            {
                "successful": [{"url": "...", "ocr_text": "..."}],
                "failed": [{"url": "...", "reason": "..."}],
                "unsupported": [{"url": "...", "reason": "..."}],
                "total": N
            }
        """
        if not self.enable_image or not image_urls:
            return {"successful": [], "failed": [], "unsupported": [], "total": 0}

        successful = []
        failed = []
        unsupported = []

        for img_url in image_urls:
            try:
                # 1. URL ê¸°ë°˜ ìºì‹œ í™•ì¸ (ë¹ ë¥¸ ê²½ë¡œ)
                cached = self._get_from_cache(img_url)
                if cached:
                    # ìºì‹œì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì— url í‚¤ ì¶”ê°€ (ìºì‹œ ë©”ì„œë“œì—ì„œ ì œê±°ë˜ë¯€ë¡œ)
                    cached["url"] = img_url

                    # ìºì‹œëœ ë°ì´í„°ì˜ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                    if cached.get('ocr_text'):
                        successful.append(cached)
                        if logger:
                            logger.log_multimodal_detail(
                                "ì´ë¯¸ì§€ OCR (URL ìºì‹œ)",
                                img_url[:50] + "..." if len(img_url) > 50 else img_url,
                                success=True,
                                detail=f"{len(cached.get('ocr_text', ''))}ì"
                            )
                    continue

                # 2. íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í•´ì‹œ ê³„ì‚°
                file_data = self._download_file(img_url)
                if not file_data:
                    failed.append({
                        "url": img_url,
                        "reason": "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
                    })
                    if logger:
                        url_display = img_url[:50] + "..." if len(img_url) > 50 else img_url
                        logger.log_multimodal_detail(
                            "ì´ë¯¸ì§€ OCR",
                            url_display,
                            success=False,
                            detail="ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
                        )
                    continue

                file_hash = self._calculate_file_hash(file_data)

                # 3. íŒŒì¼ í•´ì‹œ ê¸°ë°˜ ìºì‹œ í™•ì¸ (ì¤‘ë³µ ì´ë¯¸ì§€ ê°ì§€)
                cached_by_hash = self._get_from_cache_by_file_hash(file_hash)
                if cached_by_hash:
                    # ì¤‘ë³µ ì´ë¯¸ì§€ ë°œê²¬! OCR ìƒëµ
                    content = {
                        "url": img_url,
                        "ocr_text": cached_by_hash.get("ocr_text", ""),
                        "description": cached_by_hash.get("description", "")
                    }
                    successful.append(content)
                    # í˜„ì¬ URLë„ ìºì‹œì— ì¶”ê°€ (ë¹ ë¥¸ ì¡°íšŒìš©)
                    self._save_to_cache(img_url, content, file_hash=file_hash)

                    if logger:
                        url_display = img_url[:50] + "..." if len(img_url) > 50 else img_url
                        logger.log_multimodal_detail(
                            "ì´ë¯¸ì§€ OCR (íŒŒì¼ í•´ì‹œ ìºì‹œ)",
                            url_display,
                            success=True,
                            detail=f"ì¤‘ë³µ ì´ë¯¸ì§€ - {len(content['ocr_text'])}ì"
                        )
                    else:
                        # ë¡œê±° ì—†ì„ ë•Œ ì½˜ì†” ì¶œë ¥
                        print(f"â„¹ï¸  ì¤‘ë³µ ì´ë¯¸ì§€ ê°ì§€ (íŒŒì¼ í•´ì‹œ): OCR ìƒëµ - {len(content['ocr_text'])}ì")
                    continue

                # 4. ìƒˆ ì´ë¯¸ì§€ â†’ Upstage OCR API í˜¸ì¶œ
                ocr_result = self.upstage_client.extract_text_from_image_url(img_url)

                if ocr_result:
                    text_length = len(ocr_result.get("text", ""))

                    if text_length > 0:
                        # ì„±ê³µ: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (HTML êµ¬ì¡°ë„ í•¨ê»˜ ì €ì¥)
                        content = {
                            "url": img_url,
                            "ocr_text": ocr_result.get("text", ""),
                            "ocr_html": ocr_result.get("html", ""),  # HTML êµ¬ì¡° ë³´ì¡´ (í‘œ, ë ˆì´ì•„ì›ƒ ë“±)
                            "ocr_elements": ocr_result.get("elements", []),  # ìš”ì†Œ ì •ë³´
                            "description": ""
                        }
                        successful.append(content)
                        self._save_to_cache(img_url, content, file_hash=file_hash)

                        if logger:
                            url_display = img_url[:50] + "..." if len(img_url) > 50 else img_url
                            logger.log_multimodal_detail(
                                "ì´ë¯¸ì§€ OCR",
                                url_display,
                                success=True,
                                detail=f"{text_length}ì ì¶”ì¶œ"
                            )
                    else:
                        # ì‹¤íŒ¨: APIëŠ” ì‘ë‹µí–ˆì§€ë§Œ í…ìŠ¤íŠ¸ ì—†ìŒ
                        failed.append({
                            "url": img_url,
                            "reason": "ë¹ˆ ì´ë¯¸ì§€ ë˜ëŠ” í…ìŠ¤íŠ¸ ì¸ì‹ ì‹¤íŒ¨"
                        })
                        if logger:
                            url_display = img_url[:50] + "..." if len(img_url) > 50 else img_url
                            logger.log_multimodal_detail(
                                "ì´ë¯¸ì§€ OCR",
                                url_display,
                                success=False,
                                detail="í…ìŠ¤íŠ¸ ì—†ìŒ (ì¸ì‹ ì‹¤íŒ¨)"
                            )
                else:
                    # ì‹¤íŒ¨: API í˜¸ì¶œ ìì²´ê°€ ì‹¤íŒ¨
                    failed.append({
                        "url": img_url,
                        "reason": "API í˜¸ì¶œ ì‹¤íŒ¨"
                    })
                    if logger:
                        url_display = img_url[:50] + "..." if len(img_url) > 50 else img_url
                        logger.log_multimodal_detail(
                            "ì´ë¯¸ì§€ OCR",
                            url_display,
                            success=False,
                            detail="API í˜¸ì¶œ ì‹¤íŒ¨"
                        )

            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ: ì‹¤íŒ¨ë¡œ ë¶„ë¥˜
                error_msg = str(e)

                # ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì¸ì§€ í™•ì¸
                if "ì§€ì›í•˜ì§€ ì•ŠëŠ”" in error_msg or "unsupported" in error_msg.lower():
                    unsupported.append({
                        "url": img_url,
                        "reason": error_msg
                    })
                else:
                    failed.append({
                        "url": img_url,
                        "reason": error_msg
                    })

                if logger:
                    url_display = img_url[:50] + "..." if len(img_url) > 50 else img_url
                    logger.log_multimodal_detail(
                        "ì´ë¯¸ì§€ OCR",
                        url_display,
                        success=False,
                        detail=error_msg[:100]
                    )

        return {
            "successful": successful,
            "failed": failed,
            "unsupported": unsupported,
            "total": len(image_urls)
        }

    def process_attachments(self, attachment_urls: List[str], logger=None, category: str = "notice") -> Dict:
        """
        ì²¨ë¶€íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (Document Parse ë˜ëŠ” OCR)

        ì´ë¯¸ì§€ í™•ì¥ì ì²¨ë¶€íŒŒì¼ì€ OCRë¡œ ì²˜ë¦¬í•˜ê³ ,
        ë¬¸ì„œ í™•ì¥ìëŠ” Document Parseë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            attachment_urls: ì²¨ë¶€íŒŒì¼ URL ë¦¬ìŠ¤íŠ¸
            logger: ì»¤ìŠ¤í…€ ë¡œê±°
            category: ì¹´í…Œê³ ë¦¬

        Returns:
            {
                "successful": [{"url": "...", "type": "pdf", "text": "..."}],
                "failed": [{"url": "...", "reason": "..."}],
                "unsupported": [{"url": "...", "reason": "..."}],
                "total": N
            }
        """
        if not self.enable_attachment or not attachment_urls:
            return {"successful": [], "failed": [], "unsupported": [], "total": 0}

        successful = []
        failed = []
        unsupported = []

        for att_url in attachment_urls:
            # ğŸ”§ ZIP íŒŒì¼ ì²˜ë¦¬ (ì••ì¶• í•´ì œ í›„ ê°œë³„ íŒŒì¼ íŒŒì‹±)
            file_ext = Path(att_url).suffix.lower()
            if file_ext == '.zip':
                try:
                    if logger:
                        url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                        logger.log_multimodal_detail(
                            "ZIP íŒŒì¼ ì²˜ë¦¬",
                            url_display,
                            success=True,
                            detail="ì••ì¶• í•´ì œ ì¤‘..."
                        )

                    # ZIP íŒŒì¼ ì²˜ë¦¬
                    zip_result = self.upstage_client.process_zip_from_url(att_url)

                    # ì„±ê³µí•œ íŒŒì¼ë“¤ ì¶”ê°€
                    for item in zip_result["successful"]:
                        # ZIP ë‚´ë¶€ íŒŒì¼ì€ ë³„ë„ URLë¡œ êµ¬ë¶„
                        content = {
                            "url": f"{att_url}#{item['filename']}",  # ZIP#íŒŒì¼ëª…
                            "type": item["type"],
                            "text": item["text"],
                            "html": item.get("html", ""),
                            "from_zip": True,
                            "zip_url": att_url
                        }
                        successful.append(content)

                        # ìºì‹œ ì €ì¥ (ZIP ë‚´ë¶€ íŒŒì¼ë„ ìºì‹±)
                        self._save_to_cache(
                            content["url"],
                            {
                                "text": item["text"],
                                "html": item.get("html", ""),
                                "type": item["type"],
                                "from_zip": True
                            }
                        )

                    # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ê¸°ë¡
                    for item in zip_result["failed"]:
                        failed.append({
                            "url": f"{att_url}#{item['filename']}",
                            "reason": item["reason"]
                        })

                    if logger:
                        logger.log_multimodal_detail(
                            "ZIP íŒŒì¼ ì²˜ë¦¬",
                            url_display,
                            success=True,
                            detail=f"ì„±ê³µ {len(zip_result['successful'])}ê°œ, ì‹¤íŒ¨ {len(zip_result['failed'])}ê°œ"
                        )

                except Exception as e:
                    error_msg = str(e)
                    failed.append({
                        "url": att_url,
                        "reason": error_msg
                    })

                    if logger:
                        url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                        logger.log_multimodal_detail(
                            "ZIP íŒŒì¼ ì²˜ë¦¬",
                            url_display,
                            success=False,
                            detail=error_msg[:100]
                        )

                # ZIP íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ, ë‹¤ìŒ ì²¨ë¶€íŒŒì¼ë¡œ
                continue

            # ì´ë¯¸ì§€ í™•ì¥ì í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ê´€)
            is_image = self.upstage_client.is_image_url(att_url)

            # ì´ë¯¸ì§€ ì²¨ë¶€íŒŒì¼ì€ OCRë¡œ ì²˜ë¦¬
            if is_image:
                # ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬ (process_images ë¡œì§ê³¼ ë™ì¼)
                try:
                    # 1. URL ê¸°ë°˜ ìºì‹œ í™•ì¸
                    cached = self._get_from_cache(att_url)
                    if cached:
                        cached["url"] = att_url
                        if cached.get('text') or cached.get('ocr_text'):
                            # ì´ë¯¸ì§€ëŠ” ocr_text í‚¤ ì‚¬ìš©, ì²¨ë¶€íŒŒì¼ì€ text í‚¤ ì‚¬ìš©
                            text = cached.get('text') or cached.get('ocr_text', '')
                            content = {
                                "url": att_url,
                                "type": "image",
                                "text": text
                            }
                            successful.append(content)
                            if logger:
                                url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                                logger.log_multimodal_detail(
                                    "ì´ë¯¸ì§€ ì²¨ë¶€ OCR (URL ìºì‹œ)",
                                    url_display,
                                    success=True,
                                    detail=f"ì´ë¯¸ì§€ - {len(text)}ì"
                                )
                        continue

                    # 2. íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í•´ì‹œ ê³„ì‚°
                    file_data = self._download_file(att_url)
                    if not file_data:
                        failed.append({
                            "url": att_url,
                            "reason": "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
                        })
                        if logger:
                            url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                            logger.log_multimodal_detail(
                                "ì´ë¯¸ì§€ ì²¨ë¶€ OCR",
                                url_display,
                                success=False,
                                detail="ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
                            )
                        continue

                    file_hash = self._calculate_file_hash(file_data)

                    # 3. íŒŒì¼ í•´ì‹œ ê¸°ë°˜ ìºì‹œ í™•ì¸ (ì¤‘ë³µ ì´ë¯¸ì§€ ê°ì§€)
                    cached_by_hash = self._get_from_cache_by_file_hash(file_hash)
                    if cached_by_hash:
                        # ì¤‘ë³µ ì´ë¯¸ì§€ ë°œê²¬! OCR ìƒëµ
                        text = cached_by_hash.get("ocr_text") or cached_by_hash.get("text", "")
                        content = {
                            "url": att_url,
                            "type": "image",
                            "text": text
                        }
                        successful.append(content)
                        # í˜„ì¬ URLë„ ìºì‹œì— ì¶”ê°€
                        self._save_to_cache(att_url, {"ocr_text": text, "type": "image"}, file_hash=file_hash)

                        if logger:
                            url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                            logger.log_multimodal_detail(
                                "ì´ë¯¸ì§€ ì²¨ë¶€ OCR (íŒŒì¼ í•´ì‹œ ìºì‹œ)",
                                url_display,
                                success=True,
                                detail=f"ì¤‘ë³µ ì´ë¯¸ì§€ - {len(text)}ì"
                            )
                        else:
                            print(f"â„¹ï¸  ì¤‘ë³µ ì´ë¯¸ì§€ ê°ì§€ (íŒŒì¼ í•´ì‹œ): OCR ìƒëµ - {len(text)}ì")
                        continue

                    # 4. ìƒˆ ì´ë¯¸ì§€ â†’ OCR API í˜¸ì¶œ
                    ocr_result = self.upstage_client.extract_text_from_image_url(att_url)

                    if ocr_result and ocr_result.get("text"):
                        text = ocr_result.get("text", "")
                        html = ocr_result.get("html", "")
                        elements = ocr_result.get("elements", [])

                        content = {
                            "url": att_url,
                            "type": "image",
                            "text": text,
                            "html": html,  # HTML êµ¬ì¡° ë³´ì¡´ (í‘œ, ë ˆì´ì•„ì›ƒ ë“±)
                            "elements": elements  # ìš”ì†Œ ì •ë³´
                        }
                        successful.append(content)
                        self._save_to_cache(att_url, {
                            "ocr_text": text,
                            "ocr_html": html,
                            "ocr_elements": elements,
                            "type": "image"
                        }, file_hash=file_hash)

                        if logger:
                            url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                            logger.log_multimodal_detail(
                                "ì´ë¯¸ì§€ ì²¨ë¶€ OCR",
                                url_display,
                                success=True,
                                detail=f"ì´ë¯¸ì§€ - {len(text)}ì ì¶”ì¶œ"
                            )
                    else:
                        failed.append({
                            "url": att_url,
                            "reason": "OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"
                        })
                        if logger:
                            url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                            logger.log_multimodal_detail(
                                "ì´ë¯¸ì§€ ì²¨ë¶€ OCR",
                                url_display,
                                success=False,
                                detail="í…ìŠ¤íŠ¸ ì—†ìŒ"
                            )

                except Exception as e:
                    error_msg = str(e)
                    if "ì§€ì›í•˜ì§€ ì•ŠëŠ”" in error_msg or "unsupported" in error_msg.lower():
                        unsupported.append({
                            "url": att_url,
                            "reason": error_msg
                        })
                    else:
                        failed.append({
                            "url": att_url,
                            "reason": error_msg
                        })

                    if logger:
                        url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                        logger.log_multimodal_detail(
                            "ì´ë¯¸ì§€ ì²¨ë¶€ OCR",
                            url_display,
                            success=False,
                            detail=error_msg[:100]
                        )

                # ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ, ë‹¤ìŒ ì²¨ë¶€íŒŒì¼ë¡œ
                continue

            # ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬ (PDF, DOCX, HWP ë“±)
            try:
                # íŒŒì¼ íƒ€ì… í™•ì¸ (download.php ê°™ì€ ë™ì  URLì€ Content-Typeìœ¼ë¡œ í™•ì¸)
                # is_document_urlì€ í™•ì¥ì ì²´í¬ì´ë¯€ë¡œ ì¼ë‹¨ ì‹œë„
                # upstage_clientì—ì„œ Content-Type ê¸°ë°˜ ì²´í¬í•¨

                # ìºì‹œ í™•ì¸
                cached = self._get_from_cache(att_url)
                if cached:
                    # ìºì‹œì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì— url í‚¤ ì¶”ê°€ (ìºì‹œ ë©”ì„œë“œì—ì„œ ì œê±°ë˜ë¯€ë¡œ)
                    cached["url"] = att_url

                    # ìºì‹œëœ ë°ì´í„°ì˜ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                    if cached.get('text'):
                        successful.append(cached)
                        if logger:
                            url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                            logger.log_multimodal_detail(
                                "ë¬¸ì„œ íŒŒì‹± (ìºì‹œ)",
                                url_display,
                                success=True,
                                detail=f"{cached.get('type', 'unknown')} - {len(cached.get('text', ''))}ì"
                            )
                    continue

                # Upstage Document Parse API í˜¸ì¶œ
                parse_result = self.upstage_client.parse_document_from_url(att_url)

                if parse_result:
                    text_length = len(parse_result.get("text", ""))
                    file_type = Path(att_url).suffix.lower()[1:] if Path(att_url).suffix else "unknown"

                    if text_length > 0:
                        # ì„±ê³µ: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (HTML êµ¬ì¡°ë„ í•¨ê»˜ ì €ì¥)
                        content = {
                            "url": att_url,
                            "type": file_type,
                            "text": parse_result.get("text", ""),
                            "html": parse_result.get("html", ""),  # HTML êµ¬ì¡° ë³´ì¡´ (í‘œ, ë ˆì´ì•„ì›ƒ ë“±)
                            "elements": parse_result.get("elements", [])  # ìš”ì†Œ ì •ë³´
                        }
                        successful.append(content)
                        self._save_to_cache(att_url, content)

                        if logger:
                            url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                            logger.log_multimodal_detail(
                                "ë¬¸ì„œ íŒŒì‹±",
                                url_display,
                                success=True,
                                detail=f"{file_type} - {text_length}ì ì¶”ì¶œ"
                            )
                    else:
                        # ì‹¤íŒ¨: APIëŠ” ì‘ë‹µí–ˆì§€ë§Œ í…ìŠ¤íŠ¸ ì—†ìŒ
                        failed.append({
                            "url": att_url,
                            "reason": "ë¹ˆ ë¬¸ì„œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"
                        })
                        if logger:
                            url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                            logger.log_multimodal_detail(
                                "ë¬¸ì„œ íŒŒì‹±",
                                url_display,
                                success=False,
                                detail=f"{file_type} - í…ìŠ¤íŠ¸ ì—†ìŒ"
                            )
                else:
                    # ì‹¤íŒ¨: API í˜¸ì¶œ ìì²´ê°€ ì‹¤íŒ¨
                    failed.append({
                        "url": att_url,
                        "reason": "API í˜¸ì¶œ ì‹¤íŒ¨"
                    })
                    if logger:
                        url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                        logger.log_multimodal_detail(
                            "ë¬¸ì„œ íŒŒì‹±",
                            url_display,
                            success=False,
                            detail="API í˜¸ì¶œ ì‹¤íŒ¨"
                        )

            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ: ì‹¤íŒ¨ë¡œ ë¶„ë¥˜
                error_msg = str(e)

                # ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì¸ì§€ í™•ì¸
                if "ì§€ì›í•˜ì§€ ì•ŠëŠ”" in error_msg or "unsupported" in error_msg.lower():
                    unsupported.append({
                        "url": att_url,
                        "reason": error_msg
                    })
                else:
                    failed.append({
                        "url": att_url,
                        "reason": error_msg
                    })

                if logger:
                    url_display = att_url[:50] + "..." if len(att_url) > 50 else att_url
                    logger.log_multimodal_detail(
                        "ë¬¸ì„œ íŒŒì‹±",
                        url_display,
                        success=False,
                        detail=error_msg[:100]
                    )

        return {
            "successful": successful,
            "failed": failed,
            "unsupported": unsupported,
            "total": len(attachment_urls)
        }

    def _calculate_file_hash(self, file_data: bytes) -> str:
        """
        íŒŒì¼ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì˜ MD5 í•´ì‹œ ê³„ì‚°

        Args:
            file_data: íŒŒì¼ ë°”ì´ë„ˆë¦¬ ë°ì´í„°

        Returns:
            MD5 í•´ì‹œ ë¬¸ìì—´
        """
        return hashlib.md5(file_data).hexdigest()

    def _download_file(self, url: str) -> Optional[bytes]:
        """
        URLì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ

        Args:
            url: ë‹¤ìš´ë¡œë“œí•  URL (Data URIë„ ì§€ì›)

        Returns:
            íŒŒì¼ ë°”ì´ë„ˆë¦¬ ë°ì´í„°, ì‹¤íŒ¨ ì‹œ None
        """
        try:
            # Data URI ì²˜ë¦¬
            if url.startswith('data:'):
                import base64
                import re

                # data:image/png;base64,iVBORw0KGgo... í˜•ì‹ íŒŒì‹±
                if ';base64,' in url:
                    parts = url.split(';base64,')
                    if len(parts) == 2:
                        base64_data = parts[1]
                        return base64.b64decode(base64_data)
                return None

            # HTTP/HTTPS URL ì²˜ë¦¬
            # view_image.php ê°™ì€ í”„ë¡ì‹œ URLì„ ì‹¤ì œ ì´ë¯¸ì§€ URLë¡œ ë³€í™˜
            actual_url = url
            from urllib.parse import urlparse, parse_qs, unquote as url_unquote

            parsed = urlparse(url)

            # view_image.php?fn=... ì²˜ë¦¬
            if 'view_image.php' in parsed.path:
                query_params = parse_qs(parsed.query)
                if 'fn' in query_params:
                    fn_value = query_params['fn'][0]
                    decoded_path = url_unquote(fn_value)  # /data/editor/2511/...png

                    # ì ˆëŒ€ URLë¡œ ë³€í™˜
                    base_url = f"{parsed.scheme}://{parsed.netloc}"
                    actual_url = f"{base_url}{decoded_path}"
                    logger.info(f"ğŸ” í”„ë¡ì‹œ URL ë³€í™˜: view_image.php â†’ {decoded_path}")

            response = requests.get(actual_url, timeout=30, allow_redirects=True)
            if response.status_code == 200:
                return response.content
            return None

        except Exception as e:
            logger.warning(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜ ({url[:50]}...): {e}")
            return None

    def _get_from_cache_by_file_hash(self, file_hash: str) -> Optional[Dict]:
        """
        íŒŒì¼ í•´ì‹œë¡œ ìºì‹œ ì¡°íšŒ (ì¤‘ë³µ ì´ë¯¸ì§€ ê°ì§€)

        Args:
            file_hash: íŒŒì¼ MD5 í•´ì‹œ

        Returns:
            ìºì‹œëœ ì²˜ë¦¬ ê²°ê³¼ ë˜ëŠ” None
        """
        try:
            cached = self.cache_collection.find_one({"file_hash": file_hash})
            if cached:
                # MongoDB _id, url, file_hash ì œê±°
                cached.pop("_id", None)
                cached.pop("url", None)
                cached.pop("file_hash", None)
                return cached
        except Exception as e:
            logger.warning(f"íŒŒì¼ í•´ì‹œ ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        return None

    def _get_from_cache(self, url: str) -> Optional[Dict]:
        """ìºì‹œì—ì„œ ì²˜ë¦¬ ê²°ê³¼ ì¡°íšŒ"""
        try:
            cached = self.cache_collection.find_one({"url": url})
            if cached:
                # MongoDB _id ì œê±°
                cached.pop("_id", None)
                cached.pop("url", None)
                cached.pop("file_hash", None)
                return cached
        except Exception as e:
            logger.warning(f"ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        return None

    def _save_to_cache(self, url: str, content: Dict, file_hash: Optional[str] = None):
        """
        ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥

        Args:
            url: ì›ë³¸ URL
            content: ì²˜ë¦¬ ê²°ê³¼ (ocr_text, text ë“±)
            file_hash: íŒŒì¼ í•´ì‹œ (ì„ íƒ)
        """
        try:
            cache_data = {"url": url, **content}
            if file_hash:
                cache_data["file_hash"] = file_hash

            self.cache_collection.update_one(
                {"url": url},
                {"$set": cache_data},
                upsert=True
            )
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")

    def create_multimodal_content(
        self,
        title: str,
        url: str,
        date: str,
        text_chunks: List[str],
        image_urls: List[str],
        attachment_urls: List[str],
        category: str = "notice",
        logger=None
    ) -> Tuple[MultimodalContent, Dict]:
        """
        ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ìƒì„± (í†µí•© ì¸í„°í˜ì´ìŠ¤)

        Args:
            title: ê²Œì‹œê¸€ ì œëª©
            url: ê²Œì‹œê¸€ URL
            date: ë‚ ì§œ
            text_chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            image_urls: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
            attachment_urls: ì²¨ë¶€íŒŒì¼ URL ë¦¬ìŠ¤íŠ¸
            category: ì¹´í…Œê³ ë¦¬
            logger: ì»¤ìŠ¤í…€ ë¡œê±° (CrawlerLogger)

        Returns:
            (MultimodalContent, {"image_failures": [...], "attachment_failures": [...]})
        """
        content = MultimodalContent(title, url, date)

        # ì‹¤íŒ¨ ì •ë³´ ìˆ˜ì§‘
        failures = {
            "image_failed": [],
            "image_unsupported": [],
            "attachment_failed": [],
            "attachment_unsupported": []
        }

        # 1. í…ìŠ¤íŠ¸ ì¶”ê°€
        for chunk in text_chunks:
            content.add_text_chunk(chunk)

        # 2. ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¶”ê°€
        if image_urls:
            image_result = self.process_images(image_urls, logger=logger, category=category)
            # ì„±ê³µí•œ ì´ë¯¸ì§€ë§Œ ì¶”ê°€ (HTML êµ¬ì¡° í¬í•¨)
            for img_content in image_result["successful"]:
                content.add_image_content(
                    url=img_content["url"],
                    ocr_text=img_content.get("ocr_text", ""),
                    ocr_html=img_content.get("ocr_html", ""),  # HTML êµ¬ì¡°
                    ocr_elements=img_content.get("ocr_elements", []),  # ìš”ì†Œ ì •ë³´
                    description=img_content.get("description", "")
                )
            # ì‹¤íŒ¨ ì •ë³´ ì €ì¥
            failures["image_failed"] = image_result["failed"]
            failures["image_unsupported"] = image_result["unsupported"]

        # 3. ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ë° ì¶”ê°€
        if attachment_urls:
            attachment_result = self.process_attachments(attachment_urls, logger=logger, category=category)
            # ì„±ê³µí•œ ì²¨ë¶€íŒŒì¼ë§Œ ì¶”ê°€ (HTML êµ¬ì¡° í¬í•¨)
            for att_content in attachment_result["successful"]:
                content.add_attachment_content(
                    url=att_content["url"],
                    file_type=att_content["type"],
                    text=att_content["text"],
                    html=att_content.get("html", ""),  # HTML êµ¬ì¡°
                    elements=att_content.get("elements", [])  # ìš”ì†Œ ì •ë³´
                )
            # ì‹¤íŒ¨ ì •ë³´ ì €ì¥
            failures["attachment_failed"] = attachment_result["failed"]
            failures["attachment_unsupported"] = attachment_result["unsupported"]

        return content, failures
