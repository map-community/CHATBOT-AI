"""
í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
- ê²½ë¶ëŒ€ ì»´í“¨í„°í•™ë¶€ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ê³µì§€ì‚¬í•­, ì±„ìš©ì •ë³´, ì„¸ë¯¸ë‚˜, êµìˆ˜ì •ë³´)
- ì¦ë¶„ í¬ë¡¤ë§ìœ¼ë¡œ ìƒˆ ê²Œì‹œê¸€ë§Œ ì²˜ë¦¬
- ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±
- Pinecone ë²¡í„° DBì— ì—…ë¡œë“œ

ì‹¤í–‰ ë°©ë²•:
    python run_crawler.py
    ë˜ëŠ”
    docker exec -it knu-chatbot-app python /app/src/modules/run_crawler.py
"""
import sys
from pathlib import Path

# modules ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from pymongo import MongoClient
from config import CrawlerConfig
from state import CrawlStateManager
from processing import DocumentProcessor, EmbeddingManager
from processing.multimodal_processor import MultimodalProcessor
from crawling import (
    NoticeCrawler,
    JobCrawler,
    SeminarCrawler,
    ProfessorCrawler
)
from crawling.professor_crawler import GuestProfessorCrawler, StaffCrawler


def main():
    """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜"""

    # MongoDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    mongo_client = MongoClient(CrawlerConfig.MONGODB_URI)

    # ê° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    state_manager = CrawlStateManager(mongo_client)
    multimodal_processor = MultimodalProcessor(mongo_client=mongo_client)
    document_processor = DocumentProcessor(
        mongo_client=mongo_client,
        multimodal_processor=multimodal_processor,
        enable_multimodal=True
    )
    embedding_manager = EmbeddingManager()

    print("\n" + "="*80)
    print("ğŸš€ ë©€í‹°ëª¨ë‹¬ RAG í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("="*80 + "\n")

    # í˜„ì¬ í¬ë¡¤ë§ ìƒíƒœ ì¶œë ¥
    state_manager.print_status()

    # ì „ì²´ ì„ë² ë”© ì•„ì´í…œ ì €ì¥ìš©
    all_embedding_items = []

    # ========== 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ“‹ 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§")
    print("="*80)

    notice_crawler = NoticeCrawler()
    notice_latest_id = notice_crawler.get_latest_id()

    if notice_latest_id:
        print(f"âœ… ìµœì‹  ê³µì§€ì‚¬í•­ ID: {notice_latest_id}")

        # ì¦ë¶„ í¬ë¡¤ë§: ìƒˆ ê²Œì‹œê¸€ë§Œ í¬ë¡¤ë§
        crawl_range = state_manager.get_crawl_range('notice', notice_latest_id)

        if len(crawl_range) > 0:
            print(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {notice_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

            # URL ìƒì„± ë° í¬ë¡¤ë§
            notice_urls = notice_crawler.generate_urls(crawl_range)

            # ì¶”ê°€ URL (íŠ¹ì • ê³µì§€ì‚¬í•­)
            additional_urls = [
                f"{CrawlerConfig.BASE_URLS['notice']}&wr_id={wr_id}"
                for wr_id in CrawlerConfig.ADDITIONAL_NOTICE_IDS
            ]
            notice_urls.extend(additional_urls)

            # í¬ë¡¤ë§ ì‹¤í–‰
            notice_data = notice_crawler.crawl_urls(notice_urls)

            # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ (ì¤‘ë³µ ì²´í¬, OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± í¬í•¨)
            embedding_items, new_count = document_processor.process_documents_multimodal(notice_data, category="notice")

            all_embedding_items.extend(embedding_items)

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state_manager.update_last_processed_id('notice', notice_latest_id, new_count)
            print(f"âœ… ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
        else:
            print("â„¹ï¸  ìƒˆ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ê³µì§€ì‚¬í•­ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

    # ========== 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ’¼ 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§")
    print("="*80)

    job_crawler = JobCrawler()
    job_latest_id = job_crawler.get_latest_id()

    if job_latest_id:
        print(f"âœ… ìµœì‹  ì±„ìš©ì •ë³´ ID: {job_latest_id}")

        crawl_range = state_manager.get_crawl_range('job', job_latest_id)

        if len(crawl_range) > 0:
            print(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {job_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

            job_urls = job_crawler.generate_urls(crawl_range)
            job_data = job_crawler.crawl_urls(job_urls)

            embedding_items, new_count = document_processor.process_documents_multimodal(job_data, category="job")

            all_embedding_items.extend(embedding_items)

            state_manager.update_last_processed_id('job', job_latest_id, new_count)
            print(f"âœ… ì±„ìš©ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
        else:
            print("â„¹ï¸  ìƒˆ ì±„ìš©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì±„ìš©ì •ë³´ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

    # ========== 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ“ 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§")
    print("="*80)

    seminar_crawler = SeminarCrawler()
    seminar_latest_id = seminar_crawler.get_latest_id()

    if seminar_latest_id:
        print(f"âœ… ìµœì‹  ì„¸ë¯¸ë‚˜ ID: {seminar_latest_id}")

        crawl_range = state_manager.get_crawl_range('seminar', seminar_latest_id)

        if len(crawl_range) > 0:
            print(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {seminar_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

            seminar_urls = seminar_crawler.generate_urls(crawl_range)
            seminar_data = seminar_crawler.crawl_urls(seminar_urls)

            embedding_items, new_count = document_processor.process_documents_multimodal(seminar_data, category="seminar")

            all_embedding_items.extend(embedding_items)

            state_manager.update_last_processed_id('seminar', seminar_latest_id, new_count)
            print(f"âœ… ì„¸ë¯¸ë‚˜ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
        else:
            print("â„¹ï¸  ìƒˆ ì„¸ë¯¸ë‚˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì„¸ë¯¸ë‚˜ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

    # ========== 4. êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ‘¨â€ğŸ« 4. êµìˆ˜ ë° ì§ì› ì •ë³´ í¬ë¡¤ë§")
    print("="*80)

    # ì •êµìˆ˜
    professor_crawler = ProfessorCrawler()
    professor_data = professor_crawler.crawl_all()

    # ì´ˆë¹™êµìˆ˜
    guest_professor_crawler = GuestProfessorCrawler()
    guest_professor_data = guest_professor_crawler.crawl_all()

    # ì§ì›
    staff_crawler = StaffCrawler()
    staff_data = staff_crawler.crawl_all()

    # í•©ì¹˜ê¸°
    combined_professor_data = professor_data + guest_professor_data + staff_data

    # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬
    embedding_items, new_count = document_processor.process_documents_multimodal(combined_professor_data, category="professor")

    all_embedding_items.extend(embedding_items)

    print(f"âœ… êµìˆ˜/ì§ì› ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")

    # ========== 5. ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ (ë©€í‹°ëª¨ë‹¬) ==========
    print("\n" + "="*80)
    print("ğŸ”„ 5. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„± ë° Pinecone ì—…ë¡œë“œ")
    print("="*80)

    if all_embedding_items:
        print(f"ğŸ“Š ì´ {len(all_embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ ì²˜ë¦¬ ì˜ˆì •")
        print(f"   - í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± ê²°ê³¼ í¬í•¨\n")

        # ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)
        uploaded_count = embedding_manager.process_and_upload_items(all_embedding_items)

        print(f"âœ… ì´ {uploaded_count}ê°œ ë²¡í„° ì—…ë¡œë“œ ì™„ë£Œ")
    else:
        print("â„¹ï¸  ìƒˆë¡œ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ========== 6. ìµœì¢… ìƒíƒœ ì¶œë ¥ ==========
    print("\n" + "="*80)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ")
    print("="*80)

    state_manager.print_status()

    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n")


if __name__ == "__main__":
    main()
