"""
í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
- ê²½ë¶ëŒ€ ì»´í“¨í„°í•™ë¶€ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ê³µì§€ì‚¬í•­, ì±„ìš©ì •ë³´, ì„¸ë¯¸ë‚˜, êµìˆ˜ì •ë³´)
- ì¦ë¶„ í¬ë¡¤ë§ìœ¼ë¡œ ìƒˆ ê²Œì‹œê¸€ë§Œ ì²˜ë¦¬
- ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±
- Pinecone ë²¡í„° DBì— ì—…ë¡œë“œ

ì‹¤í–‰ ë°©ë²•:
    python run_crawler.py
    ë˜ëŠ”
    docker exec -it knu-chatbot-app python /app/src/modules/run_crawler.py
"""
import sys
from pathlib import Path

# modules ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from pymongo import MongoClient
from config import CrawlerConfig
from state import CrawlStateManager
from processing import DocumentProcessor, EmbeddingManager
from processing.multimodal_processor import MultimodalProcessor
from crawling import (
    NoticeCrawler,
    JobCrawler,
    SeminarCrawler,
    ProfessorCrawler
)
from crawling.professor_crawler import GuestProfessorCrawler, StaffCrawler
from utils.logging_config import get_logger, close_logger


def main():
    """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜"""

    # ë¡œê±° ì´ˆê¸°í™”
    logger = get_logger()

    try:
        # MongoDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        mongo_client = MongoClient(CrawlerConfig.MONGODB_URI)

        # ê° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        state_manager = CrawlStateManager(mongo_client)
        multimodal_processor = MultimodalProcessor(mongo_client=mongo_client)
        document_processor = DocumentProcessor(
            mongo_client=mongo_client,
            multimodal_processor=multimodal_processor,
            enable_multimodal=True
        )
        embedding_manager = EmbeddingManager()

        logger.section_start("ğŸš€ ë©€í‹°ëª¨ë‹¬ RAG í¬ë¡¤ëŸ¬ ì‹œì‘")

        # í˜„ì¬ í¬ë¡¤ë§ ìƒíƒœ ì¶œë ¥
        state_manager.print_status()

        # ì „ì²´ ì„ë² ë”© ì•„ì´í…œ ì €ì¥ìš©
        all_embedding_items = []

        # ========== 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ“‹ 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§")

        notice_crawler = NoticeCrawler()
        notice_latest_id = notice_crawler.get_latest_id()

        if notice_latest_id:
            logger.info(f"âœ… ìµœì‹  ê³µì§€ì‚¬í•­ ID: {notice_latest_id}")

            # ì¦ë¶„ í¬ë¡¤ë§: ìƒˆ ê²Œì‹œê¸€ë§Œ í¬ë¡¤ë§
            crawl_range = state_manager.get_crawl_range('notice', notice_latest_id)

            if len(crawl_range) > 0:
                logger.info(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {notice_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

                # URL ìƒì„± ë° í¬ë¡¤ë§
                notice_urls = notice_crawler.generate_urls(crawl_range)

                # ì¶”ê°€ URL (íŠ¹ì • ê³µì§€ì‚¬í•­)
                additional_urls = [
                    f"{CrawlerConfig.BASE_URLS['notice']}&wr_id={wr_id}"
                    for wr_id in CrawlerConfig.ADDITIONAL_NOTICE_IDS
                ]
                notice_urls.extend(additional_urls)

                # í¬ë¡¤ë§ ì‹¤í–‰
                notice_data = notice_crawler.crawl_urls(notice_urls)

                # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ (ì¤‘ë³µ ì²´í¬, OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± í¬í•¨)
                embedding_items, new_count = document_processor.process_documents_multimodal(notice_data, category="notice")

                all_embedding_items.extend(embedding_items)

                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state_manager.update_last_processed_id('notice', notice_latest_id, new_count)
                logger.info(f"âœ… ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
            else:
                logger.info("â„¹ï¸  ìƒˆ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error("âŒ ê³µì§€ì‚¬í•­ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

        # ========== 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ’¼ 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§")

        job_crawler = JobCrawler()
        job_latest_id = job_crawler.get_latest_id()

        if job_latest_id:
            logger.info(f"âœ… ìµœì‹  ì±„ìš©ì •ë³´ ID: {job_latest_id}")

            crawl_range = state_manager.get_crawl_range('job', job_latest_id)

            if len(crawl_range) > 0:
                logger.info(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {job_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

                job_urls = job_crawler.generate_urls(crawl_range)
                job_data = job_crawler.crawl_urls(job_urls)

                embedding_items, new_count = document_processor.process_documents_multimodal(job_data, category="job")

                all_embedding_items.extend(embedding_items)

                state_manager.update_last_processed_id('job', job_latest_id, new_count)
                logger.info(f"âœ… ì±„ìš©ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
            else:
                logger.info("â„¹ï¸  ìƒˆ ì±„ìš©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error("âŒ ì±„ìš©ì •ë³´ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

        # ========== 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ“ 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§")

        seminar_crawler = SeminarCrawler()
        seminar_latest_id = seminar_crawler.get_latest_id()

        if seminar_latest_id:
            logger.info(f"âœ… ìµœì‹  ì„¸ë¯¸ë‚˜ ID: {seminar_latest_id}")

            crawl_range = state_manager.get_crawl_range('seminar', seminar_latest_id)

            if len(crawl_range) > 0:
                logger.info(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {seminar_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

                seminar_urls = seminar_crawler.generate_urls(crawl_range)
                seminar_data = seminar_crawler.crawl_urls(seminar_urls)

                embedding_items, new_count = document_processor.process_documents_multimodal(seminar_data, category="seminar")

                all_embedding_items.extend(embedding_items)

                state_manager.update_last_processed_id('seminar', seminar_latest_id, new_count)
                logger.info(f"âœ… ì„¸ë¯¸ë‚˜ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
            else:
                logger.info("â„¹ï¸  ìƒˆ ì„¸ë¯¸ë‚˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error("âŒ ì„¸ë¯¸ë‚˜ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

        # ========== 4. êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ‘¨â€ğŸ« 4. êµìˆ˜ ë° ì§ì› ì •ë³´ í¬ë¡¤ë§")

        # ì •êµìˆ˜
        professor_crawler = ProfessorCrawler()
        professor_data = professor_crawler.crawl_all()

        # ì´ˆë¹™êµìˆ˜
        guest_professor_crawler = GuestProfessorCrawler()
        guest_professor_data = guest_professor_crawler.crawl_all()

        # ì§ì›
        staff_crawler = StaffCrawler()
        staff_data = staff_crawler.crawl_all()

        # í•©ì¹˜ê¸°
        combined_professor_data = professor_data + guest_professor_data + staff_data

        # êµìˆ˜/ì§ì› ì •ë³´ëŠ” í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ (ì´ë¯¸ì§€ OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± ì œì™¸)
        # ë‹¨, MongoDB ì¤‘ë³µ ì²´í¬ + ë‚´ìš© ë³€ê²½ ê°ì§€ ìˆ˜í–‰
        # êµìˆ˜ í¬ë¡¤ëŸ¬ í˜•ì‹: (title, text_content, image_list, attachment_list, date, url)
        new_count = 0
        professor_items = []
        for title, text_content, image_list, attachment_list, date, url in combined_professor_data:
            # ì¤‘ë³µ ì²´í¬ (ì´ë¯¸ì§€ + ë‚´ìš© í•´ì‹œë¡œ ì²´í¬, ë‚´ìš© ë°”ë€Œë©´ ì¬ì²˜ë¦¬)
            first_image = image_list[0] if image_list else None
            if document_processor.is_duplicate(title, first_image, text_content):
                logger.log_post_skipped("professor", title, reason="ì¤‘ë³µ")
                continue

            new_count += 1

            metadata = {
                "title": title,
                "url": url,
                "date": date,
                "content_type": "text",
                "source": "professor_info",
                "category": "professor"
            }
            professor_items.append((text_content, metadata))

            # MongoDBì— ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ (content í•´ì‹œ ì €ì¥)
            document_processor.mark_as_processed(title, first_image, text_content)

        all_embedding_items.extend(professor_items)
        logger.info(f"âœ… êµìˆ˜/ì§ì› ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(professor_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")

        # ========== 5. ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ (ë©€í‹°ëª¨ë‹¬) ==========
        logger.section_start("ğŸ”„ 5. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„± ë° Pinecone ì—…ë¡œë“œ")

        if all_embedding_items:
            logger.info(f"ğŸ“Š ì´ {len(all_embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ ì²˜ë¦¬ ì˜ˆì •")
            logger.info(f"   - í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± ê²°ê³¼ í¬í•¨")

            # ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)
            uploaded_count = embedding_manager.process_and_upload_items(all_embedding_items)

            logger.info(f"âœ… ì´ {uploaded_count}ê°œ ë²¡í„° ì—…ë¡œë“œ ì™„ë£Œ")
        else:
            logger.info("â„¹ï¸  ìƒˆë¡œ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ========== 6. Redis ìºì‹œ ì¦ë¶„ ì—…ë°ì´íŠ¸ ==========
        if all_embedding_items:
            logger.section_start("ğŸ”„ 6. Redis ìºì‹œ ì¦ë¶„ ì—…ë°ì´íŠ¸")

            try:
                import redis
                import pickle
                import os

                redis_client = redis.Redis(
                    host=os.getenv('REDIS_HOST', 'redis'),
                    port=int(os.getenv('REDIS_PORT', 6379)),
                    decode_responses=False  # pickle ì‚¬ìš© ì‹œ í•„ìš”
                )

                # ê¸°ì¡´ Redis ìºì‹œ ë¡œë“œ
                cached_data = redis_client.get('pinecone_metadata')

                if cached_data:
                    logger.info("ğŸ“¦ ê¸°ì¡´ Redis ìºì‹œ ë°œê²¬ - ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹œì‘")

                    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
                    (cached_titles, cached_texts, cached_urls, cached_dates,
                     cached_htmls, cached_content_types, cached_sources,
                     cached_image_urls, cached_attachment_urls, cached_attachment_types) = pickle.loads(cached_data)

                    original_count = len(cached_titles)
                    logger.info(f"   ê¸°ì¡´ ë¬¸ì„œ: {original_count}ê°œ")

                    # ìƒˆ ë°ì´í„° ì¶”ê°€ (all_embedding_itemsë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë³€í™˜)
                    updated_count = 0
                    added_count = 0

                    for text, metadata in all_embedding_items:
                        title = metadata.get('title', '')
                        source = metadata.get('source', 'original_post')

                        # êµìˆ˜/ì§ì› ì •ë³´: ì¤‘ë³µ ì²´í¬ í›„ ì—…ë°ì´íŠ¸ ë˜ëŠ” ì¶”ê°€
                        if source == 'professor_info':
                            # titleê³¼ sourceë¡œ ê¸°ì¡´ í•­ëª© ì°¾ê¸°
                            found_idx = -1
                            for idx in range(len(cached_titles)):
                                if cached_titles[idx] == title and cached_sources[idx] == source:
                                    found_idx = idx
                                    break

                            if found_idx >= 0:
                                # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸ (ë‚´ìš© ë³€ê²½ ë°˜ì˜)
                                cached_texts[found_idx] = text
                                cached_urls[found_idx] = metadata.get('url', '')
                                cached_dates[found_idx] = metadata.get('date', '')
                                cached_content_types[found_idx] = metadata.get('content_type', 'text')
                                cached_image_urls[found_idx] = metadata.get('image_url', '')
                                cached_attachment_urls[found_idx] = metadata.get('attachment_url', '')
                                cached_attachment_types[found_idx] = metadata.get('attachment_type', '')
                                updated_count += 1
                            else:
                                # ìƒˆ êµìˆ˜ ì¶”ê°€
                                cached_titles.append(title)
                                cached_texts.append(text)
                                cached_urls.append(metadata.get('url', ''))
                                cached_dates.append(metadata.get('date', ''))
                                cached_htmls.append('')  # HTMLì€ MongoDBì—ì„œ ì¡°íšŒ
                                cached_content_types.append(metadata.get('content_type', 'text'))
                                cached_sources.append(source)
                                cached_image_urls.append(metadata.get('image_url', ''))
                                cached_attachment_urls.append(metadata.get('attachment_url', ''))
                                cached_attachment_types.append(metadata.get('attachment_type', ''))
                                added_count += 1
                        else:
                            # ê³µì§€ì‚¬í•­/ì„¸ë¯¸ë‚˜/ì±„ìš©ì •ë³´: ë¬´ì¡°ê±´ ì¶”ê°€ (ì²­í¬ ì¤‘ë³µ ì—†ìŒ)
                            cached_titles.append(title)
                            cached_texts.append(text)
                            cached_urls.append(metadata.get('url', ''))
                            cached_dates.append(metadata.get('date', ''))
                            cached_htmls.append('')  # HTMLì€ MongoDBì—ì„œ ì¡°íšŒ
                            cached_content_types.append(metadata.get('content_type', 'text'))
                            cached_sources.append(source)
                            cached_image_urls.append(metadata.get('image_url', ''))
                            cached_attachment_urls.append(metadata.get('attachment_url', ''))
                            cached_attachment_types.append(metadata.get('attachment_type', ''))
                            added_count += 1

                    new_count = len(cached_titles) - original_count
                    logger.info(f"   ì¶”ê°€: {added_count}ê°œ, ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
                    logger.info(f"   ì´ ë¬¸ì„œ: {len(cached_titles)}ê°œ ({new_count:+d})")

                    # Redisì— ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ì €ì¥
                    updated_cache = (
                        cached_titles, cached_texts, cached_urls, cached_dates,
                        cached_htmls, cached_content_types, cached_sources,
                        cached_image_urls, cached_attachment_urls, cached_attachment_types
                    )
                    redis_client.set('pinecone_metadata', pickle.dumps(updated_cache))

                    logger.info("âœ… Redis ìºì‹œ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                else:
                    logger.info("â„¹ï¸  ê¸°ì¡´ Redis ìºì‹œ ì—†ìŒ - ë‹¤ìŒ ì•± ì¬ì‹œì‘ ì‹œ Pineconeì—ì„œ ë¡œë“œë©ë‹ˆë‹¤")

            except Exception as e:
                logger.warning(f"âš ï¸  Redis ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ì•± ì¬ì‹œì‘ ì‹œ Pineconeì—ì„œ ë¡œë“œë©ë‹ˆë‹¤): {e}")

        # ========== 7. ìµœì¢… ìƒíƒœ ì¶œë ¥ ==========
        logger.section_start("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ")

        state_manager.print_status()

        # ìµœì¢… í†µê³„ ì¶œë ¥
        logger.print_summary()

        logger.info("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

    finally:
        # ë¡œê±° ì¢…ë£Œ
        close_logger()


if __name__ == "__main__":
    main()
