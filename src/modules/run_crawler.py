"""
í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
- ê²½ë¶ëŒ€ ì»´í“¨í„°í•™ë¶€ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ê³µì§€ì‚¬í•­, ì±„ìš©ì •ë³´, ì„¸ë¯¸ë‚˜, êµìˆ˜ì •ë³´)
- ì¦ë¶„ í¬ë¡¤ë§ìœ¼ë¡œ ìƒˆ ê²Œì‹œê¸€ë§Œ ì²˜ë¦¬
- ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±
- Pinecone ë²¡í„° DBì— ì—…ë¡œë“œ

ì‹¤í–‰ ë°©ë²•:
    python run_crawler.py
    ë˜ëŠ”
    docker exec -it knu-chatbot-app python /app/src/modules/run_crawler.py
"""
import sys
from pathlib import Path

# modules ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from pymongo import MongoClient
from config import CrawlerConfig
from state import CrawlStateManager
from processing import DocumentProcessor, EmbeddingManager
from processing.multimodal_processor import MultimodalProcessor
from crawling import (
    NoticeCrawler,
    JobCrawler,
    SeminarCrawler,
    ProfessorCrawler
)
from crawling.professor_crawler import GuestProfessorCrawler, StaffCrawler
from utils.logging_config import get_logger, close_logger


def main():
    """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜"""

    # ë¡œê±° ì´ˆê¸°í™”
    logger = get_logger()

    try:
        # MongoDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        mongo_client = MongoClient(CrawlerConfig.MONGODB_URI)

        # ê° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        state_manager = CrawlStateManager(mongo_client)
        multimodal_processor = MultimodalProcessor(mongo_client=mongo_client)
        document_processor = DocumentProcessor(
            mongo_client=mongo_client,
            multimodal_processor=multimodal_processor,
            enable_multimodal=True
        )
        embedding_manager = EmbeddingManager()

        logger.section_start("ğŸš€ ë©€í‹°ëª¨ë‹¬ RAG í¬ë¡¤ëŸ¬ ì‹œì‘")

        # í˜„ì¬ í¬ë¡¤ë§ ìƒíƒœ ì¶œë ¥
        state_manager.print_status()

        # ì „ì²´ ì„ë² ë”© ì•„ì´í…œ ì €ì¥ìš©
        all_embedding_items = []

        # ========== 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ“‹ 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§")

        notice_crawler = NoticeCrawler()
        notice_latest_id = notice_crawler.get_latest_id()

        if notice_latest_id:
            logger.info(f"âœ… ìµœì‹  ê³µì§€ì‚¬í•­ ID: {notice_latest_id}")

            # ì¦ë¶„ í¬ë¡¤ë§: ìƒˆ ê²Œì‹œê¸€ë§Œ í¬ë¡¤ë§
            crawl_range = state_manager.get_crawl_range('notice', notice_latest_id)

            if len(crawl_range) > 0:
                logger.info(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {notice_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

                # URL ìƒì„± ë° í¬ë¡¤ë§
                notice_urls = notice_crawler.generate_urls(crawl_range)

                # ì¶”ê°€ URL (íŠ¹ì • ê³µì§€ì‚¬í•­)
                additional_urls = [
                    f"{CrawlerConfig.BASE_URLS['notice']}&wr_id={wr_id}"
                    for wr_id in CrawlerConfig.ADDITIONAL_NOTICE_IDS
                ]
                notice_urls.extend(additional_urls)

                # í¬ë¡¤ë§ ì‹¤í–‰
                notice_data = notice_crawler.crawl_urls(notice_urls)

                # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ (ì¤‘ë³µ ì²´í¬, OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± í¬í•¨)
                embedding_items, new_count = document_processor.process_documents_multimodal(notice_data, category="notice")

                all_embedding_items.extend(embedding_items)

                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state_manager.update_last_processed_id('notice', notice_latest_id, new_count)
                logger.info(f"âœ… ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
            else:
                logger.info("â„¹ï¸  ìƒˆ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error("âŒ ê³µì§€ì‚¬í•­ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

        # ========== 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ’¼ 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§")

        job_crawler = JobCrawler()
        job_latest_id = job_crawler.get_latest_id()

        if job_latest_id:
            logger.info(f"âœ… ìµœì‹  ì±„ìš©ì •ë³´ ID: {job_latest_id}")

            crawl_range = state_manager.get_crawl_range('job', job_latest_id)

            if len(crawl_range) > 0:
                logger.info(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {job_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

                job_urls = job_crawler.generate_urls(crawl_range)
                job_data = job_crawler.crawl_urls(job_urls)

                embedding_items, new_count = document_processor.process_documents_multimodal(job_data, category="job")

                all_embedding_items.extend(embedding_items)

                state_manager.update_last_processed_id('job', job_latest_id, new_count)
                logger.info(f"âœ… ì±„ìš©ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
            else:
                logger.info("â„¹ï¸  ìƒˆ ì±„ìš©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error("âŒ ì±„ìš©ì •ë³´ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

        # ========== 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ“ 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§")

        seminar_crawler = SeminarCrawler()
        seminar_latest_id = seminar_crawler.get_latest_id()

        if seminar_latest_id:
            logger.info(f"âœ… ìµœì‹  ì„¸ë¯¸ë‚˜ ID: {seminar_latest_id}")

            crawl_range = state_manager.get_crawl_range('seminar', seminar_latest_id)

            if len(crawl_range) > 0:
                logger.info(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {seminar_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

                seminar_urls = seminar_crawler.generate_urls(crawl_range)
                seminar_data = seminar_crawler.crawl_urls(seminar_urls)

                embedding_items, new_count = document_processor.process_documents_multimodal(seminar_data, category="seminar")

                all_embedding_items.extend(embedding_items)

                state_manager.update_last_processed_id('seminar', seminar_latest_id, new_count)
                logger.info(f"âœ… ì„¸ë¯¸ë‚˜ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ, {len(embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ")
            else:
                logger.info("â„¹ï¸  ìƒˆ ì„¸ë¯¸ë‚˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error("âŒ ì„¸ë¯¸ë‚˜ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

        # ========== 4. êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ ==========
        logger.section_start("ğŸ‘¨â€ğŸ« 4. êµìˆ˜ ë° ì§ì› ì •ë³´ í¬ë¡¤ë§")

        # ì •êµìˆ˜
        professor_crawler = ProfessorCrawler()
        professor_data = professor_crawler.crawl_all()

        # ì´ˆë¹™êµìˆ˜
        guest_professor_crawler = GuestProfessorCrawler()
        guest_professor_data = guest_professor_crawler.crawl_all()

        # ì§ì›
        staff_crawler = StaffCrawler()
        staff_data = staff_crawler.crawl_all()

        # í•©ì¹˜ê¸°
        combined_professor_data = professor_data + guest_professor_data + staff_data

        # êµìˆ˜/ì§ì› ì •ë³´ëŠ” í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ (ì´ë¯¸ì§€ OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± ì œì™¸)
        # êµìˆ˜ í¬ë¡¤ëŸ¬ í˜•ì‹: (title, text_content, image_list, attachment_list, date, url)
        for title, text_content, image_list, attachment_list, date, url in combined_professor_data:
            metadata = {
                "title": title,
                "url": url,
                "date": date,
                "content_type": "text",
                "source": "professor_info"
            }
            all_embedding_items.append((text_content, metadata))

        logger.info(f"âœ… êµìˆ˜/ì§ì› ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {len(combined_professor_data)}ê°œ ë¬¸ì„œ")

        # ========== 5. ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ (ë©€í‹°ëª¨ë‹¬) ==========
        logger.section_start("ğŸ”„ 5. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„± ë° Pinecone ì—…ë¡œë“œ")

        if all_embedding_items:
            logger.info(f"ğŸ“Š ì´ {len(all_embedding_items)}ê°œ ì„ë² ë”© ì•„ì´í…œ ì²˜ë¦¬ ì˜ˆì •")
            logger.info(f"   - í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ OCR, ì²¨ë¶€íŒŒì¼ íŒŒì‹± ê²°ê³¼ í¬í•¨")

            # ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)
            uploaded_count = embedding_manager.process_and_upload_items(all_embedding_items)

            logger.info(f"âœ… ì´ {uploaded_count}ê°œ ë²¡í„° ì—…ë¡œë“œ ì™„ë£Œ")
        else:
            logger.info("â„¹ï¸  ìƒˆë¡œ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ========== 6. ìµœì¢… ìƒíƒœ ì¶œë ¥ ==========
        logger.section_start("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ")

        state_manager.print_status()

        # ìµœì¢… í†µê³„ ì¶œë ¥
        logger.print_summary()

        logger.info("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

    finally:
        # ë¡œê±° ì¢…ë£Œ
        close_logger()


if __name__ == "__main__":
    main()
