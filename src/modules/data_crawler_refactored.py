"""
ë¦¬íŒ©í† ë§ëœ í¬ë¡¤ëŸ¬ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸

ê°œì„  ì‚¬í•­:
1. ì¦ë¶„ í¬ë¡¤ë§: ìƒˆ ê²Œì‹œê¸€ë§Œ ì²˜ë¦¬
2. ì¤‘ë³µ ì œê±°: í¬ë¡¤ë§ ì „ì— ì¤‘ë³µ ì²´í¬
3. API ë¹„ìš© ì ˆê°: ìƒˆ ë¬¸ì„œë§Œ ì„ë² ë”© ìƒì„±
4. í´ë˜ìŠ¤ ê¸°ë°˜ ì„¤ê³„: ìœ ì§€ë³´ìˆ˜ í¸ì˜ì„± í–¥ìƒ
5. ìƒíƒœ ê´€ë¦¬: í¬ë¡¤ë§ ì´ë ¥ ì¶”ì 
"""
from pymongo import MongoClient
from config import CrawlerConfig
from state import CrawlStateManager
from processing import DocumentProcessor, EmbeddingManager
from crawling import (
    NoticeCrawler,
    JobCrawler,
    SeminarCrawler,
    ProfessorCrawler
)
from crawling.professor_crawler import GuestProfessorCrawler, StaffCrawler


def main():
    """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜"""

    # MongoDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    mongo_client = MongoClient(CrawlerConfig.MONGODB_URI)

    # ê° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    state_manager = CrawlStateManager(mongo_client)
    document_processor = DocumentProcessor(mongo_client)
    embedding_manager = EmbeddingManager()

    print("\n" + "="*80)
    print("ğŸš€ ë¦¬íŒ©í† ë§ëœ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("="*80 + "\n")

    # í˜„ì¬ í¬ë¡¤ë§ ìƒíƒœ ì¶œë ¥
    state_manager.print_status()

    # ì „ì²´ ìˆ˜ì§‘ ë°ì´í„° ì €ì¥ìš©
    all_texts = []
    all_titles = []
    all_urls = []
    all_dates = []
    all_images = []

    # ========== 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ“‹ 1. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§")
    print("="*80)

    notice_crawler = NoticeCrawler()
    notice_latest_id = notice_crawler.get_latest_id()

    if notice_latest_id:
        print(f"âœ… ìµœì‹  ê³µì§€ì‚¬í•­ ID: {notice_latest_id}")

        # ì¦ë¶„ í¬ë¡¤ë§: ìƒˆ ê²Œì‹œê¸€ë§Œ í¬ë¡¤ë§
        crawl_range = state_manager.get_crawl_range('notice', notice_latest_id)

        if len(crawl_range) > 0:
            print(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {notice_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

            # URL ìƒì„± ë° í¬ë¡¤ë§
            notice_urls = notice_crawler.generate_urls(crawl_range)

            # ì¶”ê°€ URL (íŠ¹ì • ê³µì§€ì‚¬í•­)
            additional_urls = [
                f"{CrawlerConfig.BASE_URLS['notice']}&wr_id={wr_id}"
                for wr_id in CrawlerConfig.ADDITIONAL_NOTICE_IDS
            ]
            notice_urls.extend(additional_urls)

            # í¬ë¡¤ë§ ì‹¤í–‰
            notice_data = notice_crawler.crawl_urls(notice_urls)

            # ë¬¸ì„œ ì²˜ë¦¬ (ì¤‘ë³µ ì²´í¬ í¬í•¨)
            texts, titles, urls, dates, images, new_count = document_processor.process_documents(notice_data)

            all_texts.extend(texts)
            all_titles.extend(titles)
            all_urls.extend(urls)
            all_dates.extend(dates)
            all_images.extend(images)

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state_manager.update_last_processed_id('notice', notice_latest_id, new_count)
            print(f"âœ… ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ")
        else:
            print("â„¹ï¸  ìƒˆ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ê³µì§€ì‚¬í•­ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

    # ========== 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ’¼ 2. ì±„ìš©ì •ë³´ í¬ë¡¤ë§")
    print("="*80)

    job_crawler = JobCrawler()
    job_latest_id = job_crawler.get_latest_id()

    if job_latest_id:
        print(f"âœ… ìµœì‹  ì±„ìš©ì •ë³´ ID: {job_latest_id}")

        crawl_range = state_manager.get_crawl_range('job', job_latest_id)

        if len(crawl_range) > 0:
            print(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {job_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

            job_urls = job_crawler.generate_urls(crawl_range)
            job_data = job_crawler.crawl_urls(job_urls)

            texts, titles, urls, dates, images, new_count = document_processor.process_documents(job_data)

            all_texts.extend(texts)
            all_titles.extend(titles)
            all_urls.extend(urls)
            all_dates.extend(dates)
            all_images.extend(images)

            state_manager.update_last_processed_id('job', job_latest_id, new_count)
            print(f"âœ… ì±„ìš©ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ")
        else:
            print("â„¹ï¸  ìƒˆ ì±„ìš©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì±„ìš©ì •ë³´ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

    # ========== 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ“ 3. ì„¸ë¯¸ë‚˜ í¬ë¡¤ë§")
    print("="*80)

    seminar_crawler = SeminarCrawler()
    seminar_latest_id = seminar_crawler.get_latest_id()

    if seminar_latest_id:
        print(f"âœ… ìµœì‹  ì„¸ë¯¸ë‚˜ ID: {seminar_latest_id}")

        crawl_range = state_manager.get_crawl_range('seminar', seminar_latest_id)

        if len(crawl_range) > 0:
            print(f"ğŸ” í¬ë¡¤ë§í•  ë²”ìœ„: {seminar_latest_id} ~ {crawl_range[-1] + 1} ({len(crawl_range)}ê°œ)")

            seminar_urls = seminar_crawler.generate_urls(crawl_range)
            seminar_data = seminar_crawler.crawl_urls(seminar_urls)

            texts, titles, urls, dates, images, new_count = document_processor.process_documents(seminar_data)

            all_texts.extend(texts)
            all_titles.extend(titles)
            all_urls.extend(urls)
            all_dates.extend(dates)
            all_images.extend(images)

            state_manager.update_last_processed_id('seminar', seminar_latest_id, new_count)
            print(f"âœ… ì„¸ë¯¸ë‚˜ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ")
        else:
            print("â„¹ï¸  ìƒˆ ì„¸ë¯¸ë‚˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì„¸ë¯¸ë‚˜ ìµœì‹  ID ì¡°íšŒ ì‹¤íŒ¨")

    # ========== 4. êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ ==========
    print("\n" + "="*80)
    print("ğŸ‘¨â€ğŸ« 4. êµìˆ˜ ë° ì§ì› ì •ë³´ í¬ë¡¤ë§")
    print("="*80)

    # ì •êµìˆ˜
    professor_crawler = ProfessorCrawler()
    professor_data = professor_crawler.crawl_all()

    # ì´ˆë¹™êµìˆ˜
    guest_professor_crawler = GuestProfessorCrawler()
    guest_professor_data = guest_professor_crawler.crawl_all()

    # ì§ì›
    staff_crawler = StaffCrawler()
    staff_data = staff_crawler.crawl_all()

    # í•©ì¹˜ê¸°
    combined_professor_data = professor_data + guest_professor_data + staff_data

    # ë¬¸ì„œ ì²˜ë¦¬
    texts, titles, urls, dates, images, new_count = document_processor.process_documents(combined_professor_data)

    all_texts.extend(texts)
    all_titles.extend(titles)
    all_urls.extend(urls)
    all_dates.extend(dates)
    all_images.extend(images)

    print(f"âœ… êµìˆ˜/ì§ì› ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ë¬¸ì„œ")

    # ========== 5. ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ ==========
    print("\n" + "="*80)
    print("ğŸ”„ 5. ì„ë² ë”© ìƒì„± ë° Pinecone ì—…ë¡œë“œ")
    print("="*80)

    if all_texts:
        print(f"ğŸ“Š ì´ {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ ì²­í¬ ì²˜ë¦¬ ì˜ˆì •")

        # ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ
        uploaded_count = embedding_manager.process_and_upload(
            all_texts, all_titles, all_urls, all_dates
        )

        print(f"âœ… ì´ {uploaded_count}ê°œ ë²¡í„° ì—…ë¡œë“œ ì™„ë£Œ")
    else:
        print("â„¹ï¸  ìƒˆë¡œ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ========== 6. ìµœì¢… ìƒíƒœ ì¶œë ¥ ==========
    print("\n" + "="*80)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ")
    print("="*80)

    state_manager.print_status()

    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n")


if __name__ == "__main__":
    main()
