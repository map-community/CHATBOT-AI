"""
BM25 Retriever
BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ í´ë˜ìŠ¤
"""

import numpy as np
import pickle
from rank_bm25 import BM25Okapi
from typing import List, Tuple
import logging
from bs4 import BeautifulSoup
from multiprocessing import Pool, cpu_count
import time
import os

logger = logging.getLogger(__name__)


def get_safe_cpu_count() -> int:
    """
    Docker CPU ì œí•œì„ ê³ ë ¤í•œ ì•ˆì „í•œ CPU ê°œìˆ˜ ë°˜í™˜

    Returns:
        ì‚¬ìš© ê°€ëŠ¥í•œ CPU ê°œìˆ˜

    Note:
        1. í™˜ê²½ë³€ìˆ˜ OMP_NUM_THREADS ìš°ì„  ì‚¬ìš© (Dockerì—ì„œ ì„¤ì •)
        2. ì—†ìœ¼ë©´ ë¬¼ë¦¬ CPUì˜ ì ˆë°˜ ì‚¬ìš© (ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­ ìµœì†Œí™”)
        3. ìµœì†Œ 1ê°œ ë³´ì¥
    """
    # Docker í™˜ê²½ë³€ìˆ˜ ìš°ì„  í™•ì¸
    env_threads = os.getenv("OMP_NUM_THREADS")
    if env_threads:
        try:
            return max(1, int(env_threads))
        except ValueError:
            pass

    # ë¬¼ë¦¬ CPUì˜ ì ˆë°˜ ì‚¬ìš© (ì•ˆì „í•œ ê¸°ë³¸ê°’)
    physical_cores = cpu_count() or 1
    return max(1, physical_cores // 2)


def _parse_html_to_text(html_or_markdown: str) -> str:
    """
    HTML ë˜ëŠ” Markdownì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬ìš© top-level í•¨ìˆ˜)

    Args:
        html_or_markdown: HTML ë˜ëŠ” Markdown ë¬¸ìì—´

    Returns:
        íŒŒì‹±ëœ í…ìŠ¤íŠ¸

    Note:
        - Markdown (Upstage API ì œê³µ): í‘œ êµ¬ì¡° ë³´ì¡´, ê·¸ëŒ€ë¡œ ë°˜í™˜
        - HTML (fallback): BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    """
    if not html_or_markdown:
        return ""

    # Markdown í˜•ì‹ ê°ì§€ (í‘œ í˜•ì‹: '|' êµ¬ë¶„ì)
    # Markdownì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì´ë¯¸ LLMì´ ì´í•´í•˜ê¸° ì¢‹ì€ í˜•íƒœ)
    if '|' in html_or_markdown and ('---' in html_or_markdown or '\n' in html_or_markdown):
        # Markdown í‘œ í˜•ì‹ìœ¼ë¡œ ë³´ì„
        return html_or_markdown

    # HTMLì´ë©´ íŒŒì‹±
    try:
        soup = BeautifulSoup(html_or_markdown, 'html.parser')
        return soup.get_text(separator=' ', strip=True)
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return html_or_markdown


# âœ… ë³‘ë ¬ í† í°í™”ìš© ì „ì—­ í•¨ìˆ˜ (top-levelì— ì •ì˜í•´ì•¼ multiprocessingì—ì„œ pickle ê°€ëŠ¥)
_global_query_transformer = None

def _set_global_query_transformer(transformer):
    """ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ìš© ì „ì—­ transformer ì„¤ì •"""
    global _global_query_transformer
    _global_query_transformer = transformer

def _tokenize_combined_text(combined_text: str) -> list:
    """
    í…ìŠ¤íŠ¸ë¥¼ í† í°í™” (ë³‘ë ¬ ì²˜ë¦¬ìš© top-level í•¨ìˆ˜)

    Args:
        combined_text: ê²°í•©ëœ í…ìŠ¤íŠ¸ (ì œëª© + ë³¸ë¬¸ + HTML)

    Returns:
        í† í° ë¦¬ìŠ¤íŠ¸
    """
    global _global_query_transformer
    if _global_query_transformer is None:
        # fallback: ê³µë°± ê¸°ì¤€ split (í˜•íƒœì†Œ ë¶„ì„ ì—†ìŒ)
        return combined_text.split()
    return _global_query_transformer(combined_text)


class BM25Retriever:
    """
    BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í´ë˜ìŠ¤

    ì œëª©, ë³¸ë¬¸, HTML êµ¬ì¡°í™” ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ í† í°í™”í•˜ê³ , BM25 ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬
    ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    ê°œì„ ì‚¬í•­:
    - ì œëª©ë¿ë§Œ ì•„ë‹ˆë¼ ë³¸ë¬¸ë„ ê²€ìƒ‰í•˜ì—¬ ì²¨ë¶€íŒŒì¼ ë‚´ìš©ë„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - HTML êµ¬ì¡°í™” ë°ì´í„°(í‘œ ë“±)ë„ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    """

    def __init__(self,
                 titles: List[str],
                 texts: List[str],
                 urls: List[str],
                 dates: List[str],
                 query_transformer,
                 similarity_adjuster,
                 htmls: List[str] = None,
                 k1: float = 1.5,
                 b: float = 0.75,
                 redis_client = None):
        """
        BM25Retriever ì´ˆê¸°í™”

        Args:
            titles: ë¬¸ì„œ ì œëª© ë¦¬ìŠ¤íŠ¸
            texts: ë¬¸ì„œ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸
            urls: ë¬¸ì„œ URL ë¦¬ìŠ¤íŠ¸
            dates: ë¬¸ì„œ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
            query_transformer: ì§ˆë¬¸ì„ ëª…ì‚¬ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (transformed_query)
            similarity_adjuster: ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ (adjust_similarity_scores)
            htmls: HTML êµ¬ì¡°í™” ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì„ íƒ, í‘œ ê²€ìƒ‰ ê°œì„ ìš©)
            k1: BM25 k1 íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 1.5)
            b: BM25 b íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 0.75)
            redis_client: Redis í´ë¼ì´ì–¸íŠ¸ (ì„ íƒ, ìºì‹±ìš©)
        """
        self.titles = titles
        self.texts = texts
        self.urls = urls
        self.dates = dates
        self.htmls = htmls if htmls else []
        self.query_transformer = query_transformer
        self.similarity_adjuster = similarity_adjuster
        self.k1 = k1
        self.b = b
        self.redis_client = redis_client

        # ìºì‹œ í‚¤ ì„¤ì • (v2: HTML íŒŒì‹± ê²°ê³¼ í¬í•¨)
        self.cache_key = "bm25_cache_v2"

        # BM25 ì¸ë±ìŠ¤ ìƒì„± (ì œëª© + ë³¸ë¬¸ + HTML í…ìŠ¤íŠ¸ ê²°í•©í•˜ì—¬ ê²€ìƒ‰)
        self.tokenized_documents = []
        html_texts = []  # íŒŒì‹±ëœ HTML í…ìŠ¤íŠ¸
        loaded_from_cache = False

        # 1. Redis ìºì‹œ í™•ì¸
        if self.redis_client:
            try:
                cached_data = self.redis_client.get(self.cache_key)
                if cached_data:
                    cache_obj = pickle.loads(cached_data)
                    # v2 ìºì‹œ êµ¬ì¡°: {"tokenized_documents": [...], "html_texts": [...], "doc_count": N}
                    if isinstance(cache_obj, dict) and cache_obj.get("doc_count") == len(titles):
                        self.tokenized_documents = cache_obj["tokenized_documents"]
                        html_texts = cache_obj.get("html_texts", [])
                        loaded_from_cache = True
                        logger.info(f"ğŸš€ Redisì—ì„œ BM25 ìºì‹œ ë¡œë“œ ì™„ë£Œ! ({len(self.tokenized_documents)}ê°œ ë¬¸ì„œ)")
                    else:
                        logger.warning(f"âš ï¸  BM25 ìºì‹œ ë²„ì „ ë˜ëŠ” ê°œìˆ˜ ë¶ˆì¼ì¹˜. ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"âš ï¸  Redisì—ì„œ BM25 ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 2. ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if not loaded_from_cache:
            start_time = time.time()
            logger.info("ğŸ”„ BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘ (ì œëª©+ë³¸ë¬¸+HTML ê²€ìƒ‰)...")

            # 2-1. HTML íŒŒì‹± (ë³‘ë ¬ ì²˜ë¦¬)
            html_count = sum(1 for h in self.htmls if h) if self.htmls else 0
            if html_count > 0:
                logger.info(f"   ğŸ“„ HTML íŒŒì‹± ì‹œì‘ ({html_count}ê°œ, ë³‘ë ¬ ì²˜ë¦¬: {get_safe_cpu_count()}ì½”ì–´)...")
                parse_start = time.time()

                # ë³‘ë ¬ ì²˜ë¦¬ë¡œ HTML íŒŒì‹±
                with Pool(processes=get_safe_cpu_count()) as pool:
                    html_texts = pool.map(_parse_html_to_text, self.htmls)

                parse_time = time.time() - parse_start
                logger.info(f"   âœ… HTML íŒŒì‹± ì™„ë£Œ ({parse_time:.2f}ì´ˆ)")
            else:
                # HTMLì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
                html_texts = [""] * len(titles)

            # 2-2. í† í°í™” (ì œëª© + ë³¸ë¬¸ + HTML í…ìŠ¤íŠ¸)
            logger.info(f"   ğŸ”¤ í† í°í™” ì¤€ë¹„ ì¤‘ ({len(titles)}ê°œ ë¬¸ì„œ)...")
            tokenize_start = time.time()

            # âœ… 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ê²°í•©
            logger.info(f"      [1/2] í…ìŠ¤íŠ¸ ê²°í•© ì¤‘...")
            combined_texts = []
            for i, (title, text) in enumerate(zip(titles, texts)):
                html_text = html_texts[i] if i < len(html_texts) else ""
                combined = f"{title} {text} {html_text}".strip()
                combined_texts.append(combined)

            logger.info(f"      [1/2] í…ìŠ¤íŠ¸ ê²°í•© ì™„ë£Œ ({len(combined_texts)}ê°œ)")

            # âœ… 2ë‹¨ê³„: ë³‘ë ¬ í† í°í™” (ì‹¤ì œ í˜•íƒœì†Œ ë¶„ì„ - ì‹œê°„ ì†Œìš”!)
            logger.info(f"      [2/2] ë³‘ë ¬ í† í°í™” ì§„í–‰ ì¤‘ ({get_safe_cpu_count()}ì½”ì–´, Mecab í˜•íƒœì†Œ ë¶„ì„)...")
            logger.info(f"      â³ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1-2ë¶„ (13000ê°œ ê¸°ì¤€)")

            parallel_start = time.time()
            # ğŸš€ ìµœì í™” 1: chunksize ì¶”ê°€ (í”„ë¡œì„¸ìŠ¤ ìƒì„± ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”)
            # 13073ê°œ / 2ì½”ì–´ = 6500ê°œ/ì½”ì–´ â†’ chunksize ë™ì  ê³„ì‚°
            chunksize = max(1, len(combined_texts) // (get_safe_cpu_count() * 10))
            logger.info(f"      ğŸ“¦ Batch í¬ê¸°: {chunksize} (í”„ë¡œì„¸ìŠ¤ í†µì‹  ìµœì†Œí™”)")

            with Pool(processes=get_safe_cpu_count(), initializer=_set_global_query_transformer, initargs=(query_transformer,)) as pool:
                self.tokenized_documents = pool.map(_tokenize_combined_text, combined_texts, chunksize=chunksize)

            parallel_time = time.time() - parallel_start
            logger.info(f"      [2/2] ë³‘ë ¬ í† í°í™” ì™„ë£Œ! ({parallel_time:.2f}ì´ˆ, {len(combined_texts)/parallel_time:.0f}ë¬¸ì„œ/ì´ˆ)")

            tokenize_time = time.time() - tokenize_start
            logger.info(f"   âœ… í† í°í™” ì™„ë£Œ ({tokenize_time:.2f}ì´ˆ, ì†ë„: {len(titles)/tokenize_time:.0f}ë¬¸ì„œ/ì´ˆ)")

            # 3. Redisì— ì €ì¥ (v2 êµ¬ì¡°)
            if self.redis_client:
                try:
                    cache_obj = {
                        "tokenized_documents": self.tokenized_documents,
                        "html_texts": html_texts,
                        "doc_count": len(titles)
                    }
                    # 24ì‹œê°„ ìœ íš¨
                    self.redis_client.setex(self.cache_key, 86400, pickle.dumps(cache_obj))

                    # ìºì‹œ í¬ê¸° í™•ì¸
                    cache_size = len(pickle.dumps(cache_obj)) / (1024 * 1024)  # MB
                    logger.info(f"ğŸ’¾ Redisì— BM25 ìºì‹œ ì €ì¥ ì™„ë£Œ ({len(self.tokenized_documents)}ê°œ, {cache_size:.2f}MB)")
                except Exception as e:
                    logger.warning(f"âš ï¸  Redisì— BM25 ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

            total_time = time.time() - start_time
            logger.info(f"   â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

        self.bm25_index = BM25Okapi(self.tokenized_documents, k1=k1, b=b)
        html_count = sum(1 for h in self.htmls if h) if self.htmls else 0
        logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ({len(titles)}ê°œ ë¬¸ì„œ, HTML êµ¬ì¡°: {html_count}ê°œ)")

    def search(self,
               query_nouns: List[str],
               top_k: int = 25,
               normalize_factor: float = 24.0) -> Tuple[List[Tuple], np.ndarray]:
        """
        BM25 ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query_nouns: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 25)
            normalize_factor: ìœ ì‚¬ë„ ì •ê·œí™” íŒ©í„° (ê¸°ë³¸ê°’: 24.0)

        Returns:
            Tuple[List[Tuple], np.ndarray]:
                - ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (title, date, text, url)
                - ì¡°ì •ëœ ìœ ì‚¬ë„ ë°°ì—´
        """
        # BM25 ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self.bm25_index.get_scores(query_nouns)

        # ìœ ì‚¬ë„ ì •ê·œí™”
        similarities = similarities / normalize_factor

        # ìœ ì‚¬ë„ ì¡°ì • (ì œëª©-ë³¸ë¬¸ ë§¤ì¹­, í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë“±)
        adjusted_similarities = self.similarity_adjuster(
            query_nouns,
            self.titles,
            self.texts,
            similarities
        )

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ (ë‚´ë¦¼ì°¨ìˆœ)
        top_indices = np.argsort(adjusted_similarities)[-top_k:][::-1]

        # ê²°ê³¼ ë¬¸ì„œ ìƒì„±
        results = [
            (self.titles[i], self.dates[i], self.texts[i], self.urls[i])
            for i in top_indices
        ]

        logger.debug(f"âœ… BM25 ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ë°˜í™˜")

        return results, adjusted_similarities

    def get_similarity_score(self, query_nouns: List[str], doc_index: int) -> float:
        """
        íŠ¹ì • ë¬¸ì„œì— ëŒ€í•œ BM25 ìœ ì‚¬ë„ ì ìˆ˜ ë°˜í™˜

        Args:
            query_nouns: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            doc_index: ë¬¸ì„œ ì¸ë±ìŠ¤

        Returns:
            float: BM25 ìœ ì‚¬ë„ ì ìˆ˜
        """
        similarities = self.bm25_index.get_scores(query_nouns)
        return similarities[doc_index]

    def update_index(self,
                     titles: List[str],
                     texts: List[str],
                     urls: List[str],
                     dates: List[str],
                     htmls: List[str] = None):
        """
        BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ë¬¸ì„œ ì¶”ê°€/ì‚­ì œ ì‹œ ì‚¬ìš©)

        Args:
            titles: ìƒˆë¡œìš´ ë¬¸ì„œ ì œëª© ë¦¬ìŠ¤íŠ¸
            texts: ìƒˆë¡œìš´ ë¬¸ì„œ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸
            urls: ìƒˆë¡œìš´ ë¬¸ì„œ URL ë¦¬ìŠ¤íŠ¸
            dates: ìƒˆë¡œìš´ ë¬¸ì„œ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
            htmls: HTML êµ¬ì¡°í™” ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
        """
        start_time = time.time()
        logger.info("ğŸ”„ BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")

        self.titles = titles
        self.texts = texts
        self.urls = urls
        self.dates = dates
        self.htmls = htmls if htmls else []

        # HTML íŒŒì‹± (ë³‘ë ¬ ì²˜ë¦¬)
        html_count = sum(1 for h in self.htmls if h) if self.htmls else 0
        html_texts = []

        if html_count > 0:
            logger.info(f"   ğŸ“„ HTML íŒŒì‹± ì‹œì‘ ({html_count}ê°œ, ë³‘ë ¬ ì²˜ë¦¬: {get_safe_cpu_count()}ì½”ì–´)...")
            parse_start = time.time()

            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ HTML íŒŒì‹±
            with Pool(processes=get_safe_cpu_count()) as pool:
                html_texts = pool.map(_parse_html_to_text, self.htmls)

            parse_time = time.time() - parse_start
            logger.info(f"   âœ… HTML íŒŒì‹± ì™„ë£Œ ({parse_time:.2f}ì´ˆ)")
        else:
            html_texts = [""] * len(titles)

        # í† í°í™” (ì œëª© + ë³¸ë¬¸ + HTML í…ìŠ¤íŠ¸)
        logger.info(f"   ğŸ”¤ í† í°í™” ì¤€ë¹„ ì¤‘ ({len(titles)}ê°œ ë¬¸ì„œ)...")
        tokenize_start = time.time()

        # âœ… 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ê²°í•©
        logger.info(f"      [1/2] í…ìŠ¤íŠ¸ ê²°í•© ì¤‘...")
        combined_texts = []
        for i, (title, text) in enumerate(zip(titles, texts)):
            html_text = html_texts[i] if i < len(html_texts) else ""
            combined = f"{title} {text} {html_text}".strip()
            combined_texts.append(combined)

        logger.info(f"      [1/2] í…ìŠ¤íŠ¸ ê²°í•© ì™„ë£Œ ({len(combined_texts)}ê°œ)")

        # âœ… 2ë‹¨ê³„: ë³‘ë ¬ í† í°í™” (ì‹¤ì œ í˜•íƒœì†Œ ë¶„ì„ - ì‹œê°„ ì†Œìš”!)
        logger.info(f"      [2/2] ë³‘ë ¬ í† í°í™” ì§„í–‰ ì¤‘ ({get_safe_cpu_count()}ì½”ì–´, Mecab í˜•íƒœì†Œ ë¶„ì„)...")
        logger.info(f"      â³ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1-2ë¶„ (13000ê°œ ê¸°ì¤€)")

        parallel_start = time.time()
        # ğŸš€ ìµœì í™” 1: chunksize ì¶”ê°€ (í”„ë¡œì„¸ìŠ¤ ìƒì„± ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”)
        chunksize = max(1, len(combined_texts) // (get_safe_cpu_count() * 10))
        logger.info(f"      ğŸ“¦ Batch í¬ê¸°: {chunksize} (í”„ë¡œì„¸ìŠ¤ í†µì‹  ìµœì†Œí™”)")

        with Pool(processes=get_safe_cpu_count(), initializer=_set_global_query_transformer, initargs=(self.query_transformer,)) as pool:
            self.tokenized_documents = pool.map(_tokenize_combined_text, combined_texts, chunksize=chunksize)

        parallel_time = time.time() - parallel_start
        logger.info(f"      [2/2] ë³‘ë ¬ í† í°í™” ì™„ë£Œ! ({parallel_time:.2f}ì´ˆ, {len(combined_texts)/parallel_time:.0f}ë¬¸ì„œ/ì´ˆ)")

        tokenize_time = time.time() - tokenize_start
        logger.info(f"   âœ… í† í°í™” ì™„ë£Œ ({tokenize_time:.2f}ì´ˆ, ì†ë„: {len(titles)/tokenize_time:.0f}ë¬¸ì„œ/ì´ˆ)")

        # Redis ìºì‹œ ì—…ë°ì´íŠ¸ (v2 êµ¬ì¡°)
        if self.redis_client:
            try:
                cache_obj = {
                    "tokenized_documents": self.tokenized_documents,
                    "html_texts": html_texts,
                    "doc_count": len(titles)
                }
                # 24ì‹œê°„ ìœ íš¨
                self.redis_client.setex(self.cache_key, 86400, pickle.dumps(cache_obj))

                cache_size = len(pickle.dumps(cache_obj)) / (1024 * 1024)  # MB
                logger.info(f"ğŸ’¾ Redis BM25 ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(self.tokenized_documents)}ê°œ, {cache_size:.2f}MB)")
            except Exception as e:
                logger.warning(f"âš ï¸  Redis BM25 ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

        self.bm25_index = BM25Okapi(self.tokenized_documents, k1=self.k1, b=self.b)
        html_count = sum(1 for h in self.htmls if h) if self.htmls else 0

        total_time = time.time() - start_time
        logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(titles)}ê°œ ë¬¸ì„œ, HTML êµ¬ì¡°: {html_count}ê°œ, {total_time:.2f}ì´ˆ)")
