"""
BM25 Retriever
BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ í´ë˜ìŠ¤
"""

import numpy as np
import pickle
from rank_bm25 import BM25Okapi
from typing import List, Tuple
import logging
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class BM25Retriever:
    """
    BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í´ë˜ìŠ¤

    ì œëª©, ë³¸ë¬¸, HTML êµ¬ì¡°í™” ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ í† í°í™”í•˜ê³ , BM25 ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬
    ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    ê°œì„ ì‚¬í•­:
    - ì œëª©ë¿ë§Œ ì•„ë‹ˆë¼ ë³¸ë¬¸ë„ ê²€ìƒ‰í•˜ì—¬ ì²¨ë¶€íŒŒì¼ ë‚´ìš©ë„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - HTML êµ¬ì¡°í™” ë°ì´í„°(í‘œ ë“±)ë„ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    """

    def __init__(self,
                 titles: List[str],
                 texts: List[str],
                 urls: List[str],
                 dates: List[str],
                 query_transformer,
                 similarity_adjuster,
                 htmls: List[str] = None,
                 k1: float = 1.5,
                 b: float = 0.75,
                 redis_client = None):
        """
        BM25Retriever ì´ˆê¸°í™”

        Args:
            titles: ë¬¸ì„œ ì œëª© ë¦¬ìŠ¤íŠ¸
            texts: ë¬¸ì„œ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸
            urls: ë¬¸ì„œ URL ë¦¬ìŠ¤íŠ¸
            dates: ë¬¸ì„œ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
            query_transformer: ì§ˆë¬¸ì„ ëª…ì‚¬ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (transformed_query)
            similarity_adjuster: ìœ ì‚¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ (adjust_similarity_scores)
            htmls: HTML êµ¬ì¡°í™” ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì„ íƒ, í‘œ ê²€ìƒ‰ ê°œì„ ìš©)
            k1: BM25 k1 íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 1.5)
            b: BM25 b íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 0.75)
            redis_client: Redis í´ë¼ì´ì–¸íŠ¸ (ì„ íƒ, ìºì‹±ìš©)
        """
        self.titles = titles
        self.texts = texts
        self.urls = urls
        self.dates = dates
        self.htmls = htmls if htmls else []
        self.query_transformer = query_transformer
        self.similarity_adjuster = similarity_adjuster
        self.k1 = k1
        self.b = b
        self.redis_client = redis_client
        
        # ìºì‹œ í‚¤ ì„¤ì •
        self.cache_key = "bm25_tokenized_documents"

        # BM25 ì¸ë±ìŠ¤ ìƒì„± (ì œëª© + ë³¸ë¬¸ + HTML í…ìŠ¤íŠ¸ ê²°í•©í•˜ì—¬ ê²€ìƒ‰)
        self.tokenized_documents = []
        loaded_from_cache = False

        # 1. Redis ìºì‹œ í™•ì¸
        if self.redis_client:
            try:
                cached_data = self.redis_client.get(self.cache_key)
                if cached_data:
                    cached_tokens = pickle.loads(cached_data)
                    # ë¬¸ì„œ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (ê°„ë‹¨í•œ ìœ íš¨ì„± ê²€ì‚¬)
                    if len(cached_tokens) == len(titles):
                        self.tokenized_documents = cached_tokens
                        loaded_from_cache = True
                        logger.info(f"ğŸš€ Redisì—ì„œ BM25 í† í° ë¡œë“œ ì™„ë£Œ! ({len(self.tokenized_documents)}ê°œ)")
                    else:
                        logger.warning(f"âš ï¸  BM25 ìºì‹œ ê°œìˆ˜ ë¶ˆì¼ì¹˜ (ìºì‹œ: {len(cached_tokens)}, í˜„ì¬: {len(titles)}). ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"âš ï¸  Redisì—ì„œ BM25 í† í° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 2. ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if not loaded_from_cache:
            logger.info("ğŸ”„ BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘ (ì œëª©+ë³¸ë¬¸+HTML ê²€ìƒ‰)...")
            for i, (title, text) in enumerate(zip(titles, texts)):
                # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                html_text = ""
                if self.htmls and i < len(self.htmls) and self.htmls[i]:
                    try:
                        soup = BeautifulSoup(self.htmls[i], 'html.parser')
                        html_text = soup.get_text(separator=' ', strip=True)
                    except:
                        html_text = ""

                # ì œëª© + ë³¸ë¬¸ + HTML í…ìŠ¤íŠ¸ ê²°í•©
                combined = f"{title} {text} {html_text}".strip()
                self.tokenized_documents.append(query_transformer(combined))
            
            # 3. Redisì— ì €ì¥
            if self.redis_client:
                try:
                    # 24ì‹œê°„ ìœ íš¨
                    self.redis_client.setex(self.cache_key, 86400, pickle.dumps(self.tokenized_documents))
                    logger.info(f"ğŸ’¾ Redisì— BM25 í† í° ì €ì¥ ì™„ë£Œ ({len(self.tokenized_documents)}ê°œ)")
                except Exception as e:
                    logger.warning(f"âš ï¸  Redisì— BM25 í† í° ì €ì¥ ì‹¤íŒ¨: {e}")

        self.bm25_index = BM25Okapi(self.tokenized_documents, k1=k1, b=b)
        html_count = sum(1 for h in self.htmls if h) if self.htmls else 0
        logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ({len(titles)}ê°œ ë¬¸ì„œ, HTML êµ¬ì¡°: {html_count}ê°œ)")

    def search(self,
               query_nouns: List[str],
               top_k: int = 25,
               normalize_factor: float = 24.0) -> Tuple[List[Tuple], np.ndarray]:
        """
        BM25 ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query_nouns: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 25)
            normalize_factor: ìœ ì‚¬ë„ ì •ê·œí™” íŒ©í„° (ê¸°ë³¸ê°’: 24.0)

        Returns:
            Tuple[List[Tuple], np.ndarray]:
                - ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (title, date, text, url)
                - ì¡°ì •ëœ ìœ ì‚¬ë„ ë°°ì—´
        """
        # BM25 ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self.bm25_index.get_scores(query_nouns)

        # ìœ ì‚¬ë„ ì •ê·œí™”
        similarities = similarities / normalize_factor

        # ìœ ì‚¬ë„ ì¡°ì • (ì œëª©-ë³¸ë¬¸ ë§¤ì¹­, í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë“±)
        adjusted_similarities = self.similarity_adjuster(
            query_nouns,
            self.titles,
            self.texts,
            similarities
        )

        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ (ë‚´ë¦¼ì°¨ìˆœ)
        top_indices = np.argsort(adjusted_similarities)[-top_k:][::-1]

        # ê²°ê³¼ ë¬¸ì„œ ìƒì„±
        results = [
            (self.titles[i], self.dates[i], self.texts[i], self.urls[i])
            for i in top_indices
        ]

        logger.debug(f"âœ… BM25 ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ë°˜í™˜")

        return results, adjusted_similarities

    def get_similarity_score(self, query_nouns: List[str], doc_index: int) -> float:
        """
        íŠ¹ì • ë¬¸ì„œì— ëŒ€í•œ BM25 ìœ ì‚¬ë„ ì ìˆ˜ ë°˜í™˜

        Args:
            query_nouns: ê²€ìƒ‰ ì§ˆë¬¸ì˜ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            doc_index: ë¬¸ì„œ ì¸ë±ìŠ¤

        Returns:
            float: BM25 ìœ ì‚¬ë„ ì ìˆ˜
        """
        similarities = self.bm25_index.get_scores(query_nouns)
        return similarities[doc_index]

    def update_index(self,
                     titles: List[str],
                     texts: List[str],
                     urls: List[str],
                     dates: List[str],
                     htmls: List[str] = None):
        """
        BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ë¬¸ì„œ ì¶”ê°€/ì‚­ì œ ì‹œ ì‚¬ìš©)

        Args:
            titles: ìƒˆë¡œìš´ ë¬¸ì„œ ì œëª© ë¦¬ìŠ¤íŠ¸
            texts: ìƒˆë¡œìš´ ë¬¸ì„œ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸
            urls: ìƒˆë¡œìš´ ë¬¸ì„œ URL ë¦¬ìŠ¤íŠ¸
            dates: ìƒˆë¡œìš´ ë¬¸ì„œ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
            htmls: HTML êµ¬ì¡°í™” ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
        """
        logger.info("ğŸ”„ BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")

        self.titles = titles
        self.texts = texts
        self.urls = urls
        self.dates = dates
        self.htmls = htmls if htmls else []

        # ì œëª© + ë³¸ë¬¸ + HTML í…ìŠ¤íŠ¸ ê²°í•©í•˜ì—¬ ì¸ë±ìŠ¤ ìƒì„±
        self.tokenized_documents = []
        for i, (title, text) in enumerate(zip(titles, texts)):
            # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            html_text = ""
            if self.htmls and i < len(self.htmls) and self.htmls[i]:
                try:
                    soup = BeautifulSoup(self.htmls[i], 'html.parser')
                    html_text = soup.get_text(separator=' ', strip=True)
                except:
                    html_text = ""

            # ì œëª© + ë³¸ë¬¸ + HTML í…ìŠ¤íŠ¸ ê²°í•©
            combined = f"{title} {text} {html_text}".strip()
            self.tokenized_documents.append(self.query_transformer(combined))

        # Redis ìºì‹œ ì—…ë°ì´íŠ¸
        if self.redis_client:
            try:
                # 24ì‹œê°„ ìœ íš¨
                self.redis_client.setex(self.cache_key, 86400, pickle.dumps(self.tokenized_documents))
                logger.info(f"ğŸ’¾ Redis BM25 í† í° ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(self.tokenized_documents)}ê°œ)")
            except Exception as e:
                logger.warning(f"âš ï¸  Redis BM25 í† í° ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

        self.bm25_index = BM25Okapi(self.tokenized_documents, k1=self.k1, b=self.b)
        html_count = sum(1 for h in self.htmls if h) if self.htmls else 0
        logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(titles)}ê°œ ë¬¸ì„œ, HTML êµ¬ì¡°: {html_count}ê°œ)")
