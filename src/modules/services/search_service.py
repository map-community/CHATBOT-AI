"""
Search Service

ë¬¸ì„œ ê²€ìƒ‰, ë­í‚¹, ì¬ì •ë ¬ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤
"""
import logging
import re
from typing import Tuple, List, Optional
from datetime import datetime

from modules.constants import (
    NOTICE_BASE_URL,
    COMPANY_BASE_URL,
    SEMINAR_BASE_URL
)

logger = logging.getLogger(__name__)


class SearchService:
    """
    ë¬¸ì„œ ê²€ìƒ‰ ë° ë­í‚¹ ì„œë¹„ìŠ¤

    Responsibilities:
    - BM25 + Dense Retrieval ê²€ìƒ‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
    - ìµœê·¼ ê³µì§€ì‚¬í•­/ì±„ìš©/ì„¸ë¯¸ë‚˜ íŠ¹ë³„ ì²˜ë¦¬
    - Recency Boosting (ë‚ ì§œ ê¸°ë°˜ ì ìˆ˜ ë¶€ìŠ¤íŒ…)
    - URL ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    """

    def __init__(self, storage_manager):
        """
        Args:
            storage_manager: StorageManager ì¸ìŠ¤í„´ìŠ¤
        """
        self.storage = storage_manager

        # URL ìƒìˆ˜ (constants.pyì—ì„œ import)
        self.NOTICE_BASE_URL = NOTICE_BASE_URL
        self.COMPANY_BASE_URL = COMPANY_BASE_URL
        self.SEMINAR_BASE_URL = SEMINAR_BASE_URL

    def search_documents(
        self,
        user_question: str,
        transformed_query_fn,
        find_url_fn
    ) -> Tuple[Optional[List], Optional[List]]:
        """
        ë©”ì¸ ê²€ìƒ‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

        Args:
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            transformed_query_fn: ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ (ai_modules.transformed_query)
            find_url_fn: URL ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜ (ai_modules.find_url)

        Returns:
            Tuple[List, List]: (ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ì¿¼ë¦¬ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸)
                ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ (None, None) ë°˜í™˜

        Process:
            1. Query noun extraction (ëª…ì‚¬ ì¶”ì¶œ)
            2. Recent notices handling (ìµœê·¼ ê³µì§€ì‚¬í•­ íŠ¹ë³„ ì²˜ë¦¬)
            3. BM25 + Dense search (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
            4. Combine results (ê²°ê³¼ ê²°í•©)
            5. Recency boosting (ë‚ ì§œ ë¶€ìŠ¤íŒ…)
            6. URL deduplication (ì¤‘ë³µ ì œê±°)
        """
        import time

        # 1. Query Noun Extraction
        noun_time = time.time()
        query_noun = transformed_query_fn(user_question)
        query_noun_time = time.time() - noun_time
        print(f"ëª…ì‚¬í™” ë³€í™˜ ì‹œê°„ : {query_noun_time}")

        if not query_noun:
            return None, None

        # 2. Recent Notices Handling (ìµœê·¼ ê³µì§€ì‚¬í•­/ì±„ìš©/ì„¸ë¯¸ë‚˜ íŠ¹ë³„ ì²˜ë¦¬)
        recent_docs, key = self._handle_recent_notices(
            user_question, query_noun, find_url_fn
        )
        if recent_docs:
            return recent_docs, key

        # 3. BM25 Search
        bm_title_time = time.time()
        bm25_docs, bm25_similarities = self._bm25_search(query_noun)
        bm_title_f_time = time.time() - bm_title_time
        print(f"bm25 ë¬¸ì„œ ë½‘ëŠ”ì‹œê°„: {bm_title_f_time}")

        # 4. Dense Retrieval
        dense_time = time.time()
        dense_docs = self._dense_search(user_question, query_noun)
        pinecone_time = time.time() - dense_time
        print(f"íŒŒì¸ì½˜ì—ì„œ top k ë½‘ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ {pinecone_time}")

        # 5. Combine Results
        combine_time = time.time()
        combined_docs = self._combine_results(
            dense_docs, bm25_docs, bm25_similarities, query_noun, user_question
        )
        combine_f_time = time.time() - combine_time
        print(f"Bm25ë‘ pinecone ê²°í•© ì‹œê°„: {combine_f_time}")

        # 6. Recency Boosting
        boosted_docs = self._apply_recency_boost(combined_docs)

        # 7. URL Deduplication
        final_docs = self._deduplicate_by_url(boosted_docs)

        return final_docs, query_noun

    def _handle_recent_notices(
        self,
        user_question: str,
        query_noun: List[str],
        find_url_fn
    ) -> Tuple[Optional[List], Optional[List]]:
        """
        ìµœê·¼ ê³µì§€ì‚¬í•­/ì±„ìš©/ì„¸ë¯¸ë‚˜ íŠ¹ë³„ ì²˜ë¦¬

        Args:
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            query_noun: ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            find_url_fn: URL ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜

        Returns:
            Tuple[Optional[List], Optional[List]]: (ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸)
                íŠ¹ë³„ ì²˜ë¦¬ ëŒ€ìƒì´ ì•„ë‹ˆë©´ (None, None) ë°˜í™˜
        """
        import time

        # ë¶ˆìš©ì–´ ì œê±°
        remove_noticement = [
            'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ë‚´ìš©', 'ì œì¼', 'ê°€ì¥', 'ê³µê³ ', 'ê³µì§€ì‚¬í•­', 'í•„ë…',
            'ì²¨ë¶€íŒŒì¼', 'ìˆ˜ì—…', 'ì—…ë°ì´íŠ¸', 'ì»´í“¨í„°í•™ë¶€', 'ì»´í•™', 'ìƒìœ„', 'ì •ë³´',
            'ê´€ë ¨', 'ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°', 'ê³µì§€ì‚¬í•­', 'ì±„ìš©', 'ê³µê³ ',
            'ìµœê·¼', 'ìµœì‹ ', 'ì§€ê¸ˆ', 'í˜„ì¬'
        ]
        query_nouns = [noun for noun in query_noun if noun not in remove_noticement]

        # ê°œìˆ˜ ì¶”ì¶œ
        numbers = 5  # ê¸°ë³¸ 5ê°œ
        check_num = 0
        for noun in query_nouns:
            if 'ê°œ' in noun:
                num = re.findall(r'\d+', noun)
                if num:
                    numbers = int(num[0])
                    check_num = 1

        # ìµœê·¼ ê³µì§€ì‚¬í•­/ì±„ìš©/ì„¸ë¯¸ë‚˜ ì§ˆë¬¸ íŒë³„
        has_category = any(
            keyword in query_noun
            for keyword in ['ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°', 'ê³µì§€ì‚¬í•­', 'ì±„ìš©', 'ê³µê³ ']
        )
        has_recent = any(
            keyword in query_noun
            for keyword in ['ìµœê·¼', 'ìµœì‹ ', 'ì§€ê¸ˆ', 'í˜„ì¬']
        )

        # íŠ¹ë³„ ì²˜ë¦¬ ì¡°ê±´: (ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ + ìµœê·¼ í‚¤ì›Œë“œ + ëª…ì‚¬ ê±°ì˜ ì—†ìŒ) OR ê°œìˆ˜ ì§€ì •
        if not (has_category and has_recent and len(query_nouns) < 1 or check_num == 1):
            return None, None

        # ìºì‹œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        titles_from_pinecone = self.storage.cached_titles
        texts_from_pinecone = self.storage.cached_texts
        urls_from_pinecone = self.storage.cached_urls
        dates_from_pinecone = self.storage.cached_dates

        # 0ê°œ ìš”ì²­ (íŠ¹ìˆ˜ ì¼€ì´ìŠ¤)
        if numbers == 0:
            keys = ['ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°', 'ê³µì§€ì‚¬í•­', 'ì±„ìš©']
            return None, [keyword for keyword in keys if keyword in user_question]

        # ì¹´í…Œê³ ë¦¬ë³„ URL ê²€ìƒ‰
        return_docs = []
        key = None
        recent_time = time.time()

        if 'ê³µì§€ì‚¬í•­' in query_noun:
            key = ['ê³µì§€ì‚¬í•­']
            notice_url = self.NOTICE_BASE_URL + "&wr_id="
            return_docs = find_url_fn(
                notice_url, titles_from_pinecone, dates_from_pinecone,
                texts_from_pinecone, urls_from_pinecone, numbers
            )

        if 'ì±„ìš©' in query_noun:
            key = ['ì±„ìš©']
            company_url = self.COMPANY_BASE_URL + "&wr_id="
            return_docs = find_url_fn(
                company_url, titles_from_pinecone, dates_from_pinecone,
                texts_from_pinecone, urls_from_pinecone, numbers
            )

        other_key = ['ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'íŠ¹ê°•', 'ê°•ì—°']
        if any(keyword in query_noun for keyword in other_key):
            seminar_url = self.SEMINAR_BASE_URL + "&wr_id="
            key = [keyword for keyword in other_key if keyword in user_question]
            return_docs = find_url_fn(
                seminar_url, titles_from_pinecone, dates_from_pinecone,
                texts_from_pinecone, urls_from_pinecone, numbers
            )

        recent_finish_time = time.time() - recent_time
        print(f"ìµœê·¼ ê³µì§€ì‚¬í•­ ë¬¸ì„œ ë½‘ëŠ” ì‹œê°„ {recent_finish_time}")

        if len(return_docs) > 0:
            return return_docs, key

        return None, None

    def _bm25_search(self, query_noun: List[str]) -> Tuple[List, List]:
        """
        BM25 ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query_noun: ì¿¼ë¦¬ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            Tuple[List, List]: (BM25 ê²€ìƒ‰ ê²°ê³¼, ìœ ì‚¬ë„ ë¦¬ìŠ¤íŠ¸)
        """
        return self.storage.bm25_retriever.search(
            query_nouns=query_noun,
            top_k=50,  # âœ¨ 25â†’50 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ìœ„í•œ í›„ë³´êµ° í™•ëŒ€
            normalize_factor=24.0
        )

    def _dense_search(self, user_question: str, query_noun: List[str]) -> List:
        """
        Dense Retrieval ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            query_noun: ì¿¼ë¦¬ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            List: Dense ê²€ìƒ‰ ê²°ê³¼
        """
        return self.storage.dense_retriever.search(
            user_question=user_question,
            query_nouns=query_noun,
            top_k=50  # âœ¨ 30â†’50 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ìœ„í•œ í›„ë³´êµ° í™•ëŒ€
        )

    def _combine_results(
        self,
        dense_results: List,
        bm25_results: List,
        bm25_similarities: List,
        query_nouns: List[str],
        user_question: str
    ) -> List:
        """
        BM25ì™€ Dense Retrieval ê²°ê³¼ ê²°í•©

        Args:
            dense_results: Dense ê²€ìƒ‰ ê²°ê³¼
            bm25_results: BM25 ê²€ìƒ‰ ê²°ê³¼
            bm25_similarities: BM25 ìœ ì‚¬ë„ ë¦¬ìŠ¤íŠ¸
            query_nouns: ì¿¼ë¦¬ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            List: ê²°í•©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        titles_from_pinecone = self.storage.cached_titles

        return self.storage.document_combiner.combine(
            dense_results=dense_results,
            bm25_results=bm25_results,
            bm25_similarities=bm25_similarities,
            titles_from_pinecone=titles_from_pinecone,
            query_nouns=query_nouns,
            user_question=user_question,
            top_k=30  # âœ¨ 20â†’30 ì¦ê°€: URL ì¤‘ë³µ ì œê±° ì „ í›„ë³´êµ° í™•ëŒ€
        )

    def _apply_recency_boost(self, docs: List) -> List:
        """
        ë‚ ì§œ ë¶€ìŠ¤íŒ… ì ìš© (ìµœì‹  ë¬¸ì„œ ìš°ì„ )

        Args:
            docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ [(score, title, date, text, url), ...]

        Returns:
            List: ë¶€ìŠ¤íŒ… ì ìš© í›„ ì¬ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        def calculate_recency_boost(doc_date_str: str) -> float:
            """
            ë¬¸ì„œ ë‚ ì§œì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê³„ì‚°

            Args:
                doc_date_str: ISO 8601 í˜•ì‹ ë‚ ì§œ ë¬¸ìì—´

            Returns:
                float: ë¶€ìŠ¤íŒ… ê°€ì¤‘ì¹˜
                    - 6ê°œì›” ì´ë‚´: 1.5 (+50%)
                    - 1ë…„ ì´ë‚´: 1.3 (+30%)
                    - 2ë…„ ì´ë‚´: 1.1 (+10%)
                    - 2ë…„ ì´ìƒ: 0.9 (-10%)
            """
            try:
                current_date = datetime.now()
                doc_date = datetime.fromisoformat(doc_date_str.replace('+09:00', ''))

                # ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)
                days_old = (current_date - doc_date).days

                # ê°€ì¤‘ì¹˜ ê³„ì‚°
                if days_old < 0:  # ë¯¸ë˜ ë‚ ì§œ (ì˜¤ë¥˜)
                    return 1.0
                elif days_old <= 180:  # 6ê°œì›” ì´ë‚´
                    return 1.5  # 50% ë¶€ìŠ¤íŒ…
                elif days_old <= 365:  # 1ë…„ ì´ë‚´
                    return 1.3  # 30% ë¶€ìŠ¤íŒ…
                elif days_old <= 730:  # 2ë…„ ì´ë‚´
                    return 1.1  # 10% ë¶€ìŠ¤íŒ…
                else:  # 2ë…„ ì´ìƒ
                    return 0.9  # 10% íŒ¨ë„í‹°

            except Exception as e:
                logger.debug(f"ë‚ ì§œ ë¶€ìŠ¤íŒ… ê³„ì‚° ì‹¤íŒ¨: {doc_date_str} - {e}")
                return 1.0  # ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½

        # ë¶€ìŠ¤íŒ… ì ìš©
        boosted_docs = []
        for score, title, date, text, url in docs:
            boost = calculate_recency_boost(date)
            boosted_score = score * boost
            boosted_docs.append((boosted_score, title, date, text, url))

        # ë¶€ìŠ¤íŒ…ëœ ì ìˆ˜ë¡œ ì¬ì •ë ¬
        boosted_docs.sort(key=lambda x: x[0], reverse=True)

        logger.info(
            f"ğŸš€ ë‚ ì§œ ë¶€ìŠ¤íŒ… ì™„ë£Œ "
            f"(ìµœì‹  ë¬¸ì„œ ìš°ì„ : 6ê°œì›” ì´ë‚´ +50%, 1ë…„ ì´ë‚´ +30%)"
        )

        return boosted_docs

    def _deduplicate_by_url(self, docs: List) -> List:
        """
        URL ê¸°ë°˜ ì¤‘ë³µ ì œê±°

        ê°™ì€ ê²Œì‹œê¸€ì˜ ì„œë¡œ ë‹¤ë¥¸ ì²­í¬ë¥¼ ì œê±°í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ë‹¤ì–‘ì„± í™•ë³´
        ê°™ì€ URLì´ë©´ ìµœê³  ì ìˆ˜ ì²­í¬ë§Œ ì„ íƒ

        Args:
            docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ [(score, title, date, text, url), ...]

        Returns:
            List: ì¤‘ë³µ ì œê±° í›„ Top 20 ë¬¸ì„œ
        """
        import time

        dedup_time = time.time()

        seen_urls = {}  # {url: (score, title, date, text, url)}
        deduplicated_docs = []
        duplicate_count = 0
        original_count = len(docs)

        for score, title, date, text, url in docs:
            if url in seen_urls:
                # ê°™ì€ URLì´ ì´ë¯¸ ìˆìŒ â†’ ì ìˆ˜ ë¹„êµ
                existing_score = seen_urls[url][0]

                if score > existing_score:
                    # ë” ë†’ì€ ì ìˆ˜ë©´ ê¸°ì¡´ ë¬¸ì„œ ì œê±°í•˜ê³  ìƒˆ ë¬¸ì„œ ì¶”ê°€
                    deduplicated_docs.remove(seen_urls[url])
                    deduplicated_docs.append((score, title, date, text, url))
                    seen_urls[url] = (score, title, date, text, url)
                    logger.debug(
                        f"ğŸ”„ URL ì¤‘ë³µ - ë” ë†’ì€ ì ìˆ˜ë¡œ êµì²´: {title[:30]}... "
                        f"({existing_score:.2f} â†’ {score:.2f})"
                    )
                else:
                    # ë‚®ì€ ì ìˆ˜ë©´ ë¬´ì‹œ
                    duplicate_count += 1
                    logger.debug(
                        f"â­ï¸  URL ì¤‘ë³µ ì œê±°: {title[:30]}... "
                        f"(ì ìˆ˜: {score:.2f} < {existing_score:.2f})"
                    )
            else:
                # ìƒˆ URLì´ë©´ ì¶”ê°€
                seen_urls[url] = (score, title, date, text, url)
                deduplicated_docs.append((score, title, date, text, url))

        # ì ìˆ˜ìˆœ ì¬ì •ë ¬ í›„ Top 20
        deduplicated_docs.sort(key=lambda x: x[0], reverse=True)
        final_docs = deduplicated_docs[:20]

        dedup_f_time = time.time() - dedup_time
        unique_urls = len(seen_urls)
        print(
            f"URL ì¤‘ë³µ ì œê±°: {dedup_f_time:.4f}ì´ˆ "
            f"(ì›ë³¸: {original_count}ê°œ â†’ ì¤‘ë³µ {duplicate_count}ê°œ ì œê±° â†’ "
            f"ìµœì¢…: {len(final_docs)}ê°œ ì„œë¡œ ë‹¤ë¥¸ ê²Œì‹œê¸€, ê³ ìœ  URL {unique_urls}ê°œ)"
        )

        return final_docs
