"""
Response Service

ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±ì„ ì´ê´„í•˜ëŠ” ì„œë¹„ìŠ¤
ê²€ìƒ‰, Reranking, LLM ë‹µë³€ ìƒì„±, ì‘ë‹µ êµ¬ì¡°í™” ë“±ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
"""
import time
import json
import re
import logging
from typing import Dict, Any, List, Tuple, Optional

from modules.constants import (
    NOTICE_BASE_URL,
    COMPANY_BASE_URL,
    SEMINAR_BASE_URL,
    PROFESSOR_BASE_URL
)
from modules.utils.pipeline_logger import get_pipeline_logger

logger = logging.getLogger(__name__)
pipeline_log = get_pipeline_logger("modules")


class ResponseService:
    """
    ì‘ë‹µ ìƒì„± ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì„œë¹„ìŠ¤

    Responsibilities:
    - ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì „ì²´ ê´€ë¦¬
    - ê²€ìƒ‰ â†’ Reranking â†’ LLM ë‹µë³€ â†’ ì‘ë‹µ êµ¬ì¡°í™”
    - íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (í‚¤ì›Œë“œ ì „ìš© ì¿¼ë¦¬, ì´ë¯¸ì§€ ì „ìš© ë“±)
    """

    def __init__(self, storage_manager, search_service, llm_service):
        """
        Args:
            storage_manager: StorageManager ì¸ìŠ¤í„´ìŠ¤
            search_service: SearchService ì¸ìŠ¤í„´ìŠ¤
            llm_service: LLMService ì¸ìŠ¤í„´ìŠ¤
        """
        self.storage = storage_manager
        self.search_service = search_service
        self.llm_service = llm_service

    def generate_response(
        self,
        question: str,
        transformed_query_fn,
        find_url_fn,
        minimum_similarity_score: float,
        minimum_reranker_score: float = 0.0  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ (ì‚¬ìš© ì•ˆí•¨)
    ) -> Dict[str, Any]:
        """
        ë©”ì¸ ì‘ë‹µ ìƒì„± íŒŒì´í”„ë¼ì¸

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            transformed_query_fn: ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
            find_url_fn: URL ê²€ìƒ‰ í•¨ìˆ˜
            minimum_similarity_score: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (ì‚¬ìš© ì•ˆí•¨, í•˜ìœ„ í˜¸í™˜ì„±ë§Œ)
            minimum_reranker_score: ì‚¬ìš© ì•ˆí•¨ (í•˜ìœ„ í˜¸í™˜ì„±ë§Œ)

        Returns:
            Dict: ì‘ë‹µ JSON
                {
                    "answer": str,
                    "answerable": bool,
                    "references": str,
                    "disclaimer": str,
                    "images": List[str]
                }
        """
        s_time = time.time()

        # ============================================================
        # PHASE 1: ì§ˆë¬¸ ì „ì²˜ë¦¬ (Question Preprocessing)
        # ============================================================
        pipeline_log.phase_start(
            phase_num=1,
            title="ì§ˆë¬¸ ì „ì²˜ë¦¬ (Question Preprocessing)",
            purpose="ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œì™€ ì‹œê°„ ë§¥ë½ì„ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ìµœì í™”"
        )

        pipeline_log.input("ì‚¬ìš©ì ì§ˆë¬¸", question, truncate=100)

        # ì‹œê°„ ì˜ë„ íŒŒì‹±
        from datetime import datetime
        temporal_filter = self.llm_service.parse_temporal_intent(question, datetime.now())

        if temporal_filter:
            pipeline_log.metric("ì‹œê°„ ì˜ë„ ê°ì§€", "YES")
            pipeline_log.debug_data("Temporal Filter", {
                "year": temporal_filter.get('year', 'ë¯¸ì§€ì •'),
                "semester": temporal_filter.get('semester', 'ë¯¸ì§€ì •'),
                "is_ongoing": temporal_filter.get('is_ongoing', False)
            })
        else:
            pipeline_log.metric("ì‹œê°„ ì˜ë„ ê°ì§€", "NO")

        # ë¬¸ì„œ ê²€ìƒ‰ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        with pipeline_log.timer("ì´ˆê¸° ê²€ìƒ‰ (BM25 + Dense Retrieval)"):
            top_doc, query_noun = self.search_service.search_documents(
                user_question=question,
                transformed_query_fn=transformed_query_fn,
                find_url_fn=find_url_fn
            )

        pipeline_log.output("ì¶”ì¶œëœ í‚¤ì›Œë“œ", query_noun)
        pipeline_log.metric("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", len(top_doc) if top_doc else 0, "ê°œ")

        pipeline_log.phase_end(
            phase_num=1,
            summary=f"{len(query_noun) if query_noun else 0}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ, {len(top_doc) if top_doc else 0}ê°œ ë¬¸ì„œ ê²€ìƒ‰"
        )

        # query_nounì´ ì—†ê±°ë‚˜ top_docì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if not query_noun or not top_doc or len(top_doc) == 0:
            return self._build_no_result_response()

        # í‚¤ì›Œë“œ ì „ìš© ì¿¼ë¦¬ ì²˜ë¦¬ (ì±„ìš©/ê³µì§€/ì„¸ë¯¸ë‚˜ ëª©ë¡)
        keyword_response = self._handle_keyword_only_query(top_doc, query_noun, question)
        if keyword_response:
            f_time = time.time() - s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return keyword_response

        top_docs = [list(doc) for doc in top_doc]

        # ============================================================
        # PHASE 2: Reranking (ë¬¸ì„œ ì¬ìˆœìœ„í™”)
        # ============================================================
        pipeline_log.phase_start(
            phase_num=2,
            title="Reranking (ë¬¸ì„œ ì¬ìˆœìœ„í™”)",
            purpose="Semantic ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•˜ì—¬ ì •í™•ë„ í–¥ìƒ"
        )

        # Reranking ì „ Top 10 í‘œì‹œ (ì—°ì‚°ì— ì‚¬ìš©ë˜ëŠ” ëª¨ë“  í•­ëª©)
        pipeline_log.ranking_table(
            title="Reranking ì „ ê²€ìƒ‰ ê²°ê³¼",
            items=[{
                "rank": i+1,
                "score": doc[0],
                "title": doc[1],
                "date": doc[2],
                "url": doc[4]
            } for i, doc in enumerate(top_docs[:10])],
            top_k=10
        )

        # Reranking ì ìš©
        top_docs, reranking_used = self._apply_reranking(top_docs, question)

        pipeline_log.metric("Reranker ì‚¬ìš© ì—¬ë¶€", "YES" if reranking_used else "NO")
        pipeline_log.phase_end(
            phase_num=2,
            summary=f"{'Reranking ì™„ë£Œ' if reranking_used else 'ì›ë³¸ ìˆœì„œ ìœ ì§€'} ({len(top_docs)}ê°œ ë¬¸ì„œ)"
        )

        # ============================================================
        # PHASE 3: Temporal Re-boosting (ì‹œê°„ ë§¥ë½ ë³´ì •)
        # ============================================================
        if temporal_filter and reranking_used:
            pipeline_log.phase_start(
                phase_num=3,
                title="Temporal Re-boosting (ì‹œê°„ ë§¥ë½ ë³´ì •)",
                purpose="Rerankerê°€ ë¬´ì‹œí•œ ì‹œê°„ ì •ë³´ë¥¼ ë‹¤ì‹œ ë°˜ì˜í•˜ì—¬ ìµœì‹ ì„±/ê´€ë ¨ì„± í–¥ìƒ"
            )

            top_docs = self._apply_temporal_reboosting(top_docs, temporal_filter, reranking_used)

            pipeline_log.phase_end(
                phase_num=3,
                summary="ì‹œê°„ ë§¥ë½ ê¸°ë°˜ ì ìˆ˜ ì¡°ì • ì™„ë£Œ"
            )
        else:
            top_docs = self._apply_temporal_reboosting(top_docs, temporal_filter, reranking_used)

        # âœ… í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§: ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì€ ì ìˆ˜ë§Œ ì‚¬ì „ ì œê±°
        # - Top-k ê¸°ë°˜ ì ‘ê·¼ì„ ìœ ì§€í•˜ë˜, "ì ˆëŒ€ ë¶ˆê°€ëŠ¥í•œ" ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
        # - BGE: ë§¤ìš° ë‚®ì€ ìŒìˆ˜ (-8 ì´í•˜), Cohere: ê±°ì˜ 0ì— ê°€ê¹Œìš´ ê°’ (0.01 ì´í•˜)
        # - ì´ˆê¸° ê²€ìƒ‰(BM25+Dense): 0.5 ì´í•˜ (ê±°ì˜ ê´€ë ¨ ì—†ìŒ)
        if top_docs and len(top_docs) > 0:
            top_score = top_docs[0][0]

            # Reranker ì‚¬ìš© ì‹œ: ê·¹ë‹¨ì  ì €ì ìˆ˜ í•„í„°ë§
            if reranking_used:
                # BGEëŠ” ìŒìˆ˜ë„ ê°€ëŠ¥, CohereëŠ” 0~1 ë²”ìœ„
                # ë§¤ìš° ë³´ìˆ˜ì ì¸ ì„ê³„ê°’: BGE -8 ì´í•˜, Cohere 0.01 ì´í•˜ë§Œ ì œê±°
                EXTREME_LOW_THRESHOLD = -8.0  # BGE ê¸°ì¤€
                if top_score < EXTREME_LOW_THRESHOLD:
                    logger.warning(f"âš ï¸ ê·¹ë‹¨ì  ì €ì ìˆ˜ ê°ì§€: {top_score:.4f} < {EXTREME_LOW_THRESHOLD}")
                    logger.warning(f"   â†’ ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ê³¼ ê±°ì˜ ë¬´ê´€í•  ê°€ëŠ¥ì„± ë†’ìŒ")
                    f_time = time.time() - s_time
                    print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                    return self._build_no_result_response()

            # ì´ˆê¸° ê²€ìƒ‰ ì‹œ: 0.5 ì´í•˜ë§Œ ì œê±° (BM25+Dense ìŠ¤ì¼€ì¼)
            else:
                INITIAL_SEARCH_LOW_THRESHOLD = 0.5
                if top_score < INITIAL_SEARCH_LOW_THRESHOLD:
                    logger.warning(f"âš ï¸ ì´ˆê¸° ê²€ìƒ‰ ì €ì ìˆ˜ ê°ì§€: {top_score:.4f} < {INITIAL_SEARCH_LOW_THRESHOLD}")
                    logger.warning(f"   â†’ ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ê³¼ ê±°ì˜ ë¬´ê´€í•  ê°€ëŠ¥ì„± ë†’ìŒ")
                    f_time = time.time() - s_time
                    print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                    return self._build_no_result_response()

        # âœ… Top-k ê¸°ë°˜ ì ‘ê·¼: ìƒëŒ€ì  ìˆœì„œ(Ranking)ë§Œ ì‹ ë¢°
        # ì°¸ê³ : BGE ë¦¬ë­ì»¤ ì•„í‹°í´ - "ì ˆëŒ€ì  ì„ê³„ê°’ì´ ì•„ë‹Œ ìƒëŒ€ì  ìˆœì„œë¡œ íŒë‹¨"
        if reranking_used:
            logger.info("âœ… Reranker ì‚¬ìš© â†’ Top-k ê¸°ë°˜ ìƒëŒ€ì  ìˆœì„œ ì‹ ë¢°")
            logger.info("   (ê·¹ë‹¨ì  ì €ì ìˆ˜ í•„í„°ë§ í›„, LLM answerableì´ ìµœì¢… íŒë‹¨)")
        else:
            logger.info("âœ… ì´ˆê¸° ê²€ìƒ‰ â†’ Top-k ì‚¬ìš©, LLMì— ì „ë‹¬")
            logger.info("   (ê·¹ë‹¨ì  ì €ì ìˆ˜ í•„í„°ë§ í›„, LLM answerableì´ ìµœì¢… íŒë‹¨)")

        # ============================================================
        # PHASE 4: ìµœì¢… ë¬¸ì„œ ì„ íƒ ë° ê²€ì¦
        # ============================================================
        pipeline_log.phase_start(
            phase_num=4,
            title="ìµœì¢… ë¬¸ì„œ ì„ íƒ ë° ê²€ì¦",
            purpose="Top-5 ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ì„ íƒ í›„ ì ìˆ˜ ê²€ì¦ ë° ë‹¤ì–‘ì„± í™•ì¸"
        )

        # Reranking í›„ Top 5 í‘œì‹œ (ë‹¤ì–‘ì„± í™•ì¸)
        seen_urls = set()
        unique_url_count = 0
        ranking_items = []

        for i, doc in enumerate(top_docs[:10]):  # Top 10ê¹Œì§€ í™•ì¸ (ì¤‘ë³µ ê³ ë ¤)
            score, title, date, text, url = doc[:5]

            # URL ì¤‘ë³µ ì²´í¬
            if url not in seen_urls:
                seen_urls.add(url)
                unique_url_count += 1
                marker = "ğŸ†•"  # ìƒˆë¡œìš´ URL
            else:
                marker = "ğŸ”"  # ì¤‘ë³µ URL (ê°™ì€ ë¬¸ì„œì˜ ë‹¤ë¥¸ ì²­í¬)

            ranking_items.append({
                "rank": i+1,
                "score": score,
                "title": title,
                "date": date,
                "url": url,
                "marker": marker
            })

        pipeline_log.ranking_table(
            title="ìµœì¢… ìˆœìœ„ (Reranking í›„)",
            items=ranking_items,
            top_k=10
        )

        pipeline_log.metric("ë¬¸ì„œ ë‹¤ì–‘ì„±", f"Top 10 ì¤‘ {unique_url_count}ê°œ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ")

        # âœ… ë³€ê²½: Top-5 ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ì¶”ì¶œ
        top_k_unique_docs = []
        seen_titles = set()

        for doc in top_docs:
            title = doc[1]
            # ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ê°™ì€ ê²Œì‹œê¸€ì˜ ë‹¤ë¥¸ ì²­í¬ëŠ” ë‚˜ì¤‘ì— í™•ì¥)
            if title not in seen_titles:
                seen_titles.add(title)
                top_k_unique_docs.append(doc)
                if len(top_k_unique_docs) >= 5:
                    break

        # Top-5 ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œë¥¼ í†µì¼ëœ ì–‘ì‹ìœ¼ë¡œ í‘œì‹œ
        pipeline_log.ranking_table(
            title="Top-5 ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ì„ íƒ (ìµœì¢… í™•ì¥ ëŒ€ìƒ)",
            items=[{
                "rank": i+1,
                "score": doc[0],
                "title": doc[1],
                "date": doc[2],
                "url": doc[4]
            } for i, doc in enumerate(top_k_unique_docs)],
            top_k=5
        )

        # Top-1 ì •ë³´ ì €ì¥ (ì´ë¯¸ì§€ ì¡°íšŒ ë° backward compatibility)
        final_score = top_k_unique_docs[0][0] if top_k_unique_docs else 0
        final_title = top_k_unique_docs[0][1] if top_k_unique_docs else "No content"
        final_date = top_k_unique_docs[0][2] if top_k_unique_docs else "No content"
        final_text = top_k_unique_docs[0][3] if top_k_unique_docs else "No content"
        final_url = top_k_unique_docs[0][4] if top_k_unique_docs else "No URL"
        final_image = []

        pipeline_log.phase_end(
            phase_num=4,
            summary=f"Top-5 ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ì„ íƒ ì™„ë£Œ ({len(top_k_unique_docs)}ê°œ)"
        )

        # MongoDB ì—°ê²° í™•ì¸ í›„ ì´ë¯¸ì§€ URL ì¡°íšŒ (Top-1 ë¬¸ì„œë§Œ, í•˜ìœ„í˜¸í™˜ì„±)
        final_image = self._fetch_images_from_mongodb(final_title)
        if not final_image:
            final_score = 0
            final_title = "No content"
            final_date = "No content"
            final_text = "No content"
            final_url = "No URL"
            final_image = ["No content"]

        # ì´ë¯¸ì§€ë§Œ ìˆê³  í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° (Top-kë¡œ ì„ íƒë˜ì—ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ë°˜í™˜)
        if final_image[0] != "No content" and final_text == "No content":
            only_image_response = {
                "answer": None,
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time = time.time() - s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return only_image_response

        # ============================================================
        # PHASE 5: ë¬¸ì„œ í™•ì¥ (Document Enrichment)
        # ============================================================
        pipeline_log.phase_start(
            phase_num=5,
            title="ë¬¸ì„œ í™•ì¥ (Document Enrichment)",
            purpose="Top-5 ë¬¸ì„œ ê°ê°ì˜ ëª¨ë“  ì²­í¬(ë³¸ë¬¸/ì²¨ë¶€íŒŒì¼/ì´ë¯¸ì§€) ìˆ˜ì§‘"
        )

        pipeline_log.input("ì„ íƒëœ ê³ ìœ  ë¬¸ì„œ ìˆ˜", f"{len(top_k_unique_docs)}ê°œ")
        for i, doc in enumerate(top_k_unique_docs, 1):
            title = doc[1]
            pipeline_log.substep(f"{i}ìœ„: {title[:50]}...")

        enriched_docs = self._enrich_with_same_document_chunks(top_k_unique_docs)

        pipeline_log.output("í™•ì¥ëœ ì´ ì²­í¬ ê°œìˆ˜", f"{len(enriched_docs)}ê°œ")
        pipeline_log.phase_end(
            phase_num=5,
            summary=f"Top-{len(top_k_unique_docs)}ê°œ ë¬¸ì„œ â†’ {len(enriched_docs)}ê°œ ì²­í¬ë¡œ í™•ì¥ ì™„ë£Œ"
        )

        # ============================================================
        # PHASE 6: LLM ë‹µë³€ ìƒì„± (Answer Generation)
        # ============================================================
        pipeline_log.phase_start(
            phase_num=6,
            title="LLM ë‹µë³€ ìƒì„± (Answer Generation)",
            purpose="í™•ì¥ëœ ë¬¸ì„œë¥¼ Contextë¡œ LLMì— ì „ë‹¬í•˜ì—¬ ìì—°ì–´ ë‹µë³€ ìƒì„±"
        )

        with pipeline_log.timer("QA Chain ìƒì„±"):
            qa_chain, relevant_docs, relevant_docs_content = self.llm_service.get_answer_from_chain(
                enriched_docs, question, query_noun, temporal_filter
            )

        pipeline_log.metric("LLM ì „ë‹¬ ë¬¸ì„œ ê°œìˆ˜", f"{len(relevant_docs) if relevant_docs else 0}ê°œ")
        pipeline_log.metric("LLM ì „ë‹¬ Context ê¸¸ì´", f"{len(relevant_docs_content) if relevant_docs_content else 0}ì")

        # âœ… LLMì— ì „ë‹¬ë˜ëŠ” ê° ë¬¸ì„œ ëª…í™•íˆ í‘œì‹œ
        if relevant_docs:
            pipeline_log.section("LLMì— ì „ë‹¬ë˜ëŠ” ë¬¸ì„œ ëª©ë¡", "ğŸ“‹")

            # ë¬¸ì„œ ì œëª©ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ
            doc_by_title = {}
            for doc in relevant_docs:
                title = doc.metadata.get('title', 'Unknown')
                source = doc.metadata.get('source', 'unknown')
                content_type = doc.metadata.get('content_type', 'unknown')

                if title not in doc_by_title:
                    doc_by_title[title] = {
                        'title': title,
                        'url': doc.metadata.get('url', 'N/A'),
                        'date': doc.metadata.get('date', 'N/A'),
                        'chunks': []
                    }

                # ê°œí–‰ ì œê±°í•˜ì—¬ í•œ ì¤„ë¡œ í‘œì‹œ
                content_preview = doc.page_content.replace('\n', ' ').replace('\r', ' ')[:100]
                doc_by_title[title]['chunks'].append({
                    'source': source,
                    'content_type': content_type,
                    'content': content_preview
                })

            # ë¬¸ì„œë³„ë¡œ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
            for idx, (title, info) in enumerate(doc_by_title.items(), 1):
                pipeline_log.substep(f"[ë¬¸ì„œ {idx}] {title[:70]}")
                pipeline_log.substep(f"   ğŸ“… ë‚ ì§œ: {info['date']}")
                pipeline_log.substep(f"   ğŸ”— URL: {info['url'][:80]}")
                pipeline_log.substep(f"   ğŸ“¦ ì²­í¬ ê°œìˆ˜: {len(info['chunks'])}ê°œ")

                # ê° ì²­í¬ì˜ íƒ€ì… í‘œì‹œ
                chunk_types = {}
                for chunk in info['chunks']:
                    source = chunk['source']
                    chunk_types[source] = chunk_types.get(source, 0) + 1

                chunk_summary = ", ".join([f"{src}: {cnt}ê°œ" for src, cnt in chunk_types.items()])
                pipeline_log.substep(f"   ğŸ·ï¸  ì²­í¬ êµ¬ì„±: {chunk_summary}")

                # ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
                if info['chunks']:
                    pipeline_log.substep(f"   ğŸ“„ ë¯¸ë¦¬ë³´ê¸°: {info['chunks'][0]['content']}...")

                # ë¬¸ì„œ êµ¬ë¶„ì„ 
                if idx < len(doc_by_title):
                    pipeline_log.substep("   " + "-" * 70)

        # êµìˆ˜ ì—°ë½ì²˜ íŠ¹ìˆ˜ ì²˜ë¦¬
        if final_url == PROFESSOR_BASE_URL + "&lang=kor" and any(keyword in query_noun for keyword in ['ì—°ë½ì²˜', 'ì „í™”', 'ë²ˆí˜¸', 'ì „í™”ë²ˆí˜¸']):
            data = {
                "answer": "í•´ë‹¹ êµìˆ˜ë‹˜ì€ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
                "answerable": False,  # ì—°ë½ì²˜ ì •ë³´ ì—†ìŒ
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time = time.time() - s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return data

        # ê³µì§€ì‚¬í•­ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        notice_url = NOTICE_BASE_URL
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨
        if not qa_chain or not relevant_docs:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì¡°ê±´ ì§„ì…!")
            logger.warning(f"   ì¡°ê±´: not qa_chain ({not qa_chain}) or not relevant_docs ({not relevant_docs})")
            logger.warning(f"   â†’ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ ì˜ˆì •")
            # Top-kë¡œ ì„ íƒë˜ì—ˆìœ¼ë¯€ë¡œ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë°˜í™˜
            if final_image[0] != "No content":
                data = {
                    "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "answerable": True,  # ì´ë¯¸ì§€ë¡œ ë‹µë³€ ì œê³µ
                    "references": final_url,
                    "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "images": final_image
                }
                f_time = time.time() - s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return data
            else:
                f_time = time.time() - s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return not_in_notices_response

        # âœ… Top-k ê¸°ë°˜ ì ‘ê·¼: ì ˆëŒ€ì  ì„ê³„ê°’ ì œê±°
        # Reranker/ì´ˆê¸°ê²€ìƒ‰ì´ ì´ë¯¸ ìƒëŒ€ì  ìˆœì„œë¡œ Top-k ì„ íƒ
        # ìµœì¢… íŒë‹¨ì€ LLMì˜ answerable í•„ë“œì— ìœ„ì„
        logger.info(f"âœ… Top-1 ë¬¸ì„œ ì„ íƒ ì™„ë£Œ (score: {final_score:.4f})")
        logger.info(f"   â†’ LLMì— ì „ë‹¬í•˜ì—¬ answerable íŒë‹¨ (ì ˆëŒ€ì  ì„ê³„ê°’ ì‚¬ìš© ì•ˆí•¨)")

        # LLM ë‹µë³€ ìƒì„± ì‹¤í–‰
        pipeline_log.substep("LLM ë‹µë³€ ìƒì„± ì‹œì‘...")

        with pipeline_log.timer("LLM ë‹µë³€ ìƒì„±"):
            answer_result = qa_chain.invoke(question)

        pipeline_log.output("LLM ë‹µë³€ ê¸¸ì´", f"{len(answer_result)}ì")
        pipeline_log.output("LLM ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°", answer_result[:150], truncate=150)

        pipeline_log.phase_end(
            phase_num=6,
            summary=f"LLM ë‹µë³€ ìƒì„± ì™„ë£Œ ({len(answer_result)}ì)"
        )

        # ============================================================
        # PHASE 7: ì‘ë‹µ êµ¬ì¡°í™” (Response Formatting)
        # ============================================================
        pipeline_log.phase_start(
            phase_num=7,
            title="ì‘ë‹µ êµ¬ì¡°í™” (Response Formatting)",
            purpose="LLM ë‹µë³€ì„ ê²€ì¦í•˜ê³  answerable íŒë‹¨, ì°¸ê³ ë¬¸ì„œ ë° ê²½ê³  ì¶”ê°€"
        )

        # ìµœì¢… ì‘ë‹µ ìƒì„±
        data = self._build_final_response(
            answer_result=answer_result,
            relevant_docs=relevant_docs,
            relevant_docs_content=relevant_docs_content,
            final_image=final_image,
            question=question,
            temporal_filter=temporal_filter,
            final_date=final_date
        )

        pipeline_log.metric("answerable íŒë‹¨", "YES" if data['answerable'] else "NO")
        pipeline_log.metric("ì´ë¯¸ì§€ ê°œìˆ˜", f"{len(data['images'])}ê°œ")

        pipeline_log.phase_end(
            phase_num=7,
            summary=f"ì‘ë‹µ êµ¬ì¡°í™” ì™„ë£Œ (answerable: {data['answerable']})"
        )

        # ============================================================
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
        # ============================================================
        f_time = time.time() - s_time
        pipeline_log.logger.info("")
        pipeline_log.logger.info("=" * 80)
        pipeline_log.logger.info(f"âœ… RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ì™„ë£Œ")
        pipeline_log.logger.info(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {f_time:.2f}ì´ˆ")
        pipeline_log.logger.info("=" * 80)

        return data

    def _handle_keyword_only_query(
        self,
        top_doc: List,
        query_noun: List[str],
        user_question: str
    ) -> Optional[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œ ì „ìš© ì¿¼ë¦¬ ì²˜ë¦¬ (ì±„ìš©/ê³µì§€/ì„¸ë¯¸ë‚˜ ëª©ë¡)

        Args:
            top_doc: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            query_noun: ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            Optional[Dict]: í‚¤ì›Œë“œ ì „ìš© ì‘ë‹µ ë˜ëŠ” None
        """
        if len(query_noun) == 1 and any(keyword in query_noun for keyword in ['ì±„ìš©', 'ê³µì§€ì‚¬í•­', 'ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'ê°•ì—°', 'íŠ¹ê°•']):
            seen_urls = set()  # ì´ë¯¸ ë³¸ URLì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©
            response = f"'{query_noun[0]}'ì— ëŒ€í•œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
            show_url = ""
            if top_doc != None:
                for title, date, _, url in top_doc:  # top_docì—ì„œ ì œëª©, ë‚ ì§œ, URL ì¶”ì¶œ
                    if url not in seen_urls:
                        response += f"ì œëª©: {title}, ë‚ ì§œ: {date} \n----------------------------------------------------\n"
                        seen_urls.add(url)  # URL ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            if 'ì±„ìš©' in query_noun:
                show_url = COMPANY_BASE_URL + "&wr_id="
            elif 'ê³µì§€ì‚¬í•­' in query_noun:
                show_url = NOTICE_BASE_URL + "&wr_id="
            else:
                show_url = SEMINAR_BASE_URL + "&wr_id="

            # ìµœì¢… data êµ¬ì¡° ìƒì„±
            return {
                "answer": response,
                "answerable": True,  # ëª©ë¡ ì œê³µ ì„±ê³µ
                "references": show_url,  # show_urlì„ ë„˜ê¸°ê¸°
                "disclaimer": "\n\ní•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": ["No content"]
            }

        return None

    def _apply_reranking(
        self,
        top_docs: List[List],
        question: str
    ) -> Tuple[List[List], bool]:
        """
        BGE-Rerankerë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™”

        Args:
            top_docs: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            Tuple[List[List], bool]: (ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, Reranking ì‚¬ìš© ì—¬ë¶€)
        """
        reranking_used = False
        if self.storage.reranker and len(top_docs) > 1:
            # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ Reranker ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            reranker_info = self.storage.reranker.get_model_info()
            reranker_name = reranker_info.get('name', 'Reranker')
            reranker_model = reranker_info.get('model', '')

            logger.info(f"ğŸ¯ {reranker_name} í™œì„±í™”! (ëª¨ë¸: {reranker_model})")
            rerank_time = time.time()
            logger.info(f"   ì…ë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ â†’ Reranking ì‹œì‘...")

            # RerankerëŠ” tuple ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ë³€í™˜
            top_docs_tuples = [tuple(doc) for doc in top_docs]

            # Reranking (ì–´ì°¨í”¼ 1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ Top 5ë¡œ ì••ì¶•)
            reranked_docs_tuples = self.storage.reranker.rerank(
                query=question,
                documents=top_docs_tuples,
                top_k=5  # ìµœëŒ€ 5ê°œë¡œ ì••ì¶• (1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ íš¨ìœ¨í™”)
            )

            # ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            top_docs = [list(doc) for doc in reranked_docs_tuples]
            reranking_used = True  # Reranking ì‚¬ìš©ë¨

            rerank_f_time = time.time() - rerank_time
            logger.info(f"   ì¶œë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ (ì²˜ë¦¬ ì‹œê°„: {rerank_f_time:.2f}ì´ˆ)")
            print(f"âœ… Reranking ì™„ë£Œ: {rerank_f_time:.2f}ì´ˆ")
        elif not self.storage.reranker:
            logger.info("â­ï¸  BGE-Reranker ë¹„í™œì„±í™” (ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨)")
            logger.info("   â†’ ì›ë³¸ ê²€ìƒ‰ ìˆœì„œ ìœ ì§€")
        elif len(top_docs) <= 1:
            logger.info("â­ï¸  BGE-Reranker ìŠ¤í‚µ (ë¬¸ì„œ 1ê°œ ì´í•˜)")
            logger.info("   â†’ Reranking ë¶ˆí•„ìš”")

        return top_docs, reranking_used

    def _apply_temporal_reboosting(
        self,
        top_docs: List[List],
        temporal_filter: Dict,
        reranking_used: bool
    ) -> List[List]:
        """
        Reranking í›„ ì‹œê°„ ë§¥ë½ ê¸°ë°˜ ì ìˆ˜ ì¬ì¡°ì •

        RerankerëŠ” semantic similarityë§Œ ê³ ë ¤í•˜ê³  ë‚ ì§œë¥¼ ë¬´ì‹œí•˜ë¯€ë¡œ,
        ì‚¬ìš©ìê°€ ëª…ì‹œí•œ ì‹œê°„ ì •ë³´(ë…„ë„/í•™ê¸°)ë‚˜ "í˜„ì¬ ì§„í–‰ì¤‘" ì˜ë„ì— ë”°ë¼ ë¶€ìŠ¤íŒ… ì ìš©

        Args:
            top_docs: Rerankingëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            temporal_filter: ì‹œê°„ ì˜ë„ íŒŒì‹± ê²°ê³¼
                - year: ëª…ì‹œëœ ë…„ë„ (ì˜ˆ: 2024)
                - semester: ëª…ì‹œëœ í•™ê¸° (ì˜ˆ: 1, 2)
                - is_ongoing: í˜„ì¬ ì§„í–‰ì¤‘ ì˜ë„
            reranking_used: Reranking ì‚¬ìš© ì—¬ë¶€

        Returns:
            List[List]: ì‹œê°„ ë§¥ë½ ê³ ë ¤í•˜ì—¬ ì¬ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        from datetime import datetime
        from dateutil.parser import parse

        # Reranking ì‚¬ìš© ì•ˆí–ˆê±°ë‚˜, ì‹œê°„ ì˜ë„ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not reranking_used or not temporal_filter:
            return top_docs

        # ëª…ì‹œì  ì‹œê°„ ì •ë³´ (year/semester) ë˜ëŠ” is_ongoingì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        has_explicit_time = temporal_filter.get('year') or temporal_filter.get('semester')
        has_ongoing = temporal_filter.get('is_ongoing')

        if not has_explicit_time and not has_ongoing:
            return top_docs

        logger.info("=" * 60)
        logger.info("ğŸ• Temporal Re-boosting ì‹œì‘ (Rerankerì˜ ì‹œê°„ ë¬´ì‹œ ë³´ì •)")

        current_date = datetime.now()
        current_year = current_date.year
        current_month = current_date.month

        # í˜„ì¬ í•™ê¸° íŒë‹¨ (3~8ì›”: 1í•™ê¸°, 9~2ì›”: 2í•™ê¸°)
        if 3 <= current_month <= 8:
            current_semester = 1
        else:
            current_semester = 2

        # ì‚¬ìš©ìê°€ ëª…ì‹œí•œ ì‹œê°„ ì •ë³´
        target_year = temporal_filter.get('year')
        target_semester = temporal_filter.get('semester')

        # ë¶€ìŠ¤íŒ… ëª¨ë“œ ê²°ì •
        if has_explicit_time:
            # Mode 1: Explicit Year/Semester (ëª…ì‹œì  ì‹œê°„ ì§€ì •)
            mode = "explicit"
            logger.info(f"   ëª¨ë“œ: Explicit Temporal Boosting")
            logger.info(f"   ì‚¬ìš©ì ì§€ì •: {target_year or 'ë¯¸ì§€ì •'}ë…„ {target_semester or 'ë¯¸ì§€ì •'}í•™ê¸°")
        else:
            # Mode 2: Ongoing (í˜„ì¬ ì§„í–‰ì¤‘ ì˜ë„)
            mode = "ongoing"
            target_year = current_year
            target_semester = current_semester
            logger.info(f"   ëª¨ë“œ: Ongoing Temporal Boosting")
            logger.info(f"   ì‚¬ìš©ì ì˜ë„: í˜„ì¬ ì§„í–‰ì¤‘ ì •ë³´ ì°¾ê¸° (is_ongoing=true)")

        logger.info(f"   í˜„ì¬: {current_year}ë…„ {current_semester}í•™ê¸° ({current_date.strftime('%Y-%m-%d')})")

        # Re-boosting ì „ Top 3 ë¡œê¹…
        logger.info(f"   ğŸ“Š Re-boosting ì „ Top 3:")
        for i, doc in enumerate(top_docs[:3]):
            score, title, date, _, _ = doc[:5]
            logger.info(f"      {i+1}ìœ„: [{score:.4f}] {title[:40]}... ({date})")

        # ê° ë¬¸ì„œì— ëŒ€í•´ ì‹œê°„ ë§¥ë½ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
        for doc in top_docs:
            original_score = doc[0]
            doc_date_str = doc[2]  # ISO 8601 í˜•ì‹ ë‚ ì§œ
            doc_title = doc[1]

            try:
                doc_date = parse(doc_date_str)
                doc_year = doc_date.year
                doc_month = doc_date.month

                # ë¬¸ì„œ í•™ê¸° íŒë‹¨
                if 3 <= doc_month <= 8:
                    doc_semester = 1
                else:
                    doc_semester = 2

                # ì‹œê°„ ë§¥ë½ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ê³„ì‚°
                boost_factor = 1.0
                reason = ""

                if mode == "explicit":
                    # âœ… Explicit Mode: ì‚¬ìš©ìê°€ ëª…ì‹œí•œ ë…„ë„/í•™ê¸°ì— ë¶€ìŠ¤íŒ…

                    # 1. Exact Match (ë…„ë„ + í•™ê¸° ëª¨ë‘ ì¼ì¹˜)
                    if (target_year and doc_year == target_year and
                        target_semester and doc_semester == target_semester):
                        boost_factor = 2.0  # 100% ë¶€ìŠ¤íŒ…
                        reason = f"ì •í™•íˆ ì¼ì¹˜ ({target_year}ë…„ {target_semester}í•™ê¸°)"

                    # 2. Year Match (ë…„ë„ë§Œ ì¼ì¹˜, í•™ê¸° ë¯¸ì§€ì • ë˜ëŠ” ë¶ˆì¼ì¹˜)
                    elif target_year and doc_year == target_year:
                        if target_semester is None:
                            boost_factor = 1.8  # 80% ë¶€ìŠ¤íŒ… (ë…„ë„ë§Œ ì§€ì •í–ˆê³  ì¼ì¹˜)
                            reason = f"ë…„ë„ ì¼ì¹˜ ({target_year}ë…„)"
                        else:
                            boost_factor = 1.3  # 30% ë¶€ìŠ¤íŒ… (ë…„ë„ ì¼ì¹˜, í•™ê¸° ë¶ˆì¼ì¹˜)
                            reason = f"ë…„ë„ë§Œ ì¼ì¹˜ ({target_year}ë…„, í•™ê¸° ë‹¤ë¦„)"

                    # 3. Semester Match (í•™ê¸°ë§Œ ì¼ì¹˜, ë…„ë„ ë¯¸ì§€ì • ë˜ëŠ” ë¶ˆì¼ì¹˜)
                    elif target_semester and doc_semester == target_semester:
                        if target_year is None:
                            boost_factor = 1.5  # 50% ë¶€ìŠ¤íŒ… (í•™ê¸°ë§Œ ì§€ì •í–ˆê³  ì¼ì¹˜)
                            reason = f"í•™ê¸° ì¼ì¹˜ ({target_semester}í•™ê¸°)"
                        else:
                            boost_factor = 0.9  # 10% í˜ë„í‹° (í•™ê¸° ì¼ì¹˜, ë…„ë„ ë¶ˆì¼ì¹˜)
                            reason = f"í•™ê¸°ë§Œ ì¼ì¹˜ ({target_semester}í•™ê¸°, ë…„ë„ ë‹¤ë¦„)"

                    # 4. ì™„ì „ ë¶ˆì¼ì¹˜
                    else:
                        boost_factor = 0.6  # 40% í˜ë„í‹°
                        reason = f"ë¶ˆì¼ì¹˜ (ë¬¸ì„œ: {doc_year}ë…„ {doc_semester}í•™ê¸°)"

                else:
                    # âœ… Ongoing Mode: í˜„ì¬ í•™ê¸°ì— ë¶€ìŠ¤íŒ… (ê¸°ì¡´ ë¡œì§)

                    # 1. í˜„ì¬ í•™ê¸° ë¬¸ì„œ: ê°•ë ¥í•œ ë¶€ìŠ¤íŒ…
                    if doc_year == current_year and doc_semester == current_semester:
                        boost_factor = 1.8  # 80% ë¶€ìŠ¤íŒ…
                        reason = f"í˜„ì¬ í•™ê¸° ({current_year}ë…„ {current_semester}í•™ê¸°)"

                    # 2. í˜„ì¬ ì—°ë„ ë‹¤ë¥¸ í•™ê¸°: ì¤‘ê°„ ë¶€ìŠ¤íŒ…
                    elif doc_year == current_year and doc_semester != current_semester:
                        boost_factor = 1.3  # 30% ë¶€ìŠ¤íŒ…
                        reason = f"í˜„ì¬ ì—°ë„ ë‹¤ë¥¸ í•™ê¸° ({current_year}ë…„ {doc_semester}í•™ê¸°)"

                    # 3. 1ë…„ ì „: ì•½ê°„ í˜ë„í‹°
                    elif doc_year == current_year - 1:
                        boost_factor = 0.85  # 15% í˜ë„í‹°
                        reason = f"1ë…„ ì „ ({doc_year}ë…„)"

                    # 4. 2ë…„ ì´ìƒ ì „: ê°•í•œ í˜ë„í‹°
                    elif doc_year < current_year - 1:
                        boost_factor = 0.6  # 40% í˜ë„í‹°
                        reason = f"2ë…„ ì´ìƒ ì „ ({doc_year}ë…„)"

                # ì ìˆ˜ ì¡°ì •
                doc[0] = original_score * boost_factor

                if boost_factor != 1.0:
                    logger.info(f"      ğŸ“… {doc_title[:30]}...")
                    logger.info(f"         {original_score:.4f} â†’ {doc[0]:.4f} (Ã—{boost_factor:.2f}, {reason})")

            except Exception as e:
                logger.warning(f"   âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {doc_date_str} ({e})")
                continue

        # ì¬ì •ë ¬ (ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
        top_docs.sort(key=lambda x: x[0], reverse=True)

        # Re-boosting í›„ Top 3 ë¡œê¹…
        logger.info(f"   ğŸ” Re-boosting í›„ Top 3:")
        for i, doc in enumerate(top_docs[:3]):
            score, title, date, _, _ = doc[:5]
            logger.info(f"      {i+1}ìœ„: [{score:.4f}] {title[:40]}... ({date})")

        logger.info("=" * 60)

        return top_docs

    def _enrich_with_same_document_chunks(
        self,
        unique_docs: List[List]
    ) -> List[List]:
        """
        Top-K ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ìˆ˜ì§‘ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€ OCR)

        Args:
            unique_docs: Top-K ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°ë¨)

        Returns:
            List[List]: ëª¨ë“  ë¬¸ì„œì˜ í™•ì¥ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        enrich_time = time.time()

        if not unique_docs:
            return []

        pipeline_log = get_pipeline_logger()
        all_enriched_docs = []
        seen_texts = set()  # ì „ì—­ ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°ìš©

        # ê° ê³ ìœ  ë¬¸ì„œì— ëŒ€í•´ ì²­í¬ ìˆ˜ì§‘
        for doc_idx, unique_doc in enumerate(unique_docs, 1):
            doc_score = unique_doc[0]
            doc_title = unique_doc[1]
            doc_url = unique_doc[4]

            wr_id = doc_url.split('&wr_id=')[-1] if '&wr_id=' in doc_url else doc_url.split('wr_id=')[-1] if 'wr_id=' in doc_url else None

            pipeline_log.substep(f"[{doc_idx}/{len(unique_docs)}] '{doc_title[:40]}...' ì²­í¬ ìˆ˜ì§‘ ì¤‘...")

            # ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ì°¾ê¸°
            doc_chunks = []
            matched_count = 0
            duplicate_count = 0

            for i, cached_title in enumerate(self.storage.cached_titles):
                # ì œëª© ê¸°ì¤€ ë§¤ì¹­ (ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ í¬í•¨)
                if cached_title == doc_title:
                    matched_count += 1

                    text = self.storage.cached_texts[i]
                    url = self.storage.cached_urls[i]
                    content_type = self.storage.cached_content_types[i] if i < len(self.storage.cached_content_types) else "unknown"
                    source = self.storage.cached_sources[i] if i < len(self.storage.cached_sources) else "unknown"

                    # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°
                    text_key = ''.join(text.split())  # ê³µë°± ì œê±° í›„ ë¹„êµ

                    if text_key not in seen_texts:
                        seen_texts.add(text_key)
                        doc_chunks.append((
                            doc_score,  # ì›ë³¸ ë¬¸ì„œì˜ ì ìˆ˜ ìœ ì§€
                            self.storage.cached_titles[i],
                            self.storage.cached_dates[i],
                            text,
                            url,
                            self.storage.cached_htmls[i] if i < len(self.storage.cached_htmls) else "",
                            content_type,
                            source,
                            self.storage.cached_attachment_types[i] if i < len(self.storage.cached_attachment_types) else ""
                        ))
                    else:
                        duplicate_count += 1

            # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
            original_post_count = sum(1 for chunk in doc_chunks if chunk[7] == "original_post")
            image_count = sum(1 for chunk in doc_chunks if chunk[7] == "image_ocr")
            attachment_count = sum(1 for chunk in doc_chunks if chunk[7] == "document_parse")

            pipeline_log.substep(
                f"   â†’ {len(doc_chunks)}ê°œ ì²­í¬ ìˆ˜ì§‘ "
                f"(ë³¸ë¬¸: {original_post_count}, ì´ë¯¸ì§€: {image_count}, ì²¨ë¶€: {attachment_count}, ì¤‘ë³µì œê±°: {duplicate_count})"
            )

            all_enriched_docs.extend(doc_chunks)

        enrich_f_time = time.time() - enrich_time
        pipeline_log.metric("ì´ ì²­í¬ ìˆ˜ì§‘ ì‹œê°„", f"{enrich_f_time:.2f}ì´ˆ")

        return all_enriched_docs

    def _fetch_images_from_mongodb(self, final_title: str) -> List[str]:
        """
        MongoDBì—ì„œ ì´ë¯¸ì§€ URL ì¡°íšŒ

        Args:
            final_title: ë¬¸ì„œ ì œëª©

        Returns:
            List[str]: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
        """
        final_image = []

        if self.storage.mongo_collection is not None:
            record = self.storage.mongo_collection.find_one({"title": final_title})
            if record:
                if isinstance(record["image_url"], list):
                    final_image.extend(record["image_url"])
                else:
                    final_image.append(record["image_url"])
                logger.info(f"   ì´ë¯¸ì§€: {len(final_image)}ê°œ")

                # HTML êµ¬ì¡° ì •ë³´ ë¡œê¹…
                if record.get("html"):
                    html_length = len(record["html"])
                    logger.info(f"   HTML êµ¬ì¡°: âœ… ìˆìŒ ({html_length}ì)")
                else:
                    logger.info(f"   HTML êµ¬ì¡°: âŒ ì—†ìŒ")

                # ì½˜í…ì¸  íƒ€ì… ë¡œê¹…
                content_type = record.get("content_type", "unknown")
                source = record.get("source", "unknown")
                logger.info(f"   ì½˜í…ì¸  íƒ€ì…: {content_type}")
                logger.info(f"   ì†ŒìŠ¤: {source}")
            else:
                print("ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì¡´ì¬ X")
                logger.warning(f"âš ï¸  MongoDBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {final_title}")
        else:
            logger.warning("âš ï¸  MongoDB ì—°ê²° ì—†ìŒ - ì´ë¯¸ì§€ URL ì¡°íšŒ ë¶ˆê°€")
            final_image = ["No content"]

        return final_image if final_image else ["No content"]

    def _build_no_result_response(self) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì‘ë‹µ ìƒì„±

        Returns:
            Dict: ì‘ë‹µ JSON
        """
        notice_url = NOTICE_BASE_URL
        return {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

    def _build_final_response(
        self,
        answer_result: str,
        relevant_docs: List,
        relevant_docs_content: str,
        final_image: List[str],
        question: str,
        temporal_filter: Dict = None,
        final_date: str = None
    ) -> Dict[str, Any]:
        """
        ìµœì¢… ì‘ë‹µ JSON ìƒì„±

        Args:
            answer_result: LLM ìƒì„± ë‹µë³€
            relevant_docs: ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            relevant_docs_content: í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸
            final_image: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
            question: ì‚¬ìš©ì ì§ˆë¬¸
            temporal_filter: ì‹œê°„ ì˜ë„ íŒŒì‹± ê²°ê³¼
            final_date: ì„ íƒëœ ë¬¸ì„œ ë‚ ì§œ

        Returns:
            Dict: ì‘ë‹µ JSON
        """
        # âœ… JSON íŒŒì‹± ì‹œë„ (LLMì´ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ í™•ì¸)
        llm_answerable = None  # LLMì´ íŒë‹¨í•œ answerable ê°’
        llm_answer_text = None  # LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸

        try:
            # JSON íŒŒì‹± ì‹œë„
            clean_result = answer_result.strip()
            if clean_result.startswith("```json"):
                clean_result = clean_result[7:]
            if clean_result.startswith("```"):
                clean_result = clean_result[3:]
            if clean_result.endswith("```"):
                clean_result = clean_result[:-3]
            clean_result = clean_result.strip()

            parsed = json.loads(clean_result)

            # JSON íŒŒì‹± ì„±ê³µ
            if "answerable" in parsed and "answer" in parsed:
                llm_answerable = parsed["answerable"]
                llm_answer_text = parsed["answer"]
                logger.info(f"âœ… JSON íŒŒì‹± ì„±ê³µ: answerable={llm_answerable}")
                logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
                logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")
            else:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ â†’ í´ë°± ì‚¬ìš©")

        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (LLMì´ í˜•ì‹ ì•ˆ ì§€í‚´) â†’ í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš©")
            logger.debug(f"   ì—ëŸ¬: {e}")
            logger.debug(f"   ì›ë³¸ ì‘ë‹µ: {answer_result[:200]}...")

        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ: ê¸°ì¡´ answer_result ì‚¬ìš©
        if llm_answer_text is None:
            llm_answer_text = answer_result
            logger.info(f"ğŸ’¬ LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ë¹„-JSON í˜•ì‹):")
            logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
            logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")

        logger.info(f"   ì‚¬ìš©ëœ ì°¸ê³ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")

        # ë‹µë³€ ê²€ì¦ ë° ê²½ê³  ì¶”ê°€ (ë²”ìš©)
        completeness_keywords = ['ì „ë¶€', 'ëª¨ë“ ', 'ëª¨ë‘', 'ë¹ ì§ì—†ì´', 'ì „ì²´', 'ë‹¤', 'ëª…ë‹¨', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ëˆ„êµ¬']
        has_completeness_request = any(keyword in question for keyword in completeness_keywords)

        # ì™„ì „ì„± ìš”êµ¬ + Contextì™€ ë‹µë³€ ì°¨ì´ê°€ í¬ë©´ ê²½ê³ 
        if has_completeness_request:
            # Contextì— ìˆëŠ” ìˆ«ì íŒ¨í„´ (í•™ë²ˆ, ë‚ ì§œ ë“±)
            context_numbers = len(re.findall(r'\b20\d{6,8}\b', relevant_docs_content))
            answer_numbers = len(re.findall(r'\b20\d{6,8}\b', llm_answer_text))

            logger.info(f"   ğŸ“Š ì™„ì „ì„± ê²€ì¦: Context {context_numbers}ê±´ / ë‹µë³€ {answer_numbers}ê±´")

            # Contextì˜ 50% ë¯¸ë§Œë§Œ ë‹µë³€ì— í¬í•¨ë˜ë©´ ê²½ê³ 
            if context_numbers >= 10 and answer_numbers < context_numbers * 0.5:
                logger.warning(f"   âš ï¸ ì™„ì „ì„± ìš”êµ¬í–ˆìœ¼ë‚˜ ë‹µë³€ ë¶ˆì™„ì „! LLMì´ ì„ì˜ë¡œ ìš”ì•½í•œ ê²ƒìœ¼ë¡œ íŒë‹¨")
                llm_answer_text += f"\n\nâš ï¸ ì¼ë¶€ ë‚´ìš©ì´ ìƒëµë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸ì„œ: ì•½ {context_numbers}ê±´ / ë‹µë³€: {answer_numbers}ê±´). ì „ì²´ ë‚´ìš©ì€ ì°¸ê³  URLì„ í™•ì¸í•˜ì„¸ìš”."

        doc_references = "\n".join([
            f"\nì°¸ê³  ë¬¸ì„œ URL: {doc.metadata['url']}"
            for doc in relevant_docs[:1] if doc.metadata.get('url') != 'No URL'
        ])

        # âœ… answerable ìµœì¢… íŒë‹¨
        if llm_answerable is not None:
            # JSON íŒŒì‹± ì„±ê³µ â†’ LLMì´ ì§ì ‘ íŒë‹¨í•œ ê°’ ì‚¬ìš©
            answerable = llm_answerable
            logger.info(f"âœ… answerable íŒë‹¨: JSON íŒŒì‹± ê²°ê³¼ ì‚¬ìš© (LLM ì§ì ‘ íŒë‹¨: {answerable})")

            # âœ… Safety Net: LLMì´ answerable=trueë¡œ íŒë‹¨í–ˆì§€ë§Œ ë‹µë³€ì— ë¶€ì • íŒ¨í„´ì´ ìˆìœ¼ë©´ falseë¡œ ë³´ì •
            if answerable:
                # ë¶€ì • íŒ¨í„´ ëª©ë¡ (í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)
                negative_patterns = [
                    "ì— ëŒ€í•œ ë‚´ìš©ì€ ì—†ìŠµë‹ˆë‹¤",
                    "ì— ëŒ€í•œ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤",
                    "ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "ëŠ” ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤",
                    "ëŠ” ì–¸ê¸‰ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤",
                    "ì—ì„œëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤",
                    "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"
                ]

                # ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ ë¶€ì • íŒ¨í„´ ê²€ì‚¬
                if any(pattern in llm_answer_text for pattern in negative_patterns):
                    logger.warning(f"âš ï¸ LLM answerable ì˜¤íŒ ê°ì§€ (ë¶€ì • íŒ¨í„´)!")
                    logger.warning(f"   - LLM íŒë‹¨: answerable=true")
                    logger.warning(f"   - í•˜ì§€ë§Œ ë‹µë³€ì— ë¶€ì • íŒ¨í„´ í¬í•¨: {[p for p in negative_patterns if p in llm_answer_text]}")
                    logger.warning(f"   - ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:200]}...")
                    logger.warning(f"   â†’ answerable=falseë¡œ ë³´ì •")
                    answerable = False

            # âœ… Temporal Validation: í˜„ì¬ ì§„í–‰ì¤‘ ì§ˆë¬¸ì¸ë° ê³¼ê±° ë°ì´í„°ë¡œ ë‹µë³€í•˜ë©´ false
            if answerable and temporal_filter and temporal_filter.get('is_ongoing') and final_date:
                from datetime import datetime
                from dateutil.parser import parse

                try:
                    doc_date = parse(final_date)
                    current_date = datetime.now()
                    doc_year = doc_date.year
                    current_year = current_date.year

                    # 1ë…„ ì´ìƒ ì°¨ì´ë‚˜ë©´ ê³¼ê±° ë°ì´í„°ë¡œ íŒë‹¨
                    if doc_year < current_year:
                        logger.warning(f"âš ï¸ LLM answerable ì˜¤íŒ ê°ì§€ (ì‹œê°„ ë§¥ë½ ë¶ˆì¼ì¹˜)!")
                        logger.warning(f"   - LLM íŒë‹¨: answerable=true")
                        logger.warning(f"   - ì‚¬ìš©ì ì˜ë„: í˜„ì¬ ì§„í–‰ì¤‘ ì •ë³´ (is_ongoing=true)")
                        logger.warning(f"   - ë¬¸ì„œ ë‚ ì§œ: {doc_year}ë…„ (í˜„ì¬: {current_year}ë…„)")
                        logger.warning(f"   - ì‹œê°„ ì°¨ì´: {current_year - doc_year}ë…„ ì „")
                        logger.warning(f"   â†’ answerable=falseë¡œ ë³´ì •")

                        # ë‹µë³€ì— ê³¼ê±° ë°ì´í„°ë¼ëŠ” ê²½ê³  ì¶”ê°€
                        year_diff = current_year - doc_year
                        warning_prefix = f"âš ï¸ ì£¼ì˜: ì œê³µëœ ë¬¸ì„œëŠ” {doc_year}ë…„ ìë£Œì…ë‹ˆë‹¤ ({year_diff}ë…„ ì „). "
                        if not llm_answer_text.startswith("âš ï¸"):
                            llm_answer_text = warning_prefix + llm_answer_text

                        # í˜„ì¬ ì •ë³´ëŠ” ìµœì‹  ê³µì§€ í™•ì¸ ì•ˆë‚´ ì¶”ê°€
                        if "ìµœì‹  ê³µì§€" not in llm_answer_text and "ê³µì§€ì‚¬í•­ì„ í™•ì¸" not in llm_answer_text:
                            llm_answer_text += f" í˜„ì¬ {current_year}ë…„ ì •ë³´ëŠ” ìµœì‹  ê³µì§€ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

                        answerable = False

                except Exception as e:
                    logger.warning(f"   âš ï¸ Temporal Validation ì‹¤íŒ¨: {e}")
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨ â†’ í´ë°±: íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ íŒë‹¨
            answer_start = llm_answer_text[:150]
            if answer_start.startswith("ì œê³µëœ ë¬¸ì„œì—ëŠ”") and any(phrase in answer_start for phrase in ["ì—†ìŠµë‹ˆë‹¤", "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"]):
                answerable = False
            else:
                answerable = True
            logger.info(f"âš ï¸ answerable íŒë‹¨: í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš© (ê²°ê³¼: {answerable})")

        if answerable:
            logger.info("âœ… LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤")
        else:
            logger.info("âŒ LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ˆë¬¸ ì‘ì„± ìš”ì²­ ì•ˆë‚´ í‘œì‹œ)")

        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        return {
            "answer": llm_answer_text,  # JSON íŒŒì‹±ëœ ë‹µë³€ ë˜ëŠ” ì›ë³¸ ë‹µë³€
            "answerable": answerable,  # ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€
            "references": doc_references,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
