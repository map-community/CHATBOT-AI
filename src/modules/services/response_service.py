"""
Response Service

ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±ì„ ì´ê´„í•˜ëŠ” ì„œë¹„ìŠ¤
ê²€ìƒ‰, Reranking, LLM ë‹µë³€ ìƒì„±, ì‘ë‹µ êµ¬ì¡°í™” ë“±ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
"""
import time
import json
import re
import logging
from typing import Dict, Any, List, Tuple, Optional

from modules.constants import (
    NOTICE_BASE_URL,
    COMPANY_BASE_URL,
    SEMINAR_BASE_URL,
    PROFESSOR_BASE_URL
)

logger = logging.getLogger(__name__)


class ResponseService:
    """
    ì‘ë‹µ ìƒì„± ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì„œë¹„ìŠ¤

    Responsibilities:
    - ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì „ì²´ ê´€ë¦¬
    - ê²€ìƒ‰ â†’ Reranking â†’ LLM ë‹µë³€ â†’ ì‘ë‹µ êµ¬ì¡°í™”
    - íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (í‚¤ì›Œë“œ ì „ìš© ì¿¼ë¦¬, ì´ë¯¸ì§€ ì „ìš© ë“±)
    """

    def __init__(self, storage_manager, search_service, llm_service):
        """
        Args:
            storage_manager: StorageManager ì¸ìŠ¤í„´ìŠ¤
            search_service: SearchService ì¸ìŠ¤í„´ìŠ¤
            llm_service: LLMService ì¸ìŠ¤í„´ìŠ¤
        """
        self.storage = storage_manager
        self.search_service = search_service
        self.llm_service = llm_service

    def generate_response(
        self,
        question: str,
        transformed_query_fn,
        find_url_fn,
        minimum_similarity_score: float,
        minimum_reranker_score: float = 0.0  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ (ì‚¬ìš© ì•ˆí•¨)
    ) -> Dict[str, Any]:
        """
        ë©”ì¸ ì‘ë‹µ ìƒì„± íŒŒì´í”„ë¼ì¸

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            transformed_query_fn: ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
            find_url_fn: URL ê²€ìƒ‰ í•¨ìˆ˜
            minimum_similarity_score: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (ì‚¬ìš© ì•ˆí•¨, í•˜ìœ„ í˜¸í™˜ì„±ë§Œ)
            minimum_reranker_score: ì‚¬ìš© ì•ˆí•¨ (í•˜ìœ„ í˜¸í™˜ì„±ë§Œ)

        Returns:
            Dict: ì‘ë‹µ JSON
                {
                    "answer": str,
                    "answerable": bool,
                    "references": str,
                    "disclaimer": str,
                    "images": List[str]
                }
        """
        s_time = time.time()

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹… (ê°€ì¥ ë¨¼ì €!)
        logger.info(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {question}")

        # âœ… ì‹œê°„ ì˜ë„ íŒŒì‹± (LLM ë‹µë³€ ì‹œ í™œìš©)
        from datetime import datetime
        temporal_filter = self.llm_service.parse_temporal_intent(question, datetime.now())

        best_time = time.time()
        top_doc, query_noun = self.search_service.search_documents(
            user_question=question,
            transformed_query_fn=transformed_query_fn,
            find_url_fn=find_url_fn
        )
        best_f_time = time.time() - best_time
        print(f"best_docs ë½‘ëŠ” ì‹œê°„:{best_f_time}")
        logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {query_noun}")

        # query_nounì´ ì—†ê±°ë‚˜ top_docì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if not query_noun or not top_doc or len(top_doc) == 0:
            return self._build_no_result_response()

        # í‚¤ì›Œë“œ ì „ìš© ì¿¼ë¦¬ ì²˜ë¦¬ (ì±„ìš©/ê³µì§€/ì„¸ë¯¸ë‚˜ ëª©ë¡)
        keyword_response = self._handle_keyword_only_query(top_doc, query_noun, question)
        if keyword_response:
            f_time = time.time() - s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return keyword_response

        top_docs = [list(doc) for doc in top_doc]

        # âœ… Reranking ì „ Top 5 ë¡œê¹…
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Reranking ì „ ê²€ìƒ‰ ê²°ê³¼ Top {min(5, len(top_docs))}:")
        for i, doc in enumerate(top_docs[:5]):
            score, title, date, text, url = doc[:5]
            logger.info(f"   {i+1}ìœ„: [{score:.4f}] {title[:50]}... ({date})")
        logger.info("=" * 60)

        # âœ… Reranking ì ìš©
        top_docs, reranking_used = self._apply_reranking(top_docs, question)

        # âœ… Top-k ê¸°ë°˜ ì ‘ê·¼: ìƒëŒ€ì  ìˆœì„œ(Ranking)ë§Œ ì‹ ë¢°, ì ˆëŒ€ì  ì„ê³„ê°’ ì œê±°
        # ì°¸ê³ : BGE ë¦¬ë­ì»¤ ì•„í‹°í´ - "ì ˆëŒ€ì  ì„ê³„ê°’ì´ ì•„ë‹Œ ìƒëŒ€ì  ìˆœì„œë¡œ íŒë‹¨"
        if reranking_used:
            logger.info("âœ… Reranker ì‚¬ìš© â†’ Top-k ê¸°ë°˜ ìƒëŒ€ì  ìˆœì„œ ì‹ ë¢°")
            logger.info("   (ì ˆëŒ€ì  ì„ê³„ê°’ ì œê±°, LLM answerableì´ ìµœì¢… íŒë‹¨)")
        else:
            logger.info("âœ… ì´ˆê¸° ê²€ìƒ‰ â†’ Top-k ì‚¬ìš©, LLMì— ì „ë‹¬")

        # âœ… Reranking í›„ Top 5 ë¡œê¹…
        logger.info("=" * 60)
        logger.info(f"ğŸ” Reranking í›„ ìµœì¢… ê²°ê³¼ Top {min(5, len(top_docs))}:")
        seen_urls = set()
        unique_url_count = 0
        for i, doc in enumerate(top_docs[:5]):
            score, title, date, text, url = doc[:5]

            # URL ì¤‘ë³µ ì²´í¬
            if url not in seen_urls:
                seen_urls.add(url)
                unique_url_count += 1
                url_marker = "ğŸ†•"  # ìƒˆë¡œìš´ URL
            else:
                url_marker = "ğŸ”"  # ì¤‘ë³µ URL (ê°™ì€ ë¬¸ì„œì˜ ë‹¤ë¥¸ ì²­í¬)

            logger.info(f"   {i+1}ìœ„: [{score:.4f}] {url_marker} {title[:50]}... ({date})")
            logger.info(f"      URL: {url}")

        logger.info(f"   ğŸ’¡ ë‹¤ì–‘ì„±: Top 5 ì¤‘ {unique_url_count}ê°œ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œ")
        logger.info("=" * 60)

        final_score = top_docs[0][0]
        final_title = top_docs[0][1]
        final_date = top_docs[0][2]
        final_text = top_docs[0][3]
        final_url = top_docs[0][4]
        final_image = []

        # ìµœì¢… ì„ íƒëœ ë¬¸ì„œ ì •ë³´ ë¡œê¹…
        logger.info(f"ğŸ“„ ìµœì¢… ì„ íƒ ë¬¸ì„œ:")
        logger.info(f"   ì œëª©: {final_title}")
        logger.info(f"   ë‚ ì§œ: {final_date}")
        logger.info(f"   ìœ ì‚¬ë„: {final_score:.4f}")
        logger.info(f"   URL: {final_url}")
        logger.info(f"   ë³¸ë¬¸ ê¸¸ì´: {len(final_text)}ì")
        if len(final_text) > 0:
            logger.info(f"   ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {final_text[:100]}...")

        # MongoDB ì—°ê²° í™•ì¸ í›„ ì´ë¯¸ì§€ URL ì¡°íšŒ
        final_image = self._fetch_images_from_mongodb(final_title)
        if not final_image:
            final_score = 0
            final_title = "No content"
            final_date = "No content"
            final_text = "No content"
            final_url = "No URL"
            final_image = ["No content"]

        # ì´ë¯¸ì§€ë§Œ ìˆê³  í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° (Top-kë¡œ ì„ íƒë˜ì—ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ë°˜í™˜)
        if final_image[0] != "No content" and final_text == "No content":
            only_image_response = {
                "answer": None,
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time = time.time() - s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return only_image_response

        # âœ… ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ìˆ˜ì§‘ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€ OCR)
        top_docs = self._enrich_with_same_document_chunks(top_docs)

        # QA Chain ìƒì„±
        chain_time = time.time()
        qa_chain, relevant_docs, relevant_docs_content = self.llm_service.get_answer_from_chain(
            top_docs, question, query_noun, temporal_filter
        )
        chain_f_time = time.time() - chain_time
        print(f"chain ìƒì„±í•˜ëŠ” ì‹œê°„: {chain_f_time}")

        # ğŸ” ë””ë²„ê¹…: get_answer_from_chain ë°˜í™˜ê°’ í™•ì¸
        logger.info(f"ğŸ” get_answer_from_chain ë°˜í™˜ê°’ í™•ì¸:")
        logger.info(f"   qa_chain: {type(qa_chain)} (None? {qa_chain is None})")
        logger.info(f"   relevant_docs: {type(relevant_docs)} (None? {relevant_docs is None}, ê°œìˆ˜: {len(relevant_docs) if relevant_docs else 0})")
        logger.info(f"   relevant_docs_content: {type(relevant_docs_content)} (None? {relevant_docs_content is None})")

        # êµìˆ˜ ì—°ë½ì²˜ íŠ¹ìˆ˜ ì²˜ë¦¬
        if final_url == PROFESSOR_BASE_URL + "&lang=kor" and any(keyword in query_noun for keyword in ['ì—°ë½ì²˜', 'ì „í™”', 'ë²ˆí˜¸', 'ì „í™”ë²ˆí˜¸']):
            data = {
                "answer": "í•´ë‹¹ êµìˆ˜ë‹˜ì€ ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n ìì„¸í•œ ì •ë³´ëŠ” êµìˆ˜ì§„ í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.",
                "answerable": False,  # ì—°ë½ì²˜ ì •ë³´ ì—†ìŒ
                "references": final_url,
                "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": final_image
            }
            f_time = time.time() - s_time
            print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
            return data

        # ê³µì§€ì‚¬í•­ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        notice_url = NOTICE_BASE_URL
        not_in_notices_response = {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨
        if not qa_chain or not relevant_docs:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì¡°ê±´ ì§„ì…!")
            logger.warning(f"   ì¡°ê±´: not qa_chain ({not qa_chain}) or not relevant_docs ({not relevant_docs})")
            logger.warning(f"   â†’ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ ì˜ˆì •")
            # Top-kë¡œ ì„ íƒë˜ì—ˆìœ¼ë¯€ë¡œ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë°˜í™˜
            if final_image[0] != "No content":
                data = {
                    "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ë¯¸ì§€ íŒŒì¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "answerable": True,  # ì´ë¯¸ì§€ë¡œ ë‹µë³€ ì œê³µ
                    "references": final_url,
                    "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "images": final_image
                }
                f_time = time.time() - s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return data
            else:
                f_time = time.time() - s_time
                print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
                return not_in_notices_response

        # âœ… Top-k ê¸°ë°˜ ì ‘ê·¼: ì ˆëŒ€ì  ì„ê³„ê°’ ì œê±°
        # Reranker/ì´ˆê¸°ê²€ìƒ‰ì´ ì´ë¯¸ ìƒëŒ€ì  ìˆœì„œë¡œ Top-k ì„ íƒ
        # ìµœì¢… íŒë‹¨ì€ LLMì˜ answerable í•„ë“œì— ìœ„ì„
        logger.info(f"âœ… Top-1 ë¬¸ì„œ ì„ íƒ ì™„ë£Œ (score: {final_score:.4f})")
        logger.info(f"   â†’ LLMì— ì „ë‹¬í•˜ì—¬ answerable íŒë‹¨ (ì ˆëŒ€ì  ì„ê³„ê°’ ì‚¬ìš© ì•ˆí•¨)")

        # LLMì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²½ìš°
        logger.info(f"âœ… ëª¨ë“  ì¡°ê±´ í†µê³¼! LLM ë‹µë³€ ìƒì„± ì‹œì‘...")
        answer_time = time.time()

        # qa_chain.invoke() ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        answer_result = qa_chain.invoke(question)

        answer_f_time = time.time() - answer_time
        print(f"ë‹µë³€ ìƒì„±í•˜ëŠ” ì‹œê°„: {answer_f_time}")

        # ìµœì¢… ì‘ë‹µ ìƒì„±
        data = self._build_final_response(
            answer_result=answer_result,
            relevant_docs=relevant_docs,
            relevant_docs_content=relevant_docs_content,
            final_image=final_image,
            question=question
        )

        f_time = time.time() - s_time
        logger.info(f"âœ… ì´ ì²˜ë¦¬ ì‹œê°„: {f_time:.2f}ì´ˆ")
        print(f"get_ai_message ì´ ëŒì•„ê°€ëŠ” ì‹œê°„ : {f_time}")
        return data

    def _handle_keyword_only_query(
        self,
        top_doc: List,
        query_noun: List[str],
        user_question: str
    ) -> Optional[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œ ì „ìš© ì¿¼ë¦¬ ì²˜ë¦¬ (ì±„ìš©/ê³µì§€/ì„¸ë¯¸ë‚˜ ëª©ë¡)

        Args:
            top_doc: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            query_noun: ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
            user_question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            Optional[Dict]: í‚¤ì›Œë“œ ì „ìš© ì‘ë‹µ ë˜ëŠ” None
        """
        if len(query_noun) == 1 and any(keyword in query_noun for keyword in ['ì±„ìš©', 'ê³µì§€ì‚¬í•­', 'ì„¸ë¯¸ë‚˜', 'í–‰ì‚¬', 'ê°•ì—°', 'íŠ¹ê°•']):
            seen_urls = set()  # ì´ë¯¸ ë³¸ URLì„ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©
            response = f"'{query_noun[0]}'ì— ëŒ€í•œ ì •ë³´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
            show_url = ""
            if top_doc != None:
                for title, date, _, url in top_doc:  # top_docì—ì„œ ì œëª©, ë‚ ì§œ, URL ì¶”ì¶œ
                    if url not in seen_urls:
                        response += f"ì œëª©: {title}, ë‚ ì§œ: {date} \n----------------------------------------------------\n"
                        seen_urls.add(url)  # URL ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            if 'ì±„ìš©' in query_noun:
                show_url = COMPANY_BASE_URL + "&wr_id="
            elif 'ê³µì§€ì‚¬í•­' in query_noun:
                show_url = NOTICE_BASE_URL + "&wr_id="
            else:
                show_url = SEMINAR_BASE_URL + "&wr_id="

            # ìµœì¢… data êµ¬ì¡° ìƒì„±
            return {
                "answer": response,
                "answerable": True,  # ëª©ë¡ ì œê³µ ì„±ê³µ
                "references": show_url,  # show_urlì„ ë„˜ê¸°ê¸°
                "disclaimer": "\n\ní•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                "images": ["No content"]
            }

        return None

    def _apply_reranking(
        self,
        top_docs: List[List],
        question: str
    ) -> Tuple[List[List], bool]:
        """
        BGE-Rerankerë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™”

        Args:
            top_docs: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            Tuple[List[List], bool]: (ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, Reranking ì‚¬ìš© ì—¬ë¶€)
        """
        reranking_used = False
        if self.storage.reranker and len(top_docs) > 1:
            # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ Reranker ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            reranker_info = self.storage.reranker.get_model_info()
            reranker_name = reranker_info.get('name', 'Reranker')
            reranker_model = reranker_info.get('model', '')

            logger.info(f"ğŸ¯ {reranker_name} í™œì„±í™”! (ëª¨ë¸: {reranker_model})")
            rerank_time = time.time()
            logger.info(f"   ì…ë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ â†’ Reranking ì‹œì‘...")

            # RerankerëŠ” tuple ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ë³€í™˜
            top_docs_tuples = [tuple(doc) for doc in top_docs]

            # Reranking (ì–´ì°¨í”¼ 1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ Top 5ë¡œ ì••ì¶•)
            reranked_docs_tuples = self.storage.reranker.rerank(
                query=question,
                documents=top_docs_tuples,
                top_k=5  # ìµœëŒ€ 5ê°œë¡œ ì••ì¶• (1ë“±ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ íš¨ìœ¨í™”)
            )

            # ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            top_docs = [list(doc) for doc in reranked_docs_tuples]
            reranking_used = True  # Reranking ì‚¬ìš©ë¨

            rerank_f_time = time.time() - rerank_time
            logger.info(f"   ì¶œë ¥: {len(top_docs)}ê°œ ë¬¸ì„œ (ì²˜ë¦¬ ì‹œê°„: {rerank_f_time:.2f}ì´ˆ)")
            print(f"âœ… Reranking ì™„ë£Œ: {rerank_f_time:.2f}ì´ˆ")
        elif not self.storage.reranker:
            logger.info("â­ï¸  BGE-Reranker ë¹„í™œì„±í™” (ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨)")
            logger.info("   â†’ ì›ë³¸ ê²€ìƒ‰ ìˆœì„œ ìœ ì§€")
        elif len(top_docs) <= 1:
            logger.info("â­ï¸  BGE-Reranker ìŠ¤í‚µ (ë¬¸ì„œ 1ê°œ ì´í•˜)")
            logger.info("   â†’ Reranking ë¶ˆí•„ìš”")

        return top_docs, reranking_used

    def _enrich_with_same_document_chunks(
        self,
        top_docs: List[List]
    ) -> List[List]:
        """
        ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ìˆ˜ì§‘ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€ OCR)

        Args:
            top_docs: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[List]: í™•ì¥ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        enrich_time = time.time()

        # Top ë¬¸ì„œì˜ URL ì¶”ì¶œ (ê²Œì‹œê¸€ URL)
        top_url = top_docs[0][4] if top_docs else None

        if not top_url:
            return top_docs

        # âœ… ë³€ê²½: URL ê¸°ë°˜ ë§¤ì¹­ ëŒ€ì‹  ì œëª© ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš©!
        top_title = top_docs[0][1]  # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì œëª©
        wr_id = top_url.split('&wr_id=')[-1] if '&wr_id=' in top_url else top_url.split('wr_id=')[-1] if 'wr_id=' in top_url else None

        logger.info(f"ğŸ” ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ ê²€ìƒ‰: ì œëª©='{top_title}' (wr_id={wr_id})")

        # ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ì°¾ê¸° (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€ OCR)
        enriched_docs = []
        seen_texts = set()  # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°ìš©

        # ë””ë²„ê¹…: ë§¤ì¹­ ìƒí™© ì¶”ì 
        total_checked = 0
        matched_count = 0
        duplicate_count = 0

        for i, url in enumerate(self.storage.cached_urls):
            # âœ… ê°™ì€ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸ (ì œëª© ê¸°ì¤€ - ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ í¬í•¨!)
            if self.storage.cached_titles[i] == top_title:
                total_checked += 1
                matched_count += 1

                text = self.storage.cached_texts[i]
                content_type = self.storage.cached_content_types[i] if i < len(self.storage.cached_content_types) else "unknown"
                source = self.storage.cached_sources[i] if i < len(self.storage.cached_sources) else "unknown"

                # ë””ë²„ê¹… ë¡œê·¸ (ì²˜ìŒ 5ê°œë§Œ)
                if matched_count <= 5:
                    html_data = self.storage.cached_htmls[i] if i < len(self.storage.cached_htmls) else ""
                    logger.info(f"   [{matched_count}] URL: {url[:80]}...")
                    logger.info(f"       íƒ€ì…: {content_type}, ì†ŒìŠ¤: {source}")
                    logger.info(f"       í…ìŠ¤íŠ¸: {len(text)}ì, HTML: {len(html_data)}ì")
                    logger.info(f"       ì¸ë±ìŠ¤: {i}")

                # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ì§€ ì•ŠìŒ! (ì¤‘ìš”: "No content"ë„ í¬í•¨)
                text_key = ''.join(text.split())  # ê³µë°± ì œê±° í›„ ë¹„êµ

                # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±° (ë¹ˆ ë¬¸ìì—´ì€ ì œì™¸í•˜ì§€ ì•ŠìŒ!)
                if text_key not in seen_texts:
                    seen_texts.add(text_key)
                    enriched_docs.append((
                        top_docs[0][0],  # ì ìˆ˜ëŠ” top ë¬¸ì„œì™€ ë™ì¼
                        self.storage.cached_titles[i],
                        self.storage.cached_dates[i],
                        text,
                        url,
                        self.storage.cached_htmls[i] if i < len(self.storage.cached_htmls) else "",
                        self.storage.cached_content_types[i] if i < len(self.storage.cached_content_types) else "unknown",
                        self.storage.cached_sources[i] if i < len(self.storage.cached_sources) else "unknown",
                        self.storage.cached_attachment_types[i] if i < len(self.storage.cached_attachment_types) else ""
                    ))
                else:
                    duplicate_count += 1

        logger.info(f"   ğŸ“Š ë§¤ì¹­ í†µê³„: ì „ì²´ {len(self.storage.cached_urls)}ê°œ ì¤‘ {matched_count}ê°œ ë§¤ì¹­, {duplicate_count}ê°œ ì¤‘ë³µ ì œê±°")

        # ì²­í¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ top_docsë¥¼ êµì²´ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼ + ì´ë¯¸ì§€)
        if enriched_docs:
            logger.info(f"ğŸ”§ ê°™ì€ ê²Œì‹œê¸€ì˜ ëª¨ë“  ì²­í¬ ìˆ˜ì§‘: {len(top_docs)}ê°œ â†’ {len(enriched_docs)}ê°œ")

            # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
            original_post_count = 0
            image_count = 0
            attachment_count = 0

            for i, (score, title, date, text, url, html, content_type, source, attachment_type) in enumerate(enriched_docs):
                if source == "original_post":
                    original_post_count += 1
                elif source == "image_ocr":
                    image_count += 1
                elif source == "document_parse":
                    attachment_count += 1

            logger.info(f"   ğŸ“¦ ë³¸ë¬¸ ì²­í¬: {original_post_count}ê°œ")
            logger.info(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ OCR ì²­í¬: {image_count}ê°œ")
            logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì²­í¬: {attachment_count}ê°œ")
            top_docs = enriched_docs
        else:
            logger.warning(f"âš ï¸  ê°™ì€ ê²Œì‹œê¸€ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤! wr_id={wr_id}")
            logger.warning(f"   Top URL: {top_url}")

        enrich_f_time = time.time() - enrich_time
        print(f"ì²­í¬ ìˆ˜ì§‘ ì‹œê°„: {enrich_f_time}")

        return top_docs

    def _fetch_images_from_mongodb(self, final_title: str) -> List[str]:
        """
        MongoDBì—ì„œ ì´ë¯¸ì§€ URL ì¡°íšŒ

        Args:
            final_title: ë¬¸ì„œ ì œëª©

        Returns:
            List[str]: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
        """
        final_image = []

        if self.storage.mongo_collection is not None:
            record = self.storage.mongo_collection.find_one({"title": final_title})
            if record:
                if isinstance(record["image_url"], list):
                    final_image.extend(record["image_url"])
                else:
                    final_image.append(record["image_url"])
                logger.info(f"   ì´ë¯¸ì§€: {len(final_image)}ê°œ")

                # HTML êµ¬ì¡° ì •ë³´ ë¡œê¹…
                if record.get("html"):
                    html_length = len(record["html"])
                    logger.info(f"   HTML êµ¬ì¡°: âœ… ìˆìŒ ({html_length}ì)")
                else:
                    logger.info(f"   HTML êµ¬ì¡°: âŒ ì—†ìŒ")

                # ì½˜í…ì¸  íƒ€ì… ë¡œê¹…
                content_type = record.get("content_type", "unknown")
                source = record.get("source", "unknown")
                logger.info(f"   ì½˜í…ì¸  íƒ€ì…: {content_type}")
                logger.info(f"   ì†ŒìŠ¤: {source}")
            else:
                print("ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ì¡´ì¬ X")
                logger.warning(f"âš ï¸  MongoDBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {final_title}")
        else:
            logger.warning("âš ï¸  MongoDB ì—°ê²° ì—†ìŒ - ì´ë¯¸ì§€ URL ì¡°íšŒ ë¶ˆê°€")
            final_image = ["No content"]

        return final_image if final_image else ["No content"]

    def _build_no_result_response(self) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì‘ë‹µ ìƒì„±

        Returns:
            Dict: ì‘ë‹µ JSON
        """
        notice_url = NOTICE_BASE_URL
        return {
            "answer": "í•´ë‹¹ ì§ˆë¬¸ì€ ê³µì§€ì‚¬í•­ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\n ìì„¸í•œ ì‚¬í•­ì€ ê³µì§€ì‚¬í•­ì„ ì‚´í´ë´ì£¼ì„¸ìš”.",
            "answerable": False,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            "references": notice_url,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": ["No content"]
        }

    def _build_final_response(
        self,
        answer_result: str,
        relevant_docs: List,
        relevant_docs_content: str,
        final_image: List[str],
        question: str
    ) -> Dict[str, Any]:
        """
        ìµœì¢… ì‘ë‹µ JSON ìƒì„±

        Args:
            answer_result: LLM ìƒì„± ë‹µë³€
            relevant_docs: ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            relevant_docs_content: í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸
            final_image: ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            Dict: ì‘ë‹µ JSON
        """
        # âœ… JSON íŒŒì‹± ì‹œë„ (LLMì´ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ í™•ì¸)
        llm_answerable = None  # LLMì´ íŒë‹¨í•œ answerable ê°’
        llm_answer_text = None  # LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸

        try:
            # JSON íŒŒì‹± ì‹œë„
            clean_result = answer_result.strip()
            if clean_result.startswith("```json"):
                clean_result = clean_result[7:]
            if clean_result.startswith("```"):
                clean_result = clean_result[3:]
            if clean_result.endswith("```"):
                clean_result = clean_result[:-3]
            clean_result = clean_result.strip()

            parsed = json.loads(clean_result)

            # JSON íŒŒì‹± ì„±ê³µ
            if "answerable" in parsed and "answer" in parsed:
                llm_answerable = parsed["answerable"]
                llm_answer_text = parsed["answer"]
                logger.info(f"âœ… JSON íŒŒì‹± ì„±ê³µ: answerable={llm_answerable}")
                logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
                logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")
            else:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ â†’ í´ë°± ì‚¬ìš©")

        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (LLMì´ í˜•ì‹ ì•ˆ ì§€í‚´) â†’ í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš©")
            logger.debug(f"   ì—ëŸ¬: {e}")
            logger.debug(f"   ì›ë³¸ ì‘ë‹µ: {answer_result[:200]}...")

        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ: ê¸°ì¡´ answer_result ì‚¬ìš©
        if llm_answer_text is None:
            llm_answer_text = answer_result
            logger.info(f"ğŸ’¬ LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ë¹„-JSON í˜•ì‹):")
            logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(llm_answer_text)}ì")
            logger.info(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {llm_answer_text[:150]}...")

        logger.info(f"   ì‚¬ìš©ëœ ì°¸ê³ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")

        # ë‹µë³€ ê²€ì¦ ë° ê²½ê³  ì¶”ê°€ (ë²”ìš©)
        completeness_keywords = ['ì „ë¶€', 'ëª¨ë“ ', 'ëª¨ë‘', 'ë¹ ì§ì—†ì´', 'ì „ì²´', 'ë‹¤', 'ëª…ë‹¨', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ëˆ„êµ¬']
        has_completeness_request = any(keyword in question for keyword in completeness_keywords)

        # ì™„ì „ì„± ìš”êµ¬ + Contextì™€ ë‹µë³€ ì°¨ì´ê°€ í¬ë©´ ê²½ê³ 
        if has_completeness_request:
            # Contextì— ìˆëŠ” ìˆ«ì íŒ¨í„´ (í•™ë²ˆ, ë‚ ì§œ ë“±)
            context_numbers = len(re.findall(r'\b20\d{6,8}\b', relevant_docs_content))
            answer_numbers = len(re.findall(r'\b20\d{6,8}\b', llm_answer_text))

            logger.info(f"   ğŸ“Š ì™„ì „ì„± ê²€ì¦: Context {context_numbers}ê±´ / ë‹µë³€ {answer_numbers}ê±´")

            # Contextì˜ 50% ë¯¸ë§Œë§Œ ë‹µë³€ì— í¬í•¨ë˜ë©´ ê²½ê³ 
            if context_numbers >= 10 and answer_numbers < context_numbers * 0.5:
                logger.warning(f"   âš ï¸ ì™„ì „ì„± ìš”êµ¬í–ˆìœ¼ë‚˜ ë‹µë³€ ë¶ˆì™„ì „! LLMì´ ì„ì˜ë¡œ ìš”ì•½í•œ ê²ƒìœ¼ë¡œ íŒë‹¨")
                llm_answer_text += f"\n\nâš ï¸ ì¼ë¶€ ë‚´ìš©ì´ ìƒëµë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸ì„œ: ì•½ {context_numbers}ê±´ / ë‹µë³€: {answer_numbers}ê±´). ì „ì²´ ë‚´ìš©ì€ ì°¸ê³  URLì„ í™•ì¸í•˜ì„¸ìš”."

        doc_references = "\n".join([
            f"\nì°¸ê³  ë¬¸ì„œ URL: {doc.metadata['url']}"
            for doc in relevant_docs[:1] if doc.metadata.get('url') != 'No URL'
        ])

        # âœ… answerable ìµœì¢… íŒë‹¨
        if llm_answerable is not None:
            # JSON íŒŒì‹± ì„±ê³µ â†’ LLMì´ ì§ì ‘ íŒë‹¨í•œ ê°’ ì‚¬ìš©
            answerable = llm_answerable
            logger.info(f"âœ… answerable íŒë‹¨: JSON íŒŒì‹± ê²°ê³¼ ì‚¬ìš© (LLM ì§ì ‘ íŒë‹¨: {answerable})")
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨ â†’ í´ë°±: íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ íŒë‹¨
            answer_start = llm_answer_text[:150]
            if answer_start.startswith("ì œê³µëœ ë¬¸ì„œì—ëŠ”") and any(phrase in answer_start for phrase in ["ì—†ìŠµë‹ˆë‹¤", "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"]):
                answerable = False
            else:
                answerable = True
            logger.info(f"âš ï¸ answerable íŒë‹¨: í´ë°± íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš© (ê²°ê³¼: {answerable})")

        if answerable:
            logger.info("âœ… LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤")
        else:
            logger.info("âŒ LLMì´ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ˆë¬¸ ì‘ì„± ìš”ì²­ ì•ˆë‚´ í‘œì‹œ)")

        # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê°ì²´ ìƒì„±
        return {
            "answer": llm_answer_text,  # JSON íŒŒì‹±ëœ ë‹µë³€ ë˜ëŠ” ì›ë³¸ ë‹µë³€
            "answerable": answerable,  # ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€
            "references": doc_references,
            "disclaimer": "í•­ìƒ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ URLë“¤ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "images": final_image
        }
