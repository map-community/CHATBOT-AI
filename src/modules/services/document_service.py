"""
Document Service

Pinecone ë° MongoDBì—ì„œ ë¬¸ì„œ ë°ì´í„°ë¥¼ í˜ì¹­í•˜ê³  ìºì‹±í•˜ëŠ” ì„œë¹„ìŠ¤
"""
import logging
import pickle
from typing import Tuple, List, Optional

logger = logging.getLogger(__name__)


class DocumentService:
    """
    ë¬¸ì„œ ë°ì´í„° ê´€ë¦¬ ì„œë¹„ìŠ¤

    Responsibilities:
    - Pineconeì—ì„œ ì „ì²´ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í˜ì¹­
    - MongoDBì—ì„œ HTML/Markdown ì½˜í…ì¸  ì¡°íšŒ
    - Redis ìºì‹± ê´€ë¦¬
    - StorageManager ìºì‹œ ì´ˆê¸°í™”
    """

    def __init__(self, storage_manager):
        """
        Args:
            storage_manager: StorageManager ì¸ìŠ¤í„´ìŠ¤
        """
        self.storage = storage_manager

    def fetch_all_documents(self) -> Tuple[List, ...]:
        """
        Pineconeì—ì„œ ì „ì²´ ë°ì´í„°(ì œëª©, í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)ë¥¼ ì¡°íšŒ

        Process:
        1. list() ë©”ì„œë“œ(Pagination)ë¡œ ëª¨ë“  ID ê°€ì ¸ì˜¤ê¸°
        2. fetch() ë©”ì„œë“œ(Batch)ë¡œ ë©”íƒ€ë°ì´í„° íš¨ìœ¨ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        3. html_availableì¸ ê²½ìš° MongoDBì—ì„œ ì‹¤ì œ HTML ì¡°íšŒ

        Returns:
            Tuple[List, ...]: (titles, texts, urls, dates, htmls, content_types,
                               sources, image_urls, attachment_urls, attachment_types)
        """
        logger.info("ğŸ”„ Pinecone ì „ì²´ ë°ì´í„° ì¡°íšŒ ì‹œì‘...")

        # MongoDB ì—°ê²° (HTML ì¡°íšŒìš©)
        mongo_collection = None
        try:
            if self.storage.mongo_collection is not None:
                mongo_collection = self.storage.mongo_collection.database["multimodal_cache"]
                logger.info("âœ… MongoDB ì—°ê²° ì„±ê³µ (HTML ì¡°íšŒìš©)")
        except Exception as e:
            logger.warning(f"âš ï¸  MongoDB ì—°ê²° ì‹¤íŒ¨ (HTML ì—†ì´ ì§„í–‰): {e}")

        # 1. ì „ì²´ ID ê°€ì ¸ì˜¤ê¸°
        all_ids = self._fetch_all_vector_ids()
        if not all_ids:
            return self._empty_result()

        # 2. ë°°ì¹˜ë¡œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        return self._fetch_metadata_in_batches(all_ids, mongo_collection)

    def _fetch_all_vector_ids(self) -> List[str]:
        """Pineconeì—ì„œ ëª¨ë“  ë²¡í„° ID ê°€ì ¸ì˜¤ê¸°"""
        all_ids = []

        try:
            for ids in self.storage.pinecone_index.list(namespace=""):
                all_ids.extend(ids)
            logger.info(f"ğŸ“Š ì´ {len(all_ids)}ê°œì˜ ë²¡í„° IDë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ ID ë¦¬ìŠ¤íŒ… ì‹¤íŒ¨: {e}")
            logger.error("ğŸ‘‰ 'requirements.txt'ì˜ pinecone ë²„ì „ì„ í™•ì¸í•˜ê³  ì¬ë¹Œë“œí•˜ì„¸ìš”.")
            return []

        if not all_ids:
            logger.warning("âš ï¸ ì¡°íšŒëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤.")

        return all_ids

    def _fetch_metadata_in_batches(
        self,
        all_ids: List[str],
        mongo_collection
    ) -> Tuple[List, ...]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë©”íƒ€ë°ì´í„° í˜ì¹­"""
        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        titles, texts, urls, dates = [], [], [], []
        htmls, content_types, sources = [], [], []
        image_urls, attachment_urls, attachment_types = [], [], []

        # í†µê³„ ì¹´ìš´í„°
        html_available_count = 0
        mongo_found_count = 0
        html_extracted_count = 0

        # 1,000ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 1000
        for i in range(0, len(all_ids), batch_size):
            logger.info(f"â³ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({i} / {len(all_ids)})")

            batch_ids = all_ids[i:i + batch_size]

            try:
                # Pinecone Fetch
                fetch_response = self.storage.pinecone_index.fetch(ids=batch_ids)
                vectors = self._extract_vectors_from_response(fetch_response)

                # ê° ë²¡í„° ë°ì´í„° íŒŒì‹±
                for vector_id in batch_ids:
                    if vector_id not in vectors:
                        continue

                    vector_data = vectors[vector_id]
                    metadata = self._extract_metadata(vector_data)

                    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    titles.append(metadata.get("title", ""))
                    texts.append(metadata.get("text", ""))
                    url = metadata.get("url", "")
                    urls.append(url)
                    dates.append(metadata.get("date", ""))

                    # HTML ì¡°íšŒ (html_availableì¸ ê²½ìš°)
                    html = ""
                    if metadata.get("html_available"):
                        html_available_count += 1
                        html, found = self._fetch_html_from_mongodb(
                            metadata, mongo_collection, html_available_count
                        )
                        if found:
                            mongo_found_count += 1
                            html_extracted_count += 1

                    htmls.append(html)
                    content_types.append(metadata.get("content_type", "text"))
                    sources.append(metadata.get("source", "original_post"))
                    image_urls.append(metadata.get("image_url", ""))
                    attachment_urls.append(metadata.get("attachment_url", ""))
                    attachment_types.append(metadata.get("attachment_type", ""))

            except Exception as e:
                logger.error(f"âš ï¸ ë°°ì¹˜ Fetch ì‹¤íŒ¨ ({i}~{i+batch_size}): {e}")
                continue

        # í†µê³„ ë¡œê¹…
        self._log_statistics(
            len(titles), html_available_count,
            mongo_found_count, html_extracted_count
        )

        return (titles, texts, urls, dates, htmls, content_types,
                sources, image_urls, attachment_urls, attachment_types)

    def _extract_vectors_from_response(self, fetch_response) -> dict:
        """Fetch ì‘ë‹µì—ì„œ ë²¡í„° ë”•ì…”ë„ˆë¦¬ ì¶”ì¶œ (ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬)"""
        vectors = {}

        if hasattr(fetch_response, 'to_dict'):
            response_dict = fetch_response.to_dict()
            vectors = response_dict.get('vectors', {})
        elif hasattr(fetch_response, 'vectors'):
            vectors = fetch_response.vectors
        else:
            vectors = fetch_response.get('vectors', {})

        return vectors or {}

    def _extract_metadata(self, vector_data) -> dict:
        """ë²¡í„° ë°ì´í„°ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        if isinstance(vector_data, dict):
            return vector_data.get('metadata', {}) or {}
        elif hasattr(vector_data, 'metadata'):
            return vector_data.metadata or {}
        return {}

    def _fetch_html_from_mongodb(
        self,
        metadata: dict,
        mongo_collection,
        count: int
    ) -> Tuple[str, bool]:
        """
        MongoDBì—ì„œ HTML/Markdown ì¡°íšŒ

        Args:
            metadata: Pinecone ë©”íƒ€ë°ì´í„°
            mongo_collection: MongoDB ì»¬ë ‰ì…˜
            count: í˜„ì¬ê¹Œì§€ ì¡°íšŒ ì‹œë„ íšŸìˆ˜ (ë¡œê¹…ìš©)

        Returns:
            Tuple[str, bool]: (HTML/Markdown ì½˜í…ì¸ , ì°¾ì•˜ëŠ”ì§€ ì—¬ë¶€)
        """
        if mongo_collection is None:
            return "", False

        try:
            # image_url ë˜ëŠ” attachment_urlë¡œ ì¡°íšŒ
            lookup_url = metadata.get("image_url") or metadata.get("attachment_url")

            if not lookup_url:
                if count <= 3:
                    url = metadata.get("url", "")
                    logger.warning(f"âš ï¸  html_available=trueì¸ë° image_url/attachment_url ì—†ìŒ (board URL: {url[:80]}...)")
                return "", False

            # ë””ë²„ê¹… ë¡œê¹… (ì²˜ìŒ 3ê°œë§Œ)
            if count <= 3:
                logger.info(f"ğŸ” ì¡°íšŒ ì‹œë„ URL: {lookup_url[:80]}...")

            # MongoDB ì¡°íšŒ
            cached = mongo_collection.find_one({"url": lookup_url})

            if cached:
                if count <= 3:
                    logger.info(f"âœ… MongoDBì—ì„œ ë°œê²¬: {lookup_url[:80]}...")
                    logger.info(f"   í•„ë“œ: {list(cached.keys())}")

                # Markdown ìš°ì„  (Upstage API ì œê³µ, ê³ í’ˆì§ˆ í‘œ êµ¬ì¡°)
                markdown_content = cached.get("ocr_markdown") or cached.get("markdown", "")
                # Markdownì´ ì—†ìœ¼ë©´ HTML ì‚¬ìš© (fallback)
                html_content = markdown_content or cached.get("ocr_html") or cached.get("html", "")

                if html_content:
                    return html_content, True
            else:
                if count <= 3:
                    logger.warning(f"âŒ MongoDBì—ì„œ ëª» ì°¾ìŒ: {lookup_url[:80]}...")

        except Exception as e:
            logger.warning(f"MongoDB HTML ì¡°íšŒ ì‹¤íŒ¨: {e}")

        return "", False

    def _log_statistics(
        self,
        total_count: int,
        html_available: int,
        mongo_found: int,
        html_extracted: int
    ):
        """í†µê³„ ë¡œê¹…"""
        logger.info(f"âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {total_count}ê°œ ë¬¸ì„œ")
        logger.info(f"ğŸ“Š HTML ì¡°íšŒ í†µê³„:")
        logger.info(f"   - html_available=true ë¬¸ì„œ: {html_available}ê°œ")
        logger.info(f"   - MongoDBì—ì„œ ì°¾ì€ ë¬¸ì„œ: {mongo_found}ê°œ")
        logger.info(f"   - ì‹¤ì œ HTML ì¶”ì¶œ ì„±ê³µ: {html_extracted}ê°œ")

    def _empty_result(self) -> Tuple[List, ...]:
        """ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        empty_list = []
        return (empty_list, empty_list, empty_list, empty_list, empty_list,
                empty_list, empty_list, empty_list, empty_list, empty_list)

    def initialize_cache(self):
        """
        ìºì‹œ ì´ˆê¸°í™” (Redis Fast Track ì ìš©)

        Process:
        1. Redis ìºì‹œ í™•ì¸ (ìˆìœ¼ë©´ 3ì´ˆ ë¡œë”©)
        2. ì—†ìœ¼ë©´ Pineconeì—ì„œ ë‹¤ìš´ë¡œë“œ (20ë¶„ ì†Œìš”, ìµœì´ˆ 1íšŒë§Œ)
        3. Redisì— ì €ì¥ (ë‹¤ìŒ ì¬ì‹œì‘ ì‹œ Fast Track)
        4. Retriever ì´ˆê¸°í™”
        """
        try:
            logger.info("ğŸ”„ ìºì‹œ ì´ˆê¸°í™” ì‹œì‘...")

            # 1. Redis ìºì‹œ í™•ì¸ (Fast Track)
            if self._load_from_redis_cache():
                return  # Fast Track ì„±ê³µ

            # 2. Pineconeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Slow Track)
            self._load_from_pinecone()

            # 3. Redisì— ì €ì¥ (ë‹¤ìŒì„ ìœ„í•´)
            self._save_to_redis_cache()

            logger.info(f"âœ… ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! (titles: {len(self.storage.cached_titles)}, texts: {len(self.storage.cached_texts)})")
            logger.info(f"   âš ï¸  Retriever ì´ˆê¸°í™”ëŠ” ai_modulesì—ì„œ ë³„ë„ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            self._initialize_empty_cache()

    def _load_from_redis_cache(self) -> bool:
        """Redis ìºì‹œì—ì„œ ë¡œë“œ (Fast Track)"""
        if self.storage.redis_client is None:
            return False

        try:
            logger.info("ğŸ” Redis ìºì‹œ í™•ì¸ ì¤‘...")
            cached_data = self.storage.redis_client.get('pinecone_metadata')

            if not cached_data:
                logger.info("â¬‡ï¸  Redisì— ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. Pinecone ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                return False

            # Redis ìºì‹œ ë°œê²¬!
            logger.info("ğŸš€ Redis ìºì‹œ ë°œê²¬! ë¹ ë¥¸ ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

            # Pickle ë°ì´í„° ë³µì›
            (self.storage.cached_titles, self.storage.cached_texts,
             self.storage.cached_urls, self.storage.cached_dates,
             self.storage.cached_htmls, self.storage.cached_content_types,
             self.storage.cached_sources, self.storage.cached_image_urls,
             self.storage.cached_attachment_urls, self.storage.cached_attachment_types
            ) = pickle.loads(cached_data)

            self._log_cache_stats("Redis")

            logger.info(f"âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ! (titles: {len(self.storage.cached_titles)}, texts: {len(self.storage.cached_texts)})")
            logger.info(f"   âš ï¸  Retriever ì´ˆê¸°í™”ëŠ” ai_modulesì—ì„œ ë³„ë„ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸  Redis ë¡œë“œ ì‹¤íŒ¨ (Pineconeì—ì„œ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤): {e}")
            return False

    def _load_from_pinecone(self):
        """Pineconeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Slow Track)"""
        logger.info("â³ Pinecone ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ìµœì´ˆ 1íšŒ, ì•½ 20ë¶„ ì†Œìš”)...")

        (self.storage.cached_titles, self.storage.cached_texts,
         self.storage.cached_urls, self.storage.cached_dates,
         self.storage.cached_htmls, self.storage.cached_content_types,
         self.storage.cached_sources, self.storage.cached_image_urls,
         self.storage.cached_attachment_urls, self.storage.cached_attachment_types
        ) = self.fetch_all_documents()

        self._log_cache_stats("Pinecone")

    def _save_to_redis_cache(self):
        """Redisì— ìºì‹œ ì €ì¥"""
        if self.storage.redis_client is None:
            logger.warning("âš ï¸  Redis ë¯¸ì‚¬ìš© (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©)")
            return

        try:
            cache_data = (
                self.storage.cached_titles, self.storage.cached_texts,
                self.storage.cached_urls, self.storage.cached_dates,
                self.storage.cached_htmls, self.storage.cached_content_types,
                self.storage.cached_sources, self.storage.cached_image_urls,
                self.storage.cached_attachment_urls, self.storage.cached_attachment_types
            )
            # 24ì‹œê°„ ìœ íš¨ (86400ì´ˆ)
            self.storage.redis_client.setex(
                'pinecone_metadata', 86400, pickle.dumps(cache_data)
            )
            logger.info("ğŸ’¾ ë°ì´í„°ë¥¼ Redisì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ë‹¤ìŒ ì¬ì‹œì‘ë¶€í„°ëŠ” 3ì´ˆ ë¡œë”©!)")

        except Exception as e:
            logger.warning(f"âš ï¸  Redis ì €ì¥ ì‹¤íŒ¨ (ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©): {e}")

    def _log_cache_stats(self, source: str):
        """ìºì‹œ í†µê³„ ë¡œê¹…"""
        logger.info(f"âœ… {source}ì—ì„œ {len(self.storage.cached_titles)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        logger.info(f"   - HTML êµ¬ì¡° ìˆëŠ” ë¬¸ì„œ: {sum(1 for html in self.storage.cached_htmls if html)}ê°œ")
        logger.info(f"   - ì´ë¯¸ì§€ OCR ë¬¸ì„œ: {sum(1 for ct in self.storage.cached_content_types if ct == 'image')}ê°œ")
        logger.info(f"   - ì²¨ë¶€íŒŒì¼ ë¬¸ì„œ: {sum(1 for ct in self.storage.cached_content_types if ct == 'attachment')}ê°œ")

    def _initialize_empty_cache(self):
        """ì—ëŸ¬ ì‹œ ë¹ˆ ìºì‹œë¡œ ì´ˆê¸°í™”"""
        self.storage.cached_titles = []
        self.storage.cached_texts = []
        self.storage.cached_urls = []
        self.storage.cached_dates = []
        self.storage.cached_htmls = []
        self.storage.cached_content_types = []
        self.storage.cached_sources = []
        self.storage.cached_image_urls = []
        self.storage.cached_attachment_urls = []
        self.storage.cached_attachment_types = []
