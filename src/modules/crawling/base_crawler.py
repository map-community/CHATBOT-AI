"""
ê¸°ë³¸ í¬ë¡¤ëŸ¬ ì¶”ìƒ í´ë˜ìŠ¤
ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê³µí†µ ê¸°ëŠ¥ ì œê³µ
"""
from abc import ABC, abstractmethod
from typing import List, Tuple, Optional
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import re
import time
from ..config import CrawlerConfig


class BaseCrawler(ABC):
    """
    ì¶”ìƒ ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤

    ì—­í• :
    - ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê³µí†µ ê¸°ëŠ¥ ì œê³µ
    - í…œí”Œë¦¿ ë©”ì„œë“œ íŒ¨í„´ ì ìš©
    - ì¬ì‹œë„ ë¡œì§ í¬í•¨
    """

    def __init__(self, board_type: str, base_url: str):
        """
        Args:
            board_type: ê²Œì‹œíŒ íƒ€ì… ('notice', 'job', 'seminar' ë“±)
            base_url: ê²Œì‹œíŒ ê¸°ë³¸ URL
        """
        self.board_type = board_type
        self.base_url = base_url
        self.max_workers = CrawlerConfig.MAX_WORKERS
        self.max_retries = CrawlerConfig.MAX_RETRIES
        self.retry_delay = CrawlerConfig.RETRY_DELAY

    def fetch_with_retry(self, url: str) -> Optional[requests.Response]:
        """
        ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ HTTP ìš”ì²­

        Args:
            url: ìš”ì²­í•  URL

        Returns:
            Response ê°ì²´ (ì‹¤íŒ¨ ì‹œ None)
        """
        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    return response
            except Exception as e:
                if attempt < self.max_retries - 1:
                    print(f"âš ï¸  ì¬ì‹œë„ {attempt + 1}/{self.max_retries}: {url}")
                    time.sleep(self.retry_delay)
                else:
                    print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url} - {e}")

        return None

    def get_latest_id(self) -> Optional[int]:
        """
        ê²Œì‹œíŒì˜ ìµœì‹  ê²Œì‹œê¸€ ID ì¡°íšŒ

        Returns:
            ìµœì‹  ID (ì¡°íšŒ ì‹¤íŒ¨ ì‹œ None)
        """
        response = self.fetch_with_retry(self.base_url)

        if response is None:
            return None

        # URLì—ì„œ wr_id ì¶”ì¶œ
        matches = re.findall(r'wr_id=(\d+)', response.text)

        if matches:
            return max(int(wr_id) for wr_id in matches)

        return None

    @abstractmethod
    def extract_from_url(self, url: str) -> Optional[Tuple[str, str, any, str, str]]:
        """
        URLì—ì„œ ë°ì´í„° ì¶”ì¶œ (ê° í¬ë¡¤ëŸ¬ì—ì„œ êµ¬í˜„)

        Args:
            url: í¬ë¡¤ë§í•  URL

        Returns:
            (title, text, image, date, url) íŠœí”Œ (ì‹¤íŒ¨ ì‹œ None)
        """
        pass

    def crawl_urls(self, urls: List[str]) -> List[Tuple[str, str, any, str, str]]:
        """
        ì—¬ëŸ¬ URLì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§

        Args:
            urls: í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸

        Returns:
            [(title, text, image, date, url), ...] ë¦¬ìŠ¤íŠ¸
        """
        all_data = []

        print(f"\n{'='*80}")
        print(f"ğŸŒ {self.board_type.upper()} í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“‹ í¬ë¡¤ë§í•  URL ê°œìˆ˜: {len(urls)}ê°œ")
        print(f"{'='*80}\n")

        if not urls:
            print("âš ï¸  í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return all_data

        print("ğŸ”„ ì›¹ í¬ë¡¤ë§ ì¤‘...\n")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = executor.map(self.extract_from_url, urls)

        # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€
        for result in results:
            if result is not None:
                title, text, image, date, url = result
                if title is not None and title != "Unknown Title":
                    all_data.append((title, text, image, date, url))

        print(f"\n{'='*80}")
        print(f"âœ… {self.board_type.upper()} í¬ë¡¤ë§ ì™„ë£Œ! {len(all_data)}ê°œ ìˆ˜ì§‘ë¨")
        print(f"{'='*80}\n")

        return all_data

    def generate_urls(self, id_range: range) -> List[str]:
        """
        ID ë²”ìœ„ë¡œë¶€í„° URL ë¦¬ìŠ¤íŠ¸ ìƒì„±

        Args:
            id_range: ID range ê°ì²´

        Returns:
            URL ë¦¬ìŠ¤íŠ¸
        """
        urls = []
        for wr_id in id_range:
            url = f"{self.base_url}&wr_id={wr_id}"
            urls.append(url)

        return urls
